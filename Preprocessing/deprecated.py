import spacy
import fasttext
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from nltk.corpus import wordnet
import nltk
import re
from sklearn.feature_extraction.text import CountVectorizer
from typing import Optional, Union, List
import numpy as np
import numpy.typing as npt
import sys
import time

from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import threading

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (í•„ìš”ì‹œ)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("Downloading NLTK punkt tokenizer...", file=sys.stderr)
    nltk.download('punkt', quiet=True)

try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    print("Downloading NLTK wordnet...", file=sys.stderr)
    nltk.download('wordnet', quiet=True)

try:
    nltk.data.find('taggers/averaged_perceptron_tagger')
except LookupError:
    print("Downloading NLTK POS tagger...", file=sys.stderr)
    nltk.download('averaged_perceptron_tagger', quiet=True)

class Word:
    TensorType = Union[np.ndarray, 'np.typing.NDArray[np.float32]']
    
    def __init__(self, content: str = None):
        self._content: Optional[str] = content
        self._idx: Optional[int] = None
        self._w2v_emb: Optional[Word.TensorType] = None  # Word2Vec ì„ë² ë”©
        self._dbert_emb: Optional[Word.TensorType] = None  # DistilBERT ì„ë² ë”©
        self._freq: int = 0  # ë¹ˆë„ìˆ˜ëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”
        self._isnode: Optional[bool] = None
        self._attention_emb: Optional[Word.TensorType] = None  # ì–´í…ì…˜ ì„ë² ë”©
        self._concat_emb: Optional[Word.TensorType] = None  # ì—°ê²°ëœ ì„ë² ë”©
        
        # í’ˆì‚¬ ì •ë³´ ì¶”ê°€
        self._pos_tags: List[str] = []  # ì´ ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚œ ëª¨ë“  í’ˆì‚¬ë“¤
        self._pos_counts: dict = {}  # ê° í’ˆì‚¬ë³„ ë“±ì¥ íšŸìˆ˜
        self._dominant_pos: Optional[str] = None  # ê°€ì¥ ë¹ˆë²ˆí•œ í’ˆì‚¬
    
    def __str__(self):
        return self._content or ""
    
    @property
    def content(self):
        return self._content
    
    @content.setter
    def content(self, word: str):
        self._content = word
    
    @property
    def idx(self):
        return self._idx
    
    @idx.setter
    def idx(self, value: int):
        self._idx = value
    
    @property
    def freq(self):
        return self._freq
    
    @property
    def pos_tags(self):
        """ì´ ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚œ ëª¨ë“  í’ˆì‚¬ ëª©ë¡"""
        return list(self._pos_tags)
    
    @property
    def pos_counts(self):
        """ê° í’ˆì‚¬ë³„ ë“±ì¥ íšŸìˆ˜"""
        return dict(self._pos_counts)
    
    @property
    def dominant_pos(self):
        """ê°€ì¥ ë¹ˆë²ˆí•˜ê²Œ ë‚˜íƒ€ë‚˜ëŠ” í’ˆì‚¬"""
        return self._dominant_pos
    
    def increment_freq(self, pos_tag: str = None):
        """ë¹ˆë„ìˆ˜ ì¦ê°€ ë° í’ˆì‚¬ ì •ë³´ ì—…ë°ì´íŠ¸"""
        self._freq += 1
        
        if pos_tag:
            # í’ˆì‚¬ ì •ë³´ ì—…ë°ì´íŠ¸
            if pos_tag not in self._pos_counts:
                self._pos_counts[pos_tag] = 0
                self._pos_tags.append(pos_tag)
            
            self._pos_counts[pos_tag] += 1
            
            # ê°€ì¥ ë¹ˆë²ˆí•œ í’ˆì‚¬ ì—…ë°ì´íŠ¸
            self._dominant_pos = max(self._pos_counts.items(), key=lambda x: x[1])[0]
    
    def get_pos_category(self):
        """í’ˆì‚¬ë¥¼ ì£¼ìš” ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
        if not self._dominant_pos:
            return "UNKNOWN"
        
        pos = self._dominant_pos.upper()
        
        if pos.startswith('N'):
            return "NOUN"
        # ë™ì‚¬ (Verb)
        elif pos.startswith('V'):
            return "VERB"
        # í˜•ìš©ì‚¬ (Adjective)
        elif pos.startswith('J'):
            return "ADJECTIVE"
        # ë¶€ì‚¬ (Adverb)
        elif pos.startswith('R'):
            return "ADVERB"
        # ëŒ€ëª…ì‚¬ (Pronoun)
        elif pos in ['PRP', 'PRP$', 'WP', 'WP$']:
            return "PRONOUN"
        # ì „ì¹˜ì‚¬ (Preposition)
        elif pos.startswith('IN'):
            return "PREPOSITION"
        # ì ‘ì†ì‚¬ (Conjunction)
        elif pos in ['CC', 'IN']:
            return "CONJUNCTION"
        # ê´€ì‚¬ (Determiner)
        elif pos in ['DT', 'WDT']:
            return "DETERMINER"
        # ê¸°íƒ€
        else:
            return "OTHER"

    def is_noun(self):
        pos = self._dominant_pos.upper()
        if pos.startswith("N"):
            return True
        return False
    
    def is_verb(self):
        pos = self._dominant_pos.upper()
        if pos.startswith("V"):
            return True
        return False
    
    def is_adjective(self):
        pos = self._dominant_pos.upper()
        if pos.startswith("J"):
            return True
        return False

    def is_pronoun(self):
        pos = self._dominant_pos.upper()
        if pos in ['PRP', 'PRP$', 'WP', 'WP$']:
            return True
        return False

    def is_adverb(self):
        pos = self._dominant_pos.upper()
        if pos.startswith('R'):
            return True
        return False
    
class TrieNode:
    """ì ‘ë‘ì‚¬ íŠ¸ë¦¬ ë…¸ë“œ"""
    def __init__(self):
        self.children = {}
        self.word_obj: Optional[Word] = None
        self.is_end_of_word = False

class WordTrie:
    """ì ‘ë‘ì‚¬ íŠ¸ë¦¬ë¥¼ ì´ìš©í•œ Word ê°ì²´ ê´€ë¦¬"""
    def __init__(self):
        self.root = TrieNode()
        self.word_count = 0  # ê³ ìœ  ë‹¨ì–´ ê°œìˆ˜ (idx í• ë‹¹ìš©)
    
    def insert_or_get_word(self, word_content: str, pos_tag: str = None) -> Word:
        """ë‹¨ì–´ë¥¼ íŠ¸ë¦¬ì— ì‚½ì…í•˜ê±°ë‚˜ ê¸°ì¡´ Word ê°ì²´ ë°˜í™˜"""
        node = self.root
        
        # íŠ¸ë¦¬ íƒìƒ‰
        for char in word_content:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        
        # ë‹¨ì–´ ë ì§€ì ì—ì„œ Word ê°ì²´ ì²˜ë¦¬
        if node.is_end_of_word:
            # ê¸°ì¡´ ë‹¨ì–´ - ë¹ˆë„ìˆ˜ ë° í’ˆì‚¬ ì •ë³´ ì—…ë°ì´íŠ¸
            node.word_obj.increment_freq(pos_tag)
            return node.word_obj
        else:
            # ìƒˆ ë‹¨ì–´ - Word ê°ì²´ ìƒì„± ë° ì„¤ì •
            word_obj = Word(word_content)
            word_obj.idx = self.word_count
            word_obj.increment_freq(pos_tag)
            
            node.word_obj = word_obj
            node.is_end_of_word = True
            self.word_count += 1
            
            return word_obj
    
    def get_all_words(self) -> List[Word]:
        """ëª¨ë“  Word ê°ì²´ ë°˜í™˜"""
        words = []
        self._dfs_collect_words(self.root, words)
        return words
    
    def _dfs_collect_words(self, node: TrieNode, words: List[Word]):
        """DFSë¡œ ëª¨ë“  Word ê°ì²´ ìˆ˜ì§‘"""
        if node.is_end_of_word:
            words.append(node.word_obj)
        
        for child in node.children.values():
            self._dfs_collect_words(child, words)

class Sentence:
    def __init__(self, docs_ref=None):
        self._raw: Optional[str] = None
        self._lemmatised: Optional[List[str]] = None
        self._word_indices: Optional[List[int]] = None
        self._word_objects: Optional[List[Word]] = None
        self._lemmatizer = WordNetLemmatizer()  # ê° ì¸ìŠ¤í„´ìŠ¤ë§ˆë‹¤ ìƒì„±
        self._docs_ref = docs_ref  # Docs ê°ì²´ ì°¸ì¡°
        
    @property
    def raw(self):
        return self._raw

    @raw.setter
    def raw(self, sentence: str):
        self._raw = sentence
        self._lemmatised = None
        self._word_indices = None
        self._word_objects = None
        self.lemmatise()
        
    def _get_wordnet_pos(self, word_pos):
        """POS íƒœê·¸ë¥¼ WordNet í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        tag = word_pos[0].upper()
        tag_dict = {
            "J": wordnet.ADJ,
            "N": wordnet.NOUN,
            "V": wordnet.VERB,
            "R": wordnet.ADV
        }
        return tag_dict.get(tag, wordnet.NOUN)
    
    def _expand_contractions(self, text):
        """ì¶•ì•½í˜•ì„ í™•ì¥í•˜ëŠ” í•¨ìˆ˜"""
        # ì¼ë°˜ì ì¸ ì¶•ì•½í˜• ë§¤í•‘
        contractions_dict = {
            "don't": "do not",
            "doesn't": "does not", 
            "didn't": "did not",
            "can't": "cannot",
            "couldn't": "could not",
            "won't": "will not",
            "wouldn't": "would not",
            "shouldn't": "should not",
            "mustn't": "must not",
            "needn't": "need not",
            "daren't": "dare not",
            "mayn't": "may not",
            "oughtn't": "ought not",
            
            # be ë™ì‚¬
            "i'm": "i am",
            "you're": "you are", 
            "he's": "he is",
            "she's": "she is",
            "it's": "it is",
            "we're": "we are",
            "they're": "they are",
            "that's": "that is",
            "there's": "there is",
            "here's": "here is",
            "what's": "what is",
            "where's": "where is",
            "who's": "who is",
            "how's": "how is",
            
            # have ë™ì‚¬
            "i've": "i have",
            "you've": "you have",
            "we've": "we have", 
            "they've": "they have",
            "could've": "could have",
            "should've": "should have",
            "would've": "would have",
            "might've": "might have",
            "must've": "must have",
            
            # had ë™ì‚¬  
            "i'd": "i had",
            "you'd": "you had",
            "he'd": "he had",
            "she'd": "she had", 
            "it'd": "it had",
            "we'd": "we had",
            "they'd": "they had",
            
            # will ë™ì‚¬
            "i'll": "i will",
            "you'll": "you will",
            "he'll": "he will", 
            "she'll": "she will",
            "it'll": "it will",
            "we'll": "we will",
            "they'll": "they will",
            "that'll": "that will",
            
            # ê³¼ê±°í˜•
            "wasn't": "was not",
            "weren't": "were not",
            "isn't": "is not",
            "aren't": "are not",
            "hasn't": "has not",
            "haven't": "have not",
            "hadn't": "had not",
        }
        
        # ì†Œë¬¸ìë¡œ ë³€í™˜ í›„ ì¶•ì•½í˜• í™•ì¥
        text_lower = text.lower()
        for contraction, expansion in contractions_dict.items():
            text_lower = text_lower.replace(contraction, expansion)
        
        return text_lower
    
    def lemmatise(self):
        """ë¬¸ì¥ì„ í‘œì œì–´ í˜•íƒœë¡œ ë³€í™˜í•˜ê³  Word ê°ì²´ ìƒì„±"""
        if self._raw is None:
            raise ValueError("Raw sentence is not set. Please set the raw property first.")
        
        if self._lemmatised is not None:
            return self._lemmatised
        
        try:
            # 1ë‹¨ê³„: ì¶•ì•½í˜• í™•ì¥
            expanded_text = self._expand_contractions(self._raw)
            
            # 2ë‹¨ê³„: íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì†Œìœ ê²© 's ì²˜ë¦¬ ê°œì„ )
            # ì†Œìœ ê²© 'së¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë¶„ë¦¬
            cleaned_text = re.sub(r"'s\b", " s", expanded_text)  # ì†Œìœ ê²© 's ë¶„ë¦¬
            cleaned_text = re.sub(r'[^\w\s]', '', cleaned_text)  # ë‚˜ë¨¸ì§€ íŠ¹ìˆ˜ë¬¸ì ì œê±°
            
            # 3ë‹¨ê³„: í† í°í™”
            tokens = word_tokenize(cleaned_text)
            
            # 4ë‹¨ê³„: POS íƒœê¹…
            pos_tags = pos_tag(tokens)
            
            # 5ë‹¨ê³„: í‘œì œì–´ ë³€í™˜ (ëª¨ë“  ë‹¨ì–´ ìœ ì§€)
            lemmatised_words = []
            word_objects = []
            word_indices = []
            
            for word, pos in pos_tags:
                # ë„ˆë¬´ ì§§ì€ í† í° í•„í„°ë§ (ë‹¨, ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ëŠ” ìœ ì§€)
                if len(word) >= 2 or word.lower() in ['i', 'a']:
                    wordnet_pos = self._get_wordnet_pos(pos)
                    lemma = self._lemmatizer.lemmatize(word, wordnet_pos)
                    lemmatised_words.append(lemma)
                    
                    # Docs ì°¸ì¡°ê°€ ìˆìœ¼ë©´ Word ê°ì²´ ìƒì„±/ê´€ë¦¬ (í’ˆì‚¬ ì •ë³´ í¬í•¨)
                    if self._docs_ref is not None:
                        word_obj = self._docs_ref.add_word(lemma, pos)  # í’ˆì‚¬ ì •ë³´ ì „ë‹¬
                        word_objects.append(word_obj)
                        word_indices.append(word_obj.idx)
            
            self._lemmatised = lemmatised_words
            self._word_objects = word_objects
            self._word_indices = word_indices
            
            return self._lemmatised
            
        except Exception as e:
            print(f"Lemmatisation failed for sentence: {self._raw[:50]}... Error: {e}", file=sys.stderr)
            # ì‹¤íŒ¨ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
            self._lemmatised = []
            self._word_objects = []
            self._word_indices = []
            return self._lemmatised
    
    @property
    def word_indices(self):
        return self._word_indices
    
    @property
    def word_objects(self):
        return self._word_objects

class Docs:
    def __init__(self):
        self._rawdata: Optional[List[str]] = None
        self._words_list: Optional[List[Word]] = None
        self._sentence_list: Optional[List[Sentence]] = None
        self._nlp: Optional[fasttext.FastText._FastText] = None
        self._lock = threading.Lock()
        self._word_trie = WordTrie()  # ì ‘ë‘ì‚¬ íŠ¸ë¦¬
    
    @property
    def rawdata(self):
        return self._rawdata
    
    @rawdata.setter
    def rawdata(self, docs: List[str]):
        if len(docs) < 10:
            raise ValueError("Number of documents is less than 10. Not sufficient for clustering.")
        elif len(docs) < 500:
            print("Warning: Document count is low ({}). Clustering results may have reduced accuracy.".format(len(docs)), file=sys.stderr)
        else:
            print("Processing {} documents".format(len(docs)), file=sys.stderr)
        
        self._rawdata = docs
        self.create_sentence_list()
    
    def add_word(self, word_content: str, pos_tag: str = None) -> Word:
        """ë‹¨ì–´ë¥¼ íŠ¸ë¦¬ì— ì¶”ê°€í•˜ê³  Word ê°ì²´ ë°˜í™˜ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        with self._lock:
            return self._word_trie.insert_or_get_word(word_content, pos_tag)
    
    def _create_sentence_with_log(self, doc_text: str, index: int) -> Sentence:
        """ë‹¨ì¼ Sentence ê°ì²´ ìƒì„± ë° ë¡œê·¸ (ì‚¬ìš© ì•ˆí•¨ - í˜¸í™˜ì„±ìš©)"""
        sentence = Sentence(docs_ref=self)
        sentence.raw = doc_text
        return sentence

    def create_sentence_list(self, max_workers: int = 1) -> None:
        """rawdataë¡œë¶€í„° Sentence ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„±"""
        if self._rawdata is None:
            self._sentence_list = None
            return

        print(f"Creating {len(self._rawdata)} sentences...", file=sys.stderr)
        
        # ë‹¨ìˆœí•œ ìˆœì°¨ ì²˜ë¦¬ë¡œ ë³€ê²½ (ë©€í‹°ìŠ¤ë ˆë”© ì œê±°)
        sentences = []
        for i, doc_text in enumerate(self._rawdata):
            if i % 100 == 0 and i > 0:  # 100ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
                print(f"Processed {i}/{len(self._rawdata)} documents", file=sys.stderr)
            
            try:
                sentence = Sentence(docs_ref=self)
                sentence.raw = doc_text
                sentences.append(sentence)
            except Exception as e:
                print(f'Document {i} processing failed: {e}', file=sys.stderr)
                # ì‹¤íŒ¨í•œ ê²½ìš° ë¹ˆ Sentence ê°ì²´ ìƒì„±
                sentence = Sentence(docs_ref=self)
                sentence._raw = doc_text
                sentence._lemmatised = []
                sentence._word_objects = []
                sentence._word_indices = []
                sentences.append(sentence)
        
        self._sentence_list = sentences
        
        # ëª¨ë“  ë¬¸ì¥ ì²˜ë¦¬ ì™„ë£Œ í›„ words_list ì—…ë°ì´íŠ¸
        self._words_list = self._word_trie.get_all_words()
        print(f"Created {len(self._words_list)} unique words", file=sys.stderr)

    @property
    def words_list(self):
        return self._words_list
    
    @property
    def sentence_list(self):
        return self._sentence_list
    
    def __str__(self):
        if self._rawdata is None:
            return ""
        words_count = len(self._words_list) if self._words_list else 0
        sentences_count = len(self._sentence_list) if self._sentence_list else 0
        return f"raw_data: {len(self._rawdata)}\nwords_list: {words_count}\nsentence_list: {sentences_count}"
    
    def __getitem__(self, idx):
        return self._rawdata[idx]

if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ
    docs = Docs()
    
    # ì¶•ì•½í˜•ê³¼ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ê°€ í¬í•¨ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_docs = [
        "I don't think he'll come today.",
        "She can't find her keys anywhere.",
        "They won't be here until tomorrow.",
        "It's not what I've been looking for.",
        "We're going to John's house tonight.",
        "That wasn't the answer I'd expected.",
        "You shouldn't have done that.",
        "I'll be there in five minutes.",
        "The dog's tail is wagging happily.",
        "These aren't the droids you're looking for."
    ]
    
    print("ğŸ§ª TESTING CONTRACTIONS AND APOSTROPHES")
    print("="*60)
    start = time.time()
    docs.rawdata = test_docs
    end = time.time()
    print(f"{end - start:.5f} sec") 
    
    print("\n" + "="*60)
    print("DOCUMENT PROCESSING SUMMARY")
    print("="*60)
    print(docs)
    
    print("\n" + "="*60)
    print("CONTRACTION PROCESSING ANALYSIS")
    print("="*60)
    
    # ì¶•ì•½í˜•ì´ ì–´ë–»ê²Œ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
    print("Raw vs Processed comparison:")
    for i, sentence in enumerate(docs.sentence_list[:5]):
        if sentence and sentence.word_indices is not None:
            print(f"\nSentence {i+1}:")
            print(f"  ğŸ“ Raw: {sentence.raw}")
            
            # ì „ì²˜ë¦¬ ê³¼ì • ë‹¨ê³„ë³„ í™•ì¸
            expanded = sentence._expand_contractions(sentence.raw)
            cleaned_step1 = re.sub(r"'s\b", " s", expanded)
            cleaned_final = re.sub(r'[^\w\s]', '', cleaned_step1)
            
            print(f"  ğŸ”„ Expanded: '{expanded}'")
            print(f"  ğŸ§¹ Cleaned: '{cleaned_final}'")
            print(f"  ğŸ”¤ Lemmatised: {sentence._lemmatised}")
            print(f"  ğŸ“Š Word indices: {sentence.word_indices}")
            
            # ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ë‹¨ì–´ë“¤ ì²´í¬
            problematic = []
            for word in sentence._lemmatised:
                if len(word) <= 1 or word in ['t', 's', 'll', 've', 're', 'm', 'd']:
                    problematic.append(word)
            
            if problematic:
                print(f"  âš ï¸  Potentially problematic tokens: {problematic}")
            else:
                print(f"  âœ… No problematic tokens detected")
        else:
            print(f"\nSentence {i+1}: âŒ Processing failed")
    
    print("\n" + "="*60)
    print("PART-OF-SPEECH ANALYSIS")
    print("="*60)
    
    # í’ˆì‚¬ë³„ ë‹¨ì–´ ë¶„ë¥˜
    pos_categories = {}
    for word in docs.words_list:
        category = word.get_pos_category()
        if category not in pos_categories:
            pos_categories[category] = []
        pos_categories[category].append(word)
    
    print("Words by POS category:")
    for category in sorted(pos_categories.keys()):
        words_in_category = pos_categories[category]
        print(f"\nğŸ“‚ {category}: {len(words_in_category)} words")
        
        # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•´ì„œ ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
        top_words = sorted(words_in_category, key=lambda w: w.freq, reverse=True)[:5]
        for word in top_words:
            pos_info = f"{word.dominant_pos}"
            if len(word.pos_counts) > 1:
                pos_info += f" (ë‹¤ì¤‘: {word.pos_counts})"
            print(f"    '{word.content}' - freq: {word.freq}, pos: {pos_info}")
        
        if len(words_in_category) > 5:
            print(f"    ... and {len(words_in_category) - 5} more {category.lower()}s")
    
    print(f"\nğŸ” Multi-POS words (ë™ì¼ ë‹¨ì–´ì˜ ë‹¤ë¥¸ í’ˆì‚¬ ìš©ë²•):")
    multi_pos_words = [w for w in docs.words_list if len(w.pos_counts) > 1]
    if multi_pos_words:
        for word in sorted(multi_pos_words, key=lambda w: len(w.pos_counts), reverse=True)[:10]:
            print(f"  '{word.content}': {word.pos_counts} (ì£¼í’ˆì‚¬: {word.dominant_pos})")
    else:
        print("  No words with multiple POS tags found")
    
    print(f"\nğŸ“Š POS Category Distribution:")
    total_words = len(docs.words_list)
    for category in sorted(pos_categories.keys()):
        count = len(pos_categories[category])
        percentage = (count / total_words) * 100
        print(f"  {category}: {count} words ({percentage:.1f}%)")
    
    print("\n" + "="*60)
    print("DETAILED WORD ANALYSIS")
    print("="*60)
    
    print("Sample detailed word information:")
    sample_words = sorted(docs.words_list, key=lambda w: w.freq, reverse=True)[:8]
    for word in sample_words:
        print(f"\nğŸ”¤ Word: '{word.content}'")
        print(f"   ğŸ“Š Frequency: {word.freq}")
        print(f"   ğŸ”¢ Index: {word.idx}")
        print(f"   ğŸ“ Category: {word.get_pos_category()}")
        print(f"   ğŸ·ï¸  Dominant POS: {word.dominant_pos}")
        print(f"   ğŸ“‹ All POS tags: {word.pos_counts}")
        
        # í¸ì˜ ë©”ì„œë“œ í…ŒìŠ¤íŠ¸
        pos_checks = []
        if word.is_noun(): pos_checks.append("NOUN")
        if word.is_verb(): pos_checks.append("VERB") 
        if word.is_adjective(): pos_checks.append("ADJECTIVE")
        if word.is_pronoun(): pos_checks.append("PRONOUN")
        if word.is_adverb(): pos_checks.append("ADVERB")
        
        if pos_checks:
            print(f"   âœ… Type checks: {', '.join(pos_checks)}")
        else:
            print(f"   â„¹ï¸  Type: OTHER ({word.get_pos_category()})")
    
    print("\n" + "="*60)
    print("CONTRACTION DETECTION")
    print("="*60)
    
    # ì›ë³¸ ë¬¸ì¥ì—ì„œ ì¶•ì•½í˜• íŒ¨í„´ ì°¾ê¸°
    contraction_patterns = [
        "don't", "can't", "won't", "it's", "we're", "wasn't", 
        "I'd", "shouldn't", "I'll", "aren't", "you're", "he'll", "I've"
    ]
    
    found_contractions = []
    for sentence in docs.sentence_list:
        if sentence and sentence.raw:
            for pattern in contraction_patterns:
                if pattern.lower() in sentence.raw.lower():
                    found_contractions.append((pattern, sentence.raw))
    
    print(f"Found contractions in original text:")
    for contraction, sentence in found_contractions[:5]:  # ì²˜ìŒ 5ê°œë§Œ
        print(f"  '{contraction}' in: {sentence}")
    
    # ì „ì²´ í† í° ìˆ˜ vs ì˜ˆìƒ í† í° ìˆ˜ ë¹„êµ
    total_original_tokens = sum(len(doc.split()) for doc in test_docs)
    total_processed_tokens = sum(len(s.word_indices) if s.word_indices else 0 for s in docs.sentence_list)
    
    print(f"\nğŸ“Š Token count comparison:")
    print(f"  Original word count (space-split): {total_original_tokens}")
    print(f"  Processed token count: {total_processed_tokens}")
    print(f"  Difference: {total_processed_tokens - total_original_tokens}")
    
    if total_processed_tokens > total_original_tokens:
        print("  â„¹ï¸  More tokens after processing (contractions likely split)")
    elif total_processed_tokens < total_original_tokens:
        print("  âš ï¸  Fewer tokens after processing (some tokens lost)")
    else:
        print("  âœ… Same token count")
    
    print("\n" + "="*60)
    print("RECOMMENDATIONS")
    print("="*60)
    
    suspicious_words = [w for w in docs.words_list if w.content in ['t', 's', 'll', 've', 're', 'm', 'd', 'nt']]
    if suspicious_words:
        print("ğŸš¨ Issues detected with contraction handling:")
        print("   - Contractions are being split into fragments")
        print("   - Consider using a contraction expansion library")
        print("   - Or modify preprocessing to handle apostrophes better")
    else:
        print("âœ… No obvious contraction handling issues detected")
    
    print("\nSuggested improvements:")
    print("1. Use contractions library: pip install contractions")
    print("2. Expand contractions before lemmatization")
    print("3. Or modify regex to preserve apostrophes in specific contexts")