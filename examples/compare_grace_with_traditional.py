"""
GRACE vs Traditional Graph Clustering ë¹„êµ ì‹¤í—˜

GRACE (GraphMAE ê¸°ë°˜)ì™€ ì „í†µì  ê·¸ë˜í”„ í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•ë“¤ì„ ë¹„êµí•©ë‹ˆë‹¤.
- Louvain
- Leiden  
- Girvan-Newman

ë™ì¼í•œ WordGraphë¥¼ ê³µìœ í•˜ì—¬ ê³µì •í•œ ë¹„êµë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.
"""

import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'core'))

import numpy as np
import pandas as pd
import torch
from datetime import datetime

from services.GRACE import GRACEConfig, GRACEPipeline, TraditionalGraphClusteringService
from services.Document import DocumentService
from services.Graph import GraphService
from services.Graph import NodeFeatureHandler
from services.GraphMAE import GraphMAEService, GraphMAEConfig
from services.GRACE import ClusteringService
from services.Metric import MetricsService
from entities import NodeFeatureType


def run_comparison_experiment(
    csv_path: str,
    num_documents: int = 10000,
    top_n_words: int = 500,
    output_dir: str = './comparison_output'
):
    """
    GRACEì™€ ì „í†µì  í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²• ë¹„êµ ì‹¤í—˜
    
    Args:
        csv_path: ë°ì´í„°ì…‹ ê²½ë¡œ
        num_documents: ì‚¬ìš©í•  ë¬¸ì„œ ìˆ˜
        top_n_words: ê·¸ë˜í”„ ë…¸ë“œ ìˆ˜
        output_dir: ê²°ê³¼ ì €ì¥ ê²½ë¡œ
    """
    
    print("=" * 80)
    print("ğŸ”¬ GRACE vs Traditional Graph Clustering ë¹„êµ ì‹¤í—˜")
    print("=" * 80)
    print(f"\nğŸ“Š ì‹¤í—˜ ì„¤ì •:")
    print(f"  - ë¬¸ì„œ ìˆ˜: {num_documents}")
    print(f"  - ë…¸ë“œ ìˆ˜: {top_n_words}")
    print(f"  - ë°ì´í„°: {csv_path}")
    print(f"  - ì¶œë ¥: {output_dir}\n")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # ========================================
    # Step 1: ê³µí†µ ê·¸ë˜í”„ êµ¬ì¶•
    # ========================================
    print("\n[Step 1/5] ë°ì´í„° ì „ì²˜ë¦¬ ë° ì˜ë¯¸ì—°ê²°ë§ êµ¬ì¶•")
    print("-" * 80)
    
    # CSV ë¡œë“œ (pandas ì‚¬ìš©)
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
    
    df = pd.read_csv(csv_path)
    print(f"  ì „ì²´ ë°ì´í„°: {len(df)} í–‰")
    
    text_column = 'body'
    if text_column not in df.columns:
        raise ValueError(f"CSVì— '{text_column}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥: {df.columns.tolist()}")
    
    # ë¬¸ì„œ ì¶”ì¶œ
    documents = df[text_column].dropna().head(num_documents).tolist()
    print(f"  ë¡œë“œëœ ë¬¸ì„œ: {len(documents)}ê°œ")
    
    # ë¬¸ì„œ ì „ì²˜ë¦¬
    doc_service = DocumentService()
    doc_service.create_sentence_list(documents=documents)
    print(f"  ì „ì²˜ë¦¬ ì™„ë£Œ: {doc_service.get_sentence_count()}ê°œ ë¬¸ì¥")
    
    # ì˜ë¯¸ì—°ê²°ë§ êµ¬ì¶•
    graph_service = GraphService(doc_service)
    word_graph = graph_service.build_complete_graph(
        top_n=top_n_words,
        exclude_stopwords=True,
        max_length=-1
    )
    
    print(f"âœ… ê³µí†µ WordGraph ìƒì„± ì™„ë£Œ")
    print(f"   - ë…¸ë“œ: {word_graph.num_nodes}")
    print(f"   - ì—£ì§€: {word_graph.num_edges}")
    print(f"   - ë°€ë„: {word_graph.get_graph_stats()['density']:.4f}")
    
    # ========================================
    # Step 2: ì „í†µì  ê·¸ë˜í”„ í´ëŸ¬ìŠ¤í„°ë§
    # ========================================
    print("\n[Step 2/5] ì „í†µì  ê·¸ë˜í”„ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰")
    print("-" * 80)
    
    traditional_service = TraditionalGraphClusteringService(random_state=42)
    
    traditional_results = {}
    
    # 2.1. Louvain
    print("\nğŸ”µ [1/3] Louvain í´ëŸ¬ìŠ¤í„°ë§...")
    try:
        louvain_labels, louvain_metrics = traditional_service.louvain_clustering(
            word_graph, resolution=1.0
        )
        traditional_results['louvain'] = {
            'labels': louvain_labels,
            'metrics': louvain_metrics
        }
    except Exception as e:
        print(f"âš ï¸  Louvain ì‹¤íŒ¨: {e}")
        traditional_results['louvain'] = None
    
    # 2.2. Leiden
    print("\nğŸŸ¢ [2/3] Leiden í´ëŸ¬ìŠ¤í„°ë§...")
    try:
        leiden_labels, leiden_metrics = traditional_service.leiden_clustering(
            word_graph, resolution=1.0
        )
        traditional_results['leiden'] = {
            'labels': leiden_labels,
            'metrics': leiden_metrics
        }
    except Exception as e:
        print(f"âš ï¸  Leiden ì‹¤íŒ¨: {e}")
        traditional_results['leiden'] = None
    
    # 2.3. Girvan-Newman (ëŒ€ê·œëª¨ ê·¸ë˜í”„ì—ì„œëŠ” ìŠ¤í‚µ)
    print("\nğŸŸ  [3/3] Girvan-Newman í´ëŸ¬ìŠ¤í„°ë§...")
    # Girvan-Newmanì€ O(mÂ²n) ë³µì¡ë„ë¡œ ëŒ€ê·œëª¨ ê·¸ë˜í”„ì—ì„œ ë¹„í˜„ì‹¤ì 
    # ë…¼ë¬¸ì—ì„œ Louvain, Leidenë§Œìœ¼ë¡œ ì „í†µì  ë°©ë²• ëŒ€í‘œ ê°€ëŠ¥
    if word_graph.num_nodes <= 50:  # ë§¤ìš° ì‘ì€ ê·¸ë˜í”„ì—ë§Œ ì œí•œ
        try:
            print(f"   ìë™ í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì • (Modularity ìµœëŒ€í™”)")
            print(f"   â³ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ìµœëŒ€ 20ë²ˆ ë°˜ë³µ)...")
            
            gn_labels, gn_metrics = traditional_service.girvan_newman_clustering(
                word_graph, num_clusters=None, verbose=True
            )
            traditional_results['girvan_newman'] = {
                'labels': gn_labels,
                'metrics': gn_metrics
            }
        except Exception as e:
            print(f"âš ï¸  Girvan-Newman ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            traditional_results['girvan_newman'] = None
    else:
        print(f"âš ï¸  Girvan-Newman ìŠ¤í‚µ (ë…¸ë“œ: {word_graph.num_nodes} > 50)")
        print(f"   ğŸ’¡ O(mÂ²n) ë³µì¡ë„ë¡œ ëŒ€ê·œëª¨ ê·¸ë˜í”„ì—ì„œ ì‹¤í–‰ ë¶ˆê°€ëŠ¥")
        print(f"   ğŸ’¡ ë¹„êµ ëŒ€ìƒ: Louvain, Leiden (ì „í†µì  ë°©ë²• ëŒ€í‘œ)")
        traditional_results['girvan_newman'] = None
    
    # ========================================
    # Step 3: GRACE í´ëŸ¬ìŠ¤í„°ë§ (GraphMAE)
    # ========================================
    print("\n[Step 3/5] GRACE í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ (GraphMAE)")
    print("-" * 80)
    
    # ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ê³„ì‚°
    print("ğŸ“Š ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ê³„ì‚° ì¤‘...")
    node_feature_handler = NodeFeatureHandler(doc_service)
    
    # concat ë°©ì‹ìœ¼ë¡œ Word2Vec + BERT ê²°í•© (ì´ 64ì°¨ì›)
    multimodal_features = node_feature_handler.calculate_embeddings(
        words=word_graph.words,
        method='concat',
        embed_size=64
    )
    
    word_graph.set_node_features_custom(
        multimodal_features,
        feature_type=NodeFeatureType.CUSTOM
    )
    
    # GraphMAE í•™ìŠµ
    print("ğŸ§  GraphMAE ìê¸°ì§€ë„í•™ìŠµ...")
    embed_size = multimodal_features.shape[1]
    
    # GraphMAE ì„¤ì •
    graphmae_config = GraphMAEConfig.create_default(embed_size)
    graphmae_config.max_epochs = 100
    graphmae_config.learning_rate = 0.001
    graphmae_config.weight_decay = 0.0
    graphmae_config.mask_rate = 0.75
    graphmae_config.encoder_type = 'gat'
    graphmae_config.decoder_type = 'gat'
    graphmae_config.loss_fn = 'sce'
    graphmae_config.alpha_l = 2.0
    graphmae_config.device = "cuda" if torch.cuda.is_available() else "cpu"
    
    print(f"   Epochs: {graphmae_config.max_epochs}, Device: {graphmae_config.device}")
    
    # GraphMAE ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    graphmae_service = GraphMAEService(graph_service, graphmae_config)
    
    # DGL ê·¸ë˜í”„ë¡œ ë³€í™˜
    dgl_graph = graph_service.wordgraph_to_dgl(word_graph, multimodal_features)
    
    # ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
    graphmae_service.model = graphmae_service.create_mae_model(embed_size)
    device_obj = torch.device(graphmae_config.device)
    graphmae_service.model.to(device_obj)
    dgl_graph = dgl_graph.to(device_obj)
    
    optimizer = torch.optim.Adam(
        graphmae_service.model.parameters(),
        lr=graphmae_config.learning_rate,
        weight_decay=graphmae_config.weight_decay
    )
    
    # í•™ìŠµ ì‹¤í–‰
    print(f"   í•™ìŠµ ì‹œì‘: {graphmae_config.max_epochs} epochs...")
    graphmae_service.model.train()
    
    for epoch in range(graphmae_config.max_epochs):
        loss, loss_dict = graphmae_service.model(dgl_graph, dgl_graph.ndata['feat'])
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 20 == 0:
            print(f"   Epoch {epoch+1}/{graphmae_config.max_epochs}: Loss = {loss.item():.4f}")
    
    # ì„ë² ë”© ì¶”ì¶œ
    graphmae_service.model.eval()
    with torch.no_grad():
        graphmae_embeddings = graphmae_service.model.embed(dgl_graph, dgl_graph.ndata['feat'])
    
    graphmae_embeddings = graphmae_embeddings.cpu()
    print(f"âœ… GraphMAE í•™ìŠµ ì™„ë£Œ: ì„ë² ë”© shape = {graphmae_embeddings.shape}")
    
    # GRACE í´ëŸ¬ìŠ¤í„°ë§ (Elbow Method)
    print("ğŸ¯ GRACE í´ëŸ¬ìŠ¤í„°ë§ (K-means + Elbow)...")
    clustering_service = ClusteringService(random_state=42)
    grace_labels, best_k, inertias, silhouette_scores = clustering_service.auto_clustering_elbow(
        graphmae_embeddings,
        min_clusters=3,
        max_clusters=20,
        n_init=10
    )
    
    traditional_results['grace'] = {
        'labels': grace_labels,
        'metrics': {
            'method': 'grace',
            'num_clusters': best_k,
            'num_nodes': word_graph.num_nodes,
            'num_edges': word_graph.num_edges
        }
    }
    
    print(f"âœ… GRACE í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {best_k}ê°œ í´ëŸ¬ìŠ¤í„°")
    
    # ========================================
    # Step 4: ì •ëŸ‰ì  í‰ê°€
    # ========================================
    print("\n[Step 4/5] ì •ëŸ‰ì  í‰ê°€ (Clustering Metrics)")
    print("-" * 80)
    
    metrics_service = MetricsService()
    
    # ëª¨ë“  ë°©ë²•ì— ëŒ€í•´ í‰ê°€ ì§€í‘œ ê³„ì‚°
    evaluation_results = {}
    
    for method_name, result in traditional_results.items():
        if result is None:
            continue
        
        labels = result['labels']
        
        # GraphMAE ì„ë² ë”© ì‚¬ìš© (GRACE)
        if method_name == 'grace':
            embeddings_np = graphmae_embeddings.numpy()
        else:
            # ì „í†µì  ë°©ë²•ì€ ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ì‚¬ìš© (ê³µì •í•œ ë¹„êµ)
            embeddings_np = multimodal_features.numpy()
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        print(f"\nğŸ“Š {method_name.upper()} í‰ê°€:")
        
        silhouette = metrics_service.compute_silhouette_score(embeddings_np, labels)
        davies_bouldin = metrics_service.compute_davies_bouldin_score(embeddings_np, labels)
        calinski = metrics_service.compute_calinski_harabasz_score(embeddings_np, labels)
        
        print(f"   Silhouette: {silhouette:.4f}")
        print(f"   Davies-Bouldin: {davies_bouldin:.4f}")
        print(f"   Calinski-Harabasz: {calinski:.2f}")
        
        # NPMI (ì£¼ì œ ì¼ê´€ì„±)
        try:
            # í´ëŸ¬ìŠ¤í„°ë³„ ìƒìœ„ ë‹¨ì–´ ì¶”ì¶œ
            cluster_words = {}
            for cluster_id in range(result['metrics']['num_clusters']):
                mask = labels == cluster_id
                node_ids = np.where(mask)[0]
                words = [word_graph.get_word_by_node_id(int(nid)).content for nid in node_ids[:10]]
                cluster_words[cluster_id] = words
            
            npmi = metrics_service.compute_npmi_coherence(cluster_words, doc_service.sentence_list)
            print(f"   NPMI: {npmi:.4f}")
        except Exception as e:
            print(f"   NPMI: N/A ({e})")
            npmi = None
        
        evaluation_results[method_name] = {
            'num_clusters': result['metrics']['num_clusters'],
            'silhouette': float(silhouette),
            'davies_bouldin': float(davies_bouldin),
            'calinski_harabasz': float(calinski),
            'npmi': float(npmi) if npmi is not None else None,
            'modularity': result['metrics'].get('modularity', None)
        }
    
    # ========================================
    # Step 5: ê²°ê³¼ ì €ì¥
    # ========================================
    print("\n[Step 5/5] ê²°ê³¼ ì €ì¥")
    print("-" * 80)
    
    # ë¹„êµ í…Œì´ë¸” ìƒì„±
    comparison_df = pd.DataFrame(evaluation_results).T
    comparison_df = comparison_df.round(4)
    
    print("\nğŸ“Š ë¹„êµ ê²°ê³¼:")
    print(comparison_df.to_string())
    
    # CSV ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_path = f"{output_dir}/comparison_results_{timestamp}.csv"
    comparison_df.to_csv(csv_path)
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {csv_path}")
    
    # JSON ì €ì¥ (ìƒì„¸ ì •ë³´)
    import json
    json_path = f"{output_dir}/comparison_results_{timestamp}.json"
    
    # numpy íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜
    def convert_to_serializable(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32)):
            return float(obj)
        return obj
    
    json_results = {
        'experiment_config': {
            'num_documents': num_documents,
            'top_n_words': top_n_words,
            'timestamp': timestamp
        },
        'evaluation': evaluation_results,
        'cluster_distributions': {
            method: {
                'num_clusters': result['metrics']['num_clusters'],
                'distribution': {
                    int(k): int(v) for k, v in 
                    zip(*np.unique(result['labels'], return_counts=True))
                }
            }
            for method, result in traditional_results.items() if result is not None
        }
    }
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_results, f, indent=2, ensure_ascii=False, default=convert_to_serializable)
    
    print(f"ğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {json_path}")
    
    # ========================================
    # ê²°ë¡ 
    # ========================================
    print("\n" + "=" * 80)
    print("âœ… ë¹„êµ ì‹¤í—˜ ì™„ë£Œ!")
    print("=" * 80)
    print("\nğŸ“Œ ì£¼ìš” ë°œê²¬:")
    
    # ìµœê³  ì„±ëŠ¥ ë©”ì„œë“œ ì°¾ê¸°
    best_silhouette = max(evaluation_results.items(), key=lambda x: x[1]['silhouette'])
    best_davies = min(evaluation_results.items(), key=lambda x: x[1]['davies_bouldin'])
    
    print(f"  - ìµœê³  Silhouette: {best_silhouette[0].upper()} ({best_silhouette[1]['silhouette']:.4f})")
    print(f"  - ìµœì € Davies-Bouldin: {best_davies[0].upper()} ({best_davies[1]['davies_bouldin']:.4f})")
    
    if 'grace' in evaluation_results:
        grace_rank = sorted(
            evaluation_results.items(), 
            key=lambda x: x[1]['silhouette'], 
            reverse=True
        )
        grace_position = [i for i, (name, _) in enumerate(grace_rank) if name == 'grace'][0] + 1
        print(f"  - GRACE ìˆœìœ„: {grace_position}/{len(evaluation_results)} (Silhouette ê¸°ì¤€)")
    
    print("\nğŸ’¡ ë…¼ë¬¸ ì‘ì„± ì‹œ í™œìš©:")
    print("  1. GRACEê°€ ì „í†µì  ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œì§€ í™•ì¸")
    print("  2. GraphMAEì˜ íš¨ê³¼ (êµ¬ì¡° í•™ìŠµ) ì…ì¦")
    print("  3. Modularity vs í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ë©”íŠ¸ë¦­ ë¹„êµ")
    print("  4. ì •ì„±ì  ë¶„ì„: ê° ë°©ë²•ì˜ í´ëŸ¬ìŠ¤í„° ë‹¨ì–´ ë¹„êµ")
    
    return evaluation_results, traditional_results


if __name__ == '__main__':
    # ì‹¤í—˜ ì„¤ì •
    csv_path = '/home/jaehun/lab/SENTIMENT/kaggle_RC_2019-05.csv'
    
    # ì‹¤í—˜ ì‹¤í–‰ (ë§¤ìš° ì‘ì€ ì„¤ì •ìœ¼ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
    evaluation_results, clustering_results = run_comparison_experiment(
        csv_path=csv_path,
        num_documents=10000,  # ë§¤ìš° ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        top_n_words=500,     # ë§¤ìš° ì‘ì€ ê·¸ë˜í”„ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        output_dir='./comparison_output'
    )
