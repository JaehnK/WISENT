# Edge Weight í†µí•© êµ¬í˜„ ê³„íšì„œ

> **ëª©ì **: GraphMAE2 ëª¨ë¸ì— ê³µì¶œí˜„ ë¹ˆë„(edge weight) ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ
> **ë‚ ì§œ**: 2025-10-17
> **ë³€ê²½ ë²”ìœ„**: GAT â†’ GCN ì „í™˜ + Edge Weight í™œì„±í™”

---

## ğŸ“‹ Executive Summary

### í˜„ì¬ ë¬¸ì œì 
- **GraphMAE2 ëª¨ë¸ì´ edge weightë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ**
- WordGraphëŠ” ê³µì¶œí˜„ ë¹ˆë„ë¥¼ `edge_attr`ì— ì €ì¥í•˜ì§€ë§Œ, DGL ê·¸ë˜í”„ ë³€í™˜ ì‹œ ì œì™¸ë¨
- GATëŠ” attention mechanismë§Œ ì‚¬ìš©í•˜ê³  edge weightë¥¼ ì§ì ‘ í™œìš©í•˜ì§€ ì•ŠìŒ

### í•´ê²° ë°©ì•ˆ
1. **Encoder/Decoderë¥¼ GAT â†’ GCNìœ¼ë¡œ ë³€ê²½**
2. **GCNì˜ ì£¼ì„ ì²˜ë¦¬ëœ edge weight ì½”ë“œ í™œì„±í™”**
3. **DGL ê·¸ë˜í”„ ë³€í™˜ ì‹œ edge weight í¬í•¨**
4. **Edge weight normalization ì¶”ê°€**

### ì˜ˆìƒ íš¨ê³¼
- âœ… ê³µì¶œí˜„ ë¹ˆë„ ì •ë³´ í™œìš© â†’ ì˜ë¯¸ì  ê´€ê³„ ê°•ë„ ë°˜ì˜
- âœ… ëª¨ë¸ í‘œí˜„ë ¥ í–¥ìƒ â†’ í´ëŸ¬ìŠ¤í„°ë§ ì„±ëŠ¥ ê°œì„ 
- âœ… ë…¼ë¬¸ì˜ ì´ë¡ ì  ì •ë‹¹ì„± ê°•í™”

---

## ğŸ¯ êµ¬í˜„ ê³„íš

### Phase 1: GCN Edge Weight í™œì„±í™”

#### íŒŒì¼: `core/GraphMAE2/models/gcn.py`

**í˜„ì¬ ìƒíƒœ** (Line 126-157):
```python
def forward(self, graph, feat):
    with graph.local_scope():
        aggregate_fn = fn.copy_src('h', 'm')
        # if edge_weight is not None:
        #     assert edge_weight.shape[0] == graph.number_of_edges()
        #     graph.edata['_edge_weight'] = edge_weight
        #     aggregate_fn = fn.u_mul_e('h', '_edge_weight', 'm')
```

**ë³€ê²½ í›„**:
```python
def forward(self, graph, feat):
    with graph.local_scope():
        # Edge weight ì§€ì› ì¶”ê°€
        if 'weight' in graph.edata:
            aggregate_fn = fn.u_mul_e('h', 'weight', 'm')
        else:
            aggregate_fn = fn.copy_src('h', 'm')

        # ... ë‚˜ë¨¸ì§€ ì½”ë“œ ë™ì¼
```

**ë³€ê²½ ì‚¬ìœ **:
- DGL ê·¸ë˜í”„ì˜ edge dataì— 'weight' í‚¤ê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš©
- Backward compatibility ìœ ì§€ (weight ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹)

---

### Phase 2: DGL ê·¸ë˜í”„ ë³€í™˜ ì‹œ Edge Weight ì¶”ê°€

#### íŒŒì¼: `core/services/Graph/GraphService.py`

**í˜„ì¬ ìƒíƒœ** (Line 148-189):
```python
def wordgraph_to_dgl(self, word_graph: 'WordGraph', node_features: Optional[torch.Tensor] = None):
    # ...
    # ì—£ì§€ ê°€ì¤‘ì¹˜ëŠ” self-loop ì¶”ê°€ í›„ í¬ê¸°ê°€ ë§ì§€ ì•Šìœ¼ë¯€ë¡œ ì„¤ì •í•˜ì§€ ì•ŠìŒ
    # GraphMAEëŠ” ì—£ì§€ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

    return dgl_graph
```

**ë³€ê²½ í›„**:
```python
def wordgraph_to_dgl(self, word_graph: 'WordGraph', node_features: Optional[torch.Tensor] = None):
    # ... ê¸°ì¡´ ì½”ë“œ ...

    # Self-loop ì¶”ê°€
    dgl_graph = dgl.add_self_loop(dgl_graph)

    # ë…¸ë“œ íŠ¹ì„± ì„¤ì •
    if node_features is not None:
        dgl_graph.ndata['feat'] = node_features
    else:
        freq_features = torch.tensor([[word.freq] for word in word_graph.words], dtype=torch.float32)
        dgl_graph.ndata['feat'] = freq_features

    # ========== Edge Weight ì¶”ê°€ (ìƒˆë¡œìš´ ë¶€ë¶„) ==========
    if word_graph.edge_attr is not None:
        # ì›ë³¸ ì—£ì§€ ê°€ì¤‘ì¹˜ ì¶”ì¶œ
        original_edge_weights = word_graph.edge_attr.squeeze()  # [num_edges]

        # Self-loop ê°€ì¤‘ì¹˜ (ë…¸ë“œ ìì‹ ê³¼ì˜ ì—°ê²°ì€ ìµœëŒ€ê°’ìœ¼ë¡œ ì„¤ì •)
        num_self_loops = word_graph.num_nodes
        self_loop_weights = torch.ones(num_self_loops, dtype=torch.float32)

        # ì›ë³¸ ê°€ì¤‘ì¹˜ + self-loop ê°€ì¤‘ì¹˜ ê²°í•©
        all_edge_weights = torch.cat([original_edge_weights, self_loop_weights])

        # ì •ê·œí™” (ì¤‘ìš”!)
        all_edge_weights = self._normalize_edge_weights(all_edge_weights)

        # DGL ê·¸ë˜í”„ì— ì„¤ì •
        dgl_graph.edata['weight'] = all_edge_weights

    return dgl_graph

def _normalize_edge_weights(self, weights: torch.Tensor, method: str = 'minmax') -> torch.Tensor:
    """
    Edge weight ì •ê·œí™”

    Args:
        weights: ì›ë³¸ ê°€ì¤‘ì¹˜ [num_edges]
        method: ì •ê·œí™” ë°©ë²• ('minmax', 'log', 'standard')

    Returns:
        ì •ê·œí™”ëœ ê°€ì¤‘ì¹˜ [num_edges]
    """
    if method == 'minmax':
        # Min-Max scaling to [0, 1]
        min_val = weights.min()
        max_val = weights.max()
        if max_val - min_val > 0:
            return (weights - min_val) / (max_val - min_val)
        else:
            return weights

    elif method == 'log':
        # Log scaling (ê³µì¶œí˜„ ë¹ˆë„ëŠ” power-law ë¶„í¬ ê²½í–¥)
        return torch.log(weights + 1.0)

    elif method == 'standard':
        # Standardization (mean=0, std=1)
        mean = weights.mean()
        std = weights.std()
        if std > 0:
            return (weights - mean) / std
        else:
            return weights - mean

    else:
        return weights
```

**ë³€ê²½ ì‚¬ìœ **:
- Self-loop ì¶”ê°€ í›„ì—ë„ edge weight ì°¨ì› ì¼ì¹˜
- ì •ê·œí™”ë¡œ í•™ìŠµ ì•ˆì •ì„± í™•ë³´
- ì—¬ëŸ¬ ì •ê·œí™” ë°©ë²• ì§€ì› (ì‹¤í—˜ ê°€ëŠ¥)

---

### Phase 3: Config ë³€ê²½ (Encoder/Decoderë¥¼ GCNìœ¼ë¡œ)

#### íŒŒì¼: `core/services/GraphMAE/GraphMAEConfig.py`

**í˜„ì¬ ìƒíƒœ** (Line 33-34):
```python
encoder_type: str = "gat"
decoder_type: str = "gat"
```

**ë³€ê²½ í›„**:
```python
encoder_type: str = "gcn"
decoder_type: str = "gcn"
```

**ì¶”ê°€ íŒŒë¼ë¯¸í„°**:
```python
# Edge weight ê´€ë ¨ ì„¤ì •
edge_weight_normalization: str = "minmax"  # "minmax", "log", "standard", "none"
use_edge_weight: bool = True  # Edge weight ì‚¬ìš© ì—¬ë¶€ (ablation studyìš©)
```

---

### Phase 4: Edge Weight Normalization ì „ëµ ê²°ì •

#### ì˜µì…˜ ë¹„êµ

| ë°©ë²• | ìˆ˜ì‹ | ì¥ì  | ë‹¨ì  | ê¶Œì¥ |
|------|------|------|------|------|
| **Min-Max** | `(w - min) / (max - min)` | í•´ì„ ìš©ì´ [0,1] ë²”ìœ„ | Outlierì— ë¯¼ê° | â­ ê¸°ë³¸ |
| **Log** | `log(w + 1)` | Power-law ë¶„í¬ ì™„í™” | 0 ê·¼ì²˜ ì••ì¶• | ê³µì¶œí˜„ ë¹ˆë„ íŠ¹í™” |
| **Standard** | `(w - Î¼) / Ïƒ` | í†µê³„ì  ì •ê·œí™” | ìŒìˆ˜ ê°’ ê°€ëŠ¥ | GNNì—ëŠ” ë¶€ì í•© |
| **None** | `w` | ì •ë³´ ì†ì‹¤ ì—†ìŒ | í•™ìŠµ ë¶ˆì•ˆì • | ë¹„ê¶Œì¥ |

**ê¶Œì¥ ì „ëµ**:
1. **ê¸°ë³¸ê°’**: Min-Max (í•´ì„ ìš©ì´, ì•ˆì •ì )
2. **Ablation Study**: Log scalingë„ ì‹¤í—˜í•˜ì—¬ ë¹„êµ

---

### Phase 5: Backward Compatibility ë³´ì¥

#### GraphServiceì— ì˜µì…˜ ì¶”ê°€

```python
def wordgraph_to_dgl(
    self,
    word_graph: 'WordGraph',
    node_features: Optional[torch.Tensor] = None,
    use_edge_weight: bool = True,  # ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„°
    edge_weight_norm: str = 'minmax'  # ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„°
):
    """
    WordGraphë¥¼ DGL ê·¸ë˜í”„ë¡œ ë³€í™˜

    Args:
        word_graph: ë³€í™˜í•  WordGraph ê°ì²´
        node_features: ë…¸ë“œ íŠ¹ì„± í…ì„œ
        use_edge_weight: Edge weight ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ True)
        edge_weight_norm: ì •ê·œí™” ë°©ë²• ('minmax', 'log', 'standard', 'none')
    """
    # ... êµ¬í˜„ ...
```

**ë³€ê²½ ì‚¬ìœ **:
- ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„± ìœ ì§€
- Ablation study ì§€ì› (edge weight ìœ ë¬´ ë¹„êµ)

---

## ğŸ§ª ê²€ì¦ ê³„íš

### 1. Unit Test: Edge Weight ë¡œë”© í™•ì¸

```python
# tests/test_edge_weight_integration.py
def test_dgl_graph_has_edge_weights():
    """DGL ê·¸ë˜í”„ì— edge weightê°€ ì˜¬ë°”ë¥´ê²Œ ì¶”ê°€ë˜ëŠ”ì§€ í™•ì¸"""
    word_graph = create_test_word_graph()
    dgl_graph = graph_service.wordgraph_to_dgl(word_graph, use_edge_weight=True)

    assert 'weight' in dgl_graph.edata
    assert dgl_graph.edata['weight'].shape[0] == dgl_graph.num_edges()
    assert torch.all(dgl_graph.edata['weight'] >= 0)
    assert torch.all(dgl_graph.edata['weight'] <= 1)  # Min-Max normalized
```

### 2. Integration Test: GCN Forward Pass

```python
def test_gcn_uses_edge_weights():
    """GCNì´ edge weightë¥¼ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸"""
    dgl_graph = create_test_graph_with_weights()
    gcn_model = GCN(in_dim=128, num_hidden=64, out_dim=64, num_layers=2, ...)

    # Edge weight ìˆì„ ë•Œ
    output_with_weight = gcn_model(dgl_graph, node_features)

    # Edge weight ì œê±°
    dgl_graph.edata.pop('weight')
    output_without_weight = gcn_model(dgl_graph, node_features)

    # ê²°ê³¼ê°€ ë‹¬ë¼ì•¼ í•¨
    assert not torch.allclose(output_with_weight, output_without_weight)
```

### 3. End-to-End Test: GraphMAE í•™ìŠµ

```python
def test_graphmae_with_edge_weights():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ì—ì„œ edge weightê°€ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸"""
    config = GraphMAEConfig(
        encoder_type='gcn',
        decoder_type='gcn',
        use_edge_weight=True,
        edge_weight_normalization='minmax'
    )

    # í•™ìŠµ ì‹¤í–‰
    embeddings = graphmae_service.pretrain_and_extract(word_graph, embed_size=64)

    # ì„ë² ë”©ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
    assert embeddings.shape == (word_graph.num_nodes, 64)
    assert not torch.isnan(embeddings).any()
```

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜ ì„¤ê³„

### Ablation Study í™•ì¥

ê¸°ì¡´ ablation studyì— ë‹¤ìŒ ì‹¤í—˜ ì¶”ê°€:

#### ì‹¤í—˜ 6: Edge Weight Ablation

```python
# AblationService.pyì— ì¶”ê°€
def run_edge_weight_ablation(self, num_runs: int = 5) -> Dict[str, Any]:
    """
    Edge weight ì‚¬ìš© ìœ ë¬´ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ

    ì‹¤í—˜ ì¡°ê±´:
    1. GCN without edge weight (baseline)
    2. GCN with edge weight (Min-Max)
    3. GCN with edge weight (Log scaling)

    ê³ ì • ë³€ìˆ˜:
    - encoder_type: gcn
    - decoder_type: gcn
    - embed_size: 64
    - mask_rate: 0.3
    - epochs: 1000
    """
    results = {}

    # 1. Without edge weight
    config_no_weight = GRACEConfig(
        encoder_type='gcn',
        decoder_type='gcn',
        use_edge_weight=False
    )
    results['no_weight'] = self._run_multiple_experiments(config_no_weight, num_runs)

    # 2. With edge weight (Min-Max)
    config_minmax = GRACEConfig(
        encoder_type='gcn',
        decoder_type='gcn',
        use_edge_weight=True,
        edge_weight_normalization='minmax'
    )
    results['minmax'] = self._run_multiple_experiments(config_minmax, num_runs)

    # 3. With edge weight (Log)
    config_log = GRACEConfig(
        encoder_type='gcn',
        decoder_type='gcn',
        use_edge_weight=True,
        edge_weight_normalization='log'
    )
    results['log'] = self._run_multiple_experiments(config_log, num_runs)

    # í†µê³„ì  ê²€ì •
    stats = self._statistical_comparison(
        baseline=results['no_weight'],
        treatments={
            'minmax': results['minmax'],
            'log': results['log']
        }
    )

    return {
        'results': results,
        'statistics': stats
    }
```

### ì˜ˆìƒ ê²°ê³¼

| Configuration | Silhouette | Davies-Bouldin | NPMI | Improvement |
|---------------|------------|----------------|------|-------------|
| GCN (no weight) | 0.42 Â± 0.03 | 1.25 Â± 0.08 | 0.31 Â± 0.02 | Baseline |
| GCN (Min-Max) | **0.48 Â± 0.02** | **1.12 Â± 0.06** | **0.36 Â± 0.02** | **+14.3%** |
| GCN (Log) | 0.46 Â± 0.03 | 1.18 Â± 0.07 | 0.34 Â± 0.03 | +9.5% |

**í•´ì„**:
- Edge weight ì‚¬ìš© ì‹œ ëª¨ë“  ë©”íŠ¸ë¦­ì—ì„œ ìœ ì˜ë¯¸í•œ ê°œì„ 
- Min-Max normalizationì´ Log scalingë³´ë‹¤ ì•½ê°„ ìš°ìˆ˜
- NPMI ê°œì„ ì´ ê°€ì¥ í¼ (ê³µì¶œí˜„ ì •ë³´ê°€ ì§ì ‘ ë°˜ì˜ë˜ë¯€ë¡œ)

---

## ğŸ”„ ë¡¤ë°± ê³„íš

ë§Œì•½ edge weight í†µí•©ì´ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¤ëŠ” ê²½ìš°:

### ì˜µì…˜ 1: Config ë³€ê²½ìœ¼ë¡œ ì¦‰ì‹œ ë¡¤ë°±
```python
# GraphMAEConfig.py
use_edge_weight: bool = False  # ì¦‰ì‹œ ë¹„í™œì„±í™”
encoder_type: str = "gat"      # GATë¡œ ë³µê·€
decoder_type: str = "gat"
```

### ì˜µì…˜ 2: Git Revert
```bash
git revert <commit_hash>  # Edge weight í†µí•© ì»¤ë°‹ ë˜ëŒë¦¬ê¸°
```

### ì‹¤íŒ¨ ê¸°ì¤€
ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¼ë„ ë°œìƒ ì‹œ ë¡¤ë°± ê³ ë ¤:
1. **í•™ìŠµ ë¶ˆì•ˆì •**: Lossê°€ NaNì´ ë˜ê±°ë‚˜ ë°œì‚°
2. **ì„±ëŠ¥ ì €í•˜**: ëª¨ë“  ë©”íŠ¸ë¦­ì—ì„œ 5% ì´ìƒ í•˜ë½
3. **í†µê³„ì  ìœ ì˜ì„± ì—†ìŒ**: p-value > 0.05

---

## ğŸ“ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: Core Implementation
- [ ] `gcn.py`: Edge weight ì§€ì› ì½”ë“œ ì‘ì„± ë° ì£¼ì„ í•´ì œ
- [ ] `GraphService.py`: `_normalize_edge_weights()` ë©”ì„œë“œ ì¶”ê°€
- [ ] `GraphService.py`: `wordgraph_to_dgl()` ìˆ˜ì • (edge weight ì¶”ê°€)
- [ ] `GraphMAEConfig.py`: `encoder_type`, `decoder_type` ë³€ê²½
- [ ] `GraphMAEConfig.py`: `use_edge_weight`, `edge_weight_normalization` íŒŒë¼ë¯¸í„° ì¶”ê°€

### Phase 2: Testing
- [ ] Unit test: `test_dgl_graph_has_edge_weights()`
- [ ] Unit test: `test_edge_weight_normalization()`
- [ ] Integration test: `test_gcn_uses_edge_weights()`
- [ ] End-to-end test: `test_graphmae_with_edge_weights()`

### Phase 3: Ablation Study
- [ ] `AblationService.py`: `run_edge_weight_ablation()` ì¶”ê°€
- [ ] ì‹¤í—˜ ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘
- [ ] í†µê³„ì  ìœ ì˜ì„± ê²€ì • (t-test, Cohen's d)

### Phase 4: Documentation
- [ ] ì½”ë“œ ì£¼ì„ ì—…ë°ì´íŠ¸ (docstring)
- [ ] `GraphMAE2_Implementation_Report.md` ì—…ë°ì´íŠ¸
- [ ] ë…¼ë¬¸ ì´ˆì•ˆì— edge weight ì •ë‹¹í™” ì„¹ì…˜ ì¶”ê°€

### Phase 5: Validation
- [ ] Baseline (GAT without edge weight) ì¬ì‹¤í–‰
- [ ] New approach (GCN with edge weight) ì‹¤í–‰
- [ ] ì„±ëŠ¥ ë¹„êµ ë° ë¶„ì„
- [ ] ë¡¤ë°± ì—¬ë¶€ ê²°ì •

---

## ğŸ¯ ì„±ê³µ ê¸°ì¤€

### ìµœì†Œ ì„±ê³µ ê¸°ì¤€ (Must-Have)
1. âœ… GCNì´ edge weightë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí•˜ê³  ì‚¬ìš©
2. âœ… í•™ìŠµì´ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´ (Loss < 1.0, no NaN)
3. âœ… ìµœì†Œ í•˜ë‚˜ì˜ ë©”íŠ¸ë¦­ì—ì„œ 5% ì´ìƒ ê°œì„ 

### ì´ìƒì  ì„±ê³µ ê¸°ì¤€ (Should-Have)
1. âœ… ëª¨ë“  ë©”íŠ¸ë¦­(Silhouette, Davies-Bouldin, NPMI)ì—ì„œ ê°œì„ 
2. âœ… í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ê°œì„  (p < 0.05, Cohen's d > 0.5)
3. âœ… NPMIì—ì„œ 10% ì´ìƒ ê°œì„  (ê³µì¶œí˜„ ì •ë³´ í™œìš© íš¨ê³¼)

### ì¶”ê°€ ê°€ì¹˜ (Nice-to-Have)
1. âœ… ì „í†µì  í´ëŸ¬ìŠ¤í„°ë§(Louvain, Leiden) ëŒ€ë¹„ ìš°ìœ„ í™•ë³´
2. âœ… Computational cost ì¦ê°€ < 20%
3. âœ… ë‹¤ì–‘í•œ ì •ê·œí™” ë°©ë²• ê°„ trade-off ë¶„ì„

---

## ğŸ“… ì˜ˆìƒ ì¼ì •

| Phase | ì‘ì—… | ì˜ˆìƒ ì‹œê°„ | ë‹´ë‹¹ |
|-------|------|----------|------|
| **Day 1** | Core Implementation | 4h | Claude + ì‚¬ìš©ì |
| **Day 2** | Testing & Debugging | 3h | Claude + ì‚¬ìš©ì |
| **Day 3** | Ablation Study ì‹¤í–‰ | 6h | ìë™ ì‹¤í–‰ |
| **Day 4** | ê²°ê³¼ ë¶„ì„ ë° ë¬¸ì„œí™” | 2h | ì‚¬ìš©ì |
| **Day 5** | ê²€ì¦ ë° ë¡¤ë°± ê²°ì • | 1h | ì‚¬ìš©ì |
| **Total** | | **16h** | |

---

## ğŸš¨ ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘ ë°©ì•ˆ

### Risk 1: í•™ìŠµ ë¶ˆì•ˆì • (Loss ë°œì‚°)
**ì›ì¸**: Edge weight ë²”ìœ„ê°€ ë„ˆë¬´ í¼
**ëŒ€ì‘**:
- Min-Max normalization ì ìš©
- Learning rate ê°ì†Œ (0.001 â†’ 0.0005)
- Gradient clipping ì¶”ê°€

### Risk 2: ì„±ëŠ¥ ì €í•˜
**ì›ì¸**: Edge weight noiseê°€ ì‹ í˜¸ë³´ë‹¤ í¼
**ëŒ€ì‘**:
- Log scaling ì‹œë„ (power-law ì™„í™”)
- Edge pruning (ë‚®ì€ ê°€ì¤‘ì¹˜ ì œê±°)
- GATì™€ GCN ì•™ìƒë¸” ê³ ë ¤

### Risk 3: ê³„ì‚° ë¹„ìš© ì¦ê°€
**ì›ì¸**: Edge weight ì²˜ë¦¬ overhead
**ëŒ€ì‘**:
- Sparse matrix ìµœì í™” í™œìš©
- Batch size ì¡°ì •
- Early stopping ì ìš©

---

## ğŸ“š ì°¸ê³  ë¬¸í—Œ

### ì´ë¡ ì  ë°°ê²½
1. **GCN ì›ë…¼ë¬¸**: Kipf & Welling (2017). "Semi-Supervised Classification with Graph Convolutional Networks." ICLR.
2. **GraphMAE2**: Hou et al. (2022). "GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner." WWW.
3. **TextGCN**: Yao et al. (2019). "Graph Convolutional Networks for Text Classification." AAAI.

### êµ¬í˜„ ì°¸ê³ 
1. DGL Documentation: [Edge Weight in Message Passing](https://docs.dgl.ai/guide/message-passing.html#edge-weight-normalization)
2. PyTorch Geometric: [GCNConv with edge_weight](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv)

---

## ğŸ’¡ ì¶”ê°€ ê°œì„  ì•„ì´ë””ì–´ (Future Work)

### 1. Learnable Edge Weight
```python
# Edge weightë¥¼ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¡œ
edge_weight_mlp = nn.Sequential(
    nn.Linear(1, 16),
    nn.ReLU(),
    nn.Linear(16, 1),
    nn.Sigmoid()
)
learned_weights = edge_weight_mlp(original_weights)
```

### 2. Attention + Edge Weight ê²°í•©
```python
# GATì˜ attention scoreì™€ edge weightë¥¼ ê²°í•©
final_score = alpha * attention_score + (1 - alpha) * edge_weight
```

### 3. Edge Feature (Multi-dimensional)
```python
# ê³µì¶œí˜„ ë¹ˆë„ ì™¸ì— ì¶”ê°€ ì—£ì§€ íŠ¹ì„± ì‚¬ìš©
edge_features = [co_occurrence, pmi, cosine_similarity]  # 3ì°¨ì›
```

---

**ì‘ì„±ì**: Claude (Anthropic)
**ì‘ì„±ì¼**: 2025-10-17
**ë²„ì „**: 1.0
**ê²€í†  í•„ìš”**: ì‚¬ìš©ì ìŠ¹ì¸ í›„ êµ¬í˜„ ì‹œì‘
