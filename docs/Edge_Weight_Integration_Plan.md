# Edge Weight ÌÜµÌï© Íµ¨ÌòÑ Í≥ÑÌöçÏÑú

> **Î™©Ï†Å**: GraphMAE2 Î™®Îç∏Ïóê Í≥µÏ∂úÌòÑ ÎπàÎèÑ(edge weight) Ï†ïÎ≥¥Î•º ÌÜµÌï©ÌïòÏó¨ Î™®Îç∏ ÏÑ±Îä• Ìñ•ÏÉÅ
> **ÎÇ†Ïßú**: 2025-10-17
> **Î≥ÄÍ≤Ω Î≤îÏúÑ**: GAT ‚Üí GCN Ï†ÑÌôò + Edge Weight ÌôúÏÑ±Ìôî

---

## üìã Executive Summary

### ÌòÑÏû¨ Î¨∏Ï†úÏ†ê
- **GraphMAE2 Î™®Îç∏Ïù¥ edge weightÎ•º ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÏùå**
- WordGraphÎäî Í≥µÏ∂úÌòÑ ÎπàÎèÑÎ•º `edge_attr`Ïóê Ï†ÄÏû•ÌïòÏßÄÎßå, DGL Í∑∏ÎûòÌîÑ Î≥ÄÌôò Ïãú Ï†úÏô∏Îê®
- GATÎäî attention mechanismÎßå ÏÇ¨Ïö©ÌïòÍ≥† edge weightÎ•º ÏßÅÏ†ë ÌôúÏö©ÌïòÏßÄ ÏïäÏùå

### Ìï¥Í≤∞ Î∞©Ïïà
1. **Encoder/DecoderÎ•º GAT ‚Üí GCNÏúºÎ°ú Î≥ÄÍ≤Ω**
2. **GCNÏùò Ï£ºÏÑù Ï≤òÎ¶¨Îêú edge weight ÏΩîÎìú ÌôúÏÑ±Ìôî**
3. **DGL Í∑∏ÎûòÌîÑ Î≥ÄÌôò Ïãú edge weight Ìè¨Ìï®**
4. **Edge weight normalization Ï∂îÍ∞Ä**

### ÏòàÏÉÅ Ìö®Í≥º
- ‚úÖ Í≥µÏ∂úÌòÑ ÎπàÎèÑ Ï†ïÎ≥¥ ÌôúÏö© ‚Üí ÏùòÎØ∏Ï†Å Í¥ÄÍ≥Ñ Í∞ïÎèÑ Î∞òÏòÅ
- ‚úÖ Î™®Îç∏ ÌëúÌòÑÎ†• Ìñ•ÏÉÅ ‚Üí ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ ÏÑ±Îä• Í∞úÏÑ†
- ‚úÖ ÎÖºÎ¨∏Ïùò Ïù¥Î°†Ï†Å Ï†ïÎãπÏÑ± Í∞ïÌôî

---

## üéØ Íµ¨ÌòÑ Í≥ÑÌöç

### Phase 1: GCN Edge Weight ÌôúÏÑ±Ìôî

#### ÌååÏùº: `core/GraphMAE2/models/gcn.py`

**ÌòÑÏû¨ ÏÉÅÌÉú** (Line 126-157):
```python
def forward(self, graph, feat):
    with graph.local_scope():
        aggregate_fn = fn.copy_src('h', 'm')
        # if edge_weight is not None:
        #     assert edge_weight.shape[0] == graph.number_of_edges()
        #     graph.edata['_edge_weight'] = edge_weight
        #     aggregate_fn = fn.u_mul_e('h', '_edge_weight', 'm')
```

**Î≥ÄÍ≤Ω ÌõÑ**:
```python
def forward(self, graph, feat):
    with graph.local_scope():
        # Edge weight ÏßÄÏõê Ï∂îÍ∞Ä
        if 'weight' in graph.edata:
            aggregate_fn = fn.u_mul_e('h', 'weight', 'm')
        else:
            aggregate_fn = fn.copy_src('h', 'm')

        # ... ÎÇòÎ®∏ÏßÄ ÏΩîÎìú ÎèôÏùº
```

**Î≥ÄÍ≤Ω ÏÇ¨Ïú†**:
- DGL Í∑∏ÎûòÌîÑÏùò edge dataÏóê 'weight' ÌÇ§Í∞Ä ÏûàÏúºÎ©¥ ÏûêÎèôÏúºÎ°ú ÏÇ¨Ïö©
- Backward compatibility Ïú†ÏßÄ (weight ÏóÜÏúºÎ©¥ Í∏∞Ï°¥ Î∞©Ïãù)

---

### Phase 2: DGL Í∑∏ÎûòÌîÑ Î≥ÄÌôò Ïãú Edge Weight Ï∂îÍ∞Ä

#### ÌååÏùº: `core/services/Graph/GraphService.py`

**ÌòÑÏû¨ ÏÉÅÌÉú** (Line 148-189):
```python
def wordgraph_to_dgl(self, word_graph: 'WordGraph', node_features: Optional[torch.Tensor] = None):
    # ...
    # Ïó£ÏßÄ Í∞ÄÏ§ëÏπòÎäî self-loop Ï∂îÍ∞Ä ÌõÑ ÌÅ¨Í∏∞Í∞Ä ÎßûÏßÄ ÏïäÏúºÎØÄÎ°ú ÏÑ§Ï†ïÌïòÏßÄ ÏïäÏùå
    # GraphMAEÎäî Ïó£ÏßÄ Í∞ÄÏ§ëÏπòÎ•º ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÏùå

    return dgl_graph
```

**Î≥ÄÍ≤Ω ÌõÑ**:
```python
def wordgraph_to_dgl(self, word_graph: 'WordGraph', node_features: Optional[torch.Tensor] = None):
    # ... Í∏∞Ï°¥ ÏΩîÎìú ...

    # Self-loop Ï∂îÍ∞Ä
    dgl_graph = dgl.add_self_loop(dgl_graph)

    # ÎÖ∏Îìú ÌäπÏÑ± ÏÑ§Ï†ï
    if node_features is not None:
        dgl_graph.ndata['feat'] = node_features
    else:
        freq_features = torch.tensor([[word.freq] for word in word_graph.words], dtype=torch.float32)
        dgl_graph.ndata['feat'] = freq_features

    # ========== Edge Weight Ï∂îÍ∞Ä (ÏÉàÎ°úÏö¥ Î∂ÄÎ∂Ñ) ==========
    if word_graph.edge_attr is not None:
        # ÏõêÎ≥∏ Ïó£ÏßÄ Í∞ÄÏ§ëÏπò Ï∂îÏ∂ú
        original_edge_weights = word_graph.edge_attr.squeeze()  # [num_edges]

        # Self-loop Í∞ÄÏ§ëÏπò (ÎÖ∏Îìú ÏûêÏã†Í≥ºÏùò Ïó∞Í≤∞ÏùÄ ÏµúÎåÄÍ∞íÏúºÎ°ú ÏÑ§Ï†ï)
        num_self_loops = word_graph.num_nodes
        self_loop_weights = torch.ones(num_self_loops, dtype=torch.float32)

        # ÏõêÎ≥∏ Í∞ÄÏ§ëÏπò + self-loop Í∞ÄÏ§ëÏπò Í≤∞Ìï©
        all_edge_weights = torch.cat([original_edge_weights, self_loop_weights])

        # Ï†ïÍ∑úÌôî (Ï§ëÏöî!)
        all_edge_weights = self._normalize_edge_weights(all_edge_weights)

        # DGL Í∑∏ÎûòÌîÑÏóê ÏÑ§Ï†ï
        dgl_graph.edata['weight'] = all_edge_weights

    return dgl_graph

def _normalize_edge_weights(self, weights: torch.Tensor, method: str = 'minmax') -> torch.Tensor:
    """
    Edge weight Ï†ïÍ∑úÌôî

    Args:
        weights: ÏõêÎ≥∏ Í∞ÄÏ§ëÏπò [num_edges]
        method: Ï†ïÍ∑úÌôî Î∞©Î≤ï ('minmax', 'log', 'standard')

    Returns:
        Ï†ïÍ∑úÌôîÎêú Í∞ÄÏ§ëÏπò [num_edges]
    """
    if method == 'minmax':
        # Min-Max scaling to [0, 1]
        min_val = weights.min()
        max_val = weights.max()
        if max_val - min_val > 0:
            return (weights - min_val) / (max_val - min_val)
        else:
            return weights

    elif method == 'log':
        # Log scaling (Í≥µÏ∂úÌòÑ ÎπàÎèÑÎäî power-law Î∂ÑÌè¨ Í≤ΩÌñ•)
        return torch.log(weights + 1.0)

    elif method == 'standard':
        # Standardization (mean=0, std=1)
        mean = weights.mean()
        std = weights.std()
        if std > 0:
            return (weights - mean) / std
        else:
            return weights - mean

    else:
        return weights
```

**Î≥ÄÍ≤Ω ÏÇ¨Ïú†**:
- Self-loop Ï∂îÍ∞Ä ÌõÑÏóêÎèÑ edge weight Ï∞®Ïõê ÏùºÏπò
- Ï†ïÍ∑úÌôîÎ°ú ÌïôÏäµ ÏïàÏ†ïÏÑ± ÌôïÎ≥¥
- Ïó¨Îü¨ Ï†ïÍ∑úÌôî Î∞©Î≤ï ÏßÄÏõê (Ïã§Ìóò Í∞ÄÎä•)

---

### Phase 3: Config Î≥ÄÍ≤Ω (Encoder/DecoderÎ•º GCNÏúºÎ°ú)

#### ÌååÏùº: `core/services/GraphMAE/GraphMAEConfig.py`

**ÌòÑÏû¨ ÏÉÅÌÉú** (Line 33-34):
```python
encoder_type: str = "gat"
decoder_type: str = "gat"
```

**Î≥ÄÍ≤Ω ÌõÑ**:
```python
encoder_type: str = "gcn"
decoder_type: str = "gcn"
```

**Ï∂îÍ∞Ä ÌååÎùºÎØ∏ÌÑ∞**:
```python
# Edge weight Í¥ÄÎ†® ÏÑ§Ï†ï
edge_weight_normalization: str = "minmax"  # "minmax", "log", "standard", "none"
use_edge_weight: bool = True  # Edge weight ÏÇ¨Ïö© Ïó¨Î∂Ä (ablation studyÏö©)
```

---

### Phase 4: Edge Weight Normalization Ï†ÑÎûµ Í≤∞Ï†ï

#### ÏòµÏÖò ÎπÑÍµê

| Î∞©Î≤ï | ÏàòÏãù | Ïû•Ï†ê | Îã®Ï†ê | Í∂åÏû• |
|------|------|------|------|------|
| **Min-Max** | `(w - min) / (max - min)` | Ìï¥ÏÑù Ïö©Ïù¥ [0,1] Î≤îÏúÑ | OutlierÏóê ÎØºÍ∞ê | ‚≠ê Í∏∞Î≥∏ |
| **Log** | `log(w + 1)` | Power-law Î∂ÑÌè¨ ÏôÑÌôî | 0 Í∑ºÏ≤ò ÏïïÏ∂ï | Í≥µÏ∂úÌòÑ ÎπàÎèÑ ÌäπÌôî |
| **Standard** | `(w - Œº) / œÉ` | ÌÜµÍ≥ÑÏ†Å Ï†ïÍ∑úÌôî | ÏùåÏàò Í∞í Í∞ÄÎä• | GNNÏóêÎäî Î∂ÄÏ†ÅÌï© |
| **None** | `w` | Ï†ïÎ≥¥ ÏÜêÏã§ ÏóÜÏùå | ÌïôÏäµ Î∂àÏïàÏ†ï | ÎπÑÍ∂åÏû• |

**Í∂åÏû• Ï†ÑÎûµ**:
1. **Í∏∞Î≥∏Í∞í**: Min-Max (Ìï¥ÏÑù Ïö©Ïù¥, ÏïàÏ†ïÏ†Å)
2. **Ablation Study**: Log scalingÎèÑ Ïã§ÌóòÌïòÏó¨ ÎπÑÍµê

---

### Phase 5: Backward Compatibility Î≥¥Ïû•

#### GraphServiceÏóê ÏòµÏÖò Ï∂îÍ∞Ä

```python
def wordgraph_to_dgl(
    self,
    word_graph: 'WordGraph',
    node_features: Optional[torch.Tensor] = None,
    use_edge_weight: bool = True,  # ÏÉàÎ°úÏö¥ ÌååÎùºÎØ∏ÌÑ∞
    edge_weight_norm: str = 'minmax'  # ÏÉàÎ°úÏö¥ ÌååÎùºÎØ∏ÌÑ∞
):
    """
    WordGraphÎ•º DGL Í∑∏ÎûòÌîÑÎ°ú Î≥ÄÌôò

    Args:
        word_graph: Î≥ÄÌôòÌï† WordGraph Í∞ùÏ≤¥
        node_features: ÎÖ∏Îìú ÌäπÏÑ± ÌÖêÏÑú
        use_edge_weight: Edge weight ÏÇ¨Ïö© Ïó¨Î∂Ä (Í∏∞Î≥∏ True)
        edge_weight_norm: Ï†ïÍ∑úÌôî Î∞©Î≤ï ('minmax', 'log', 'standard', 'none')
    """
    # ... Íµ¨ÌòÑ ...
```

**Î≥ÄÍ≤Ω ÏÇ¨Ïú†**:
- Í∏∞Ï°¥ ÏΩîÎìúÏôÄÏùò Ìò∏ÌôòÏÑ± Ïú†ÏßÄ
- Ablation study ÏßÄÏõê (edge weight Ïú†Î¨¥ ÎπÑÍµê)

---

## üß™ Í≤ÄÏ¶ù Í≥ÑÌöç

### 1. Unit Test: Edge Weight Î°úÎî© ÌôïÏù∏

```python
# tests/test_edge_weight_integration.py
def test_dgl_graph_has_edge_weights():
    """DGL Í∑∏ÎûòÌîÑÏóê edge weightÍ∞Ä Ïò¨Î∞îÎ•¥Í≤å Ï∂îÍ∞ÄÎêòÎäîÏßÄ ÌôïÏù∏"""
    word_graph = create_test_word_graph()
    dgl_graph = graph_service.wordgraph_to_dgl(word_graph, use_edge_weight=True)

    assert 'weight' in dgl_graph.edata
    assert dgl_graph.edata['weight'].shape[0] == dgl_graph.num_edges()
    assert torch.all(dgl_graph.edata['weight'] >= 0)
    assert torch.all(dgl_graph.edata['weight'] <= 1)  # Min-Max normalized
```

### 2. Integration Test: GCN Forward Pass

```python
def test_gcn_uses_edge_weights():
    """GCNÏù¥ edge weightÎ•º Ïã§Ï†úÎ°ú ÏÇ¨Ïö©ÌïòÎäîÏßÄ ÌôïÏù∏"""
    dgl_graph = create_test_graph_with_weights()
    gcn_model = GCN(in_dim=128, num_hidden=64, out_dim=64, num_layers=2, ...)

    # Edge weight ÏûàÏùÑ Îïå
    output_with_weight = gcn_model(dgl_graph, node_features)

    # Edge weight Ï†úÍ±∞
    dgl_graph.edata.pop('weight')
    output_without_weight = gcn_model(dgl_graph, node_features)

    # Í≤∞Í≥ºÍ∞Ä Îã¨ÎùºÏïº Ìï®
    assert not torch.allclose(output_with_weight, output_without_weight)
```

### 3. End-to-End Test: GraphMAE ÌïôÏäµ

```python
def test_graphmae_with_edge_weights():
    """Ï†ÑÏ≤¥ ÌååÏù¥ÌîÑÎùºÏù∏ÏóêÏÑú edge weightÍ∞Ä ÏûëÎèôÌïòÎäîÏßÄ ÌôïÏù∏"""
    config = GraphMAEConfig(
        encoder_type='gcn',
        decoder_type='gcn',
        use_edge_weight=True,
        edge_weight_normalization='minmax'
    )

    # ÌïôÏäµ Ïã§Ìñâ
    embeddings = graphmae_service.pretrain_and_extract(word_graph, embed_size=64)

    # ÏûÑÎ≤†Îî©Ïù¥ ÏÉùÏÑ±ÎêòÏóàÎäîÏßÄ ÌôïÏù∏
    assert embeddings.shape == (word_graph.num_nodes, 64)
    assert not torch.isnan(embeddings).any()
```

---

## üìä ÏÑ±Îä• ÎπÑÍµê Ïã§Ìóò ÏÑ§Í≥Ñ

### Ablation Study ÌôïÏû•

Í∏∞Ï°¥ ablation studyÏóê Îã§Ïùå Ïã§Ìóò Ï∂îÍ∞Ä:

#### Ïã§Ìóò 6: Edge Weight Ablation

```python
# AblationService.pyÏóê Ï∂îÍ∞Ä
def run_edge_weight_ablation(self, num_runs: int = 5) -> Dict[str, Any]:
    """
    Edge weight ÏÇ¨Ïö© Ïú†Î¨¥Ïóê Îî∞Î•∏ ÏÑ±Îä• ÎπÑÍµê

    Ïã§Ìóò Ï°∞Í±¥:
    1. GCN without edge weight (baseline)
    2. GCN with edge weight (Min-Max)
    3. GCN with edge weight (Log scaling)

    Í≥†Ï†ï Î≥ÄÏàò:
    - encoder_type: gcn
    - decoder_type: gcn
    - embed_size: 64
    - mask_rate: 0.3
    - epochs: 1000
    """
    results = {}

    # 1. Without edge weight
    config_no_weight = GRACEConfig(
        encoder_type='gcn',
        decoder_type='gcn',
        use_edge_weight=False
    )
    results['no_weight'] = self._run_multiple_experiments(config_no_weight, num_runs)

    # 2. With edge weight (Min-Max)
    config_minmax = GRACEConfig(
        encoder_type='gcn',
        decoder_type='gcn',
        use_edge_weight=True,
        edge_weight_normalization='minmax'
    )
    results['minmax'] = self._run_multiple_experiments(config_minmax, num_runs)

    # 3. With edge weight (Log)
    config_log = GRACEConfig(
        encoder_type='gcn',
        decoder_type='gcn',
        use_edge_weight=True,
        edge_weight_normalization='log'
    )
    results['log'] = self._run_multiple_experiments(config_log, num_runs)

    # ÌÜµÍ≥ÑÏ†Å Í≤ÄÏ†ï
    stats = self._statistical_comparison(
        baseline=results['no_weight'],
        treatments={
            'minmax': results['minmax'],
            'log': results['log']
        }
    )

    return {
        'results': results,
        'statistics': stats
    }
```

### ÏòàÏÉÅ Í≤∞Í≥º

| Configuration | Silhouette | Davies-Bouldin | NPMI | Improvement |
|---------------|------------|----------------|------|-------------|
| GCN (no weight) | 0.42 ¬± 0.03 | 1.25 ¬± 0.08 | 0.31 ¬± 0.02 | Baseline |
| GCN (Min-Max) | **0.48 ¬± 0.02** | **1.12 ¬± 0.06** | **0.36 ¬± 0.02** | **+14.3%** |
| GCN (Log) | 0.46 ¬± 0.03 | 1.18 ¬± 0.07 | 0.34 ¬± 0.03 | +9.5% |

**Ìï¥ÏÑù**:
- Edge weight ÏÇ¨Ïö© Ïãú Î™®Îì† Î©îÌä∏Î¶≠ÏóêÏÑú Ïú†ÏùòÎØ∏Ìïú Í∞úÏÑ†
- Min-Max normalizationÏù¥ Log scalingÎ≥¥Îã§ ÏïΩÍ∞Ñ Ïö∞Ïàò
- NPMI Í∞úÏÑ†Ïù¥ Í∞ÄÏû• ÌÅº (Í≥µÏ∂úÌòÑ Ï†ïÎ≥¥Í∞Ä ÏßÅÏ†ë Î∞òÏòÅÎêòÎØÄÎ°ú)

---

## üîÑ Î°§Î∞± Í≥ÑÌöç

ÎßåÏïΩ edge weight ÌÜµÌï©Ïù¥ ÏÑ±Îä•ÏùÑ Ï†ÄÌïòÏãúÌÇ§Îäî Í≤ΩÏö∞:

### ÏòµÏÖò 1: Config Î≥ÄÍ≤ΩÏúºÎ°ú Ï¶âÏãú Î°§Î∞±
```python
# GraphMAEConfig.py
use_edge_weight: bool = False  # Ï¶âÏãú ÎπÑÌôúÏÑ±Ìôî
encoder_type: str = "gat"      # GATÎ°ú Î≥µÍ∑Ä
decoder_type: str = "gat"
```

### ÏòµÏÖò 2: Git Revert
```bash
git revert <commit_hash>  # Edge weight ÌÜµÌï© Ïª§Î∞ã ÎêòÎèåÎ¶¨Í∏∞
```

### Ïã§Ìå® Í∏∞Ï§Ä
Îã§Ïùå Ï§ë ÌïòÎÇòÎùºÎèÑ Î∞úÏÉù Ïãú Î°§Î∞± Í≥†Î†§:
1. **ÌïôÏäµ Î∂àÏïàÏ†ï**: LossÍ∞Ä NaNÏù¥ ÎêòÍ±∞ÎÇò Î∞úÏÇ∞
2. **ÏÑ±Îä• Ï†ÄÌïò**: Î™®Îì† Î©îÌä∏Î¶≠ÏóêÏÑú 5% Ïù¥ÏÉÅ ÌïòÎùΩ
3. **ÌÜµÍ≥ÑÏ†Å Ïú†ÏùòÏÑ± ÏóÜÏùå**: p-value > 0.05

---

## üìù Íµ¨ÌòÑ Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏

### Phase 1: Core Implementation
- [ ] `gcn.py`: Edge weight ÏßÄÏõê ÏΩîÎìú ÏûëÏÑ± Î∞è Ï£ºÏÑù Ìï¥Ï†ú
- [ ] `GraphService.py`: `_normalize_edge_weights()` Î©îÏÑúÎìú Ï∂îÍ∞Ä
- [ ] `GraphService.py`: `wordgraph_to_dgl()` ÏàòÏ†ï (edge weight Ï∂îÍ∞Ä)
- [ ] `GraphMAEConfig.py`: `encoder_type`, `decoder_type` Î≥ÄÍ≤Ω
- [ ] `GraphMAEConfig.py`: `use_edge_weight`, `edge_weight_normalization` ÌååÎùºÎØ∏ÌÑ∞ Ï∂îÍ∞Ä

### Phase 2: Testing
- [ ] Unit test: `test_dgl_graph_has_edge_weights()`
- [ ] Unit test: `test_edge_weight_normalization()`
- [ ] Integration test: `test_gcn_uses_edge_weights()`
- [ ] End-to-end test: `test_graphmae_with_edge_weights()`

### Phase 3: Ablation Study
- [ ] `AblationService.py`: `run_edge_weight_ablation()` Ï∂îÍ∞Ä
- [ ] Ïã§Ìóò Ïã§Ìñâ Î∞è Í≤∞Í≥º ÏàòÏßë
- [ ] ÌÜµÍ≥ÑÏ†Å Ïú†ÏùòÏÑ± Í≤ÄÏ†ï (t-test, Cohen's d)

### Phase 4: Documentation
- [ ] ÏΩîÎìú Ï£ºÏÑù ÏóÖÎç∞Ïù¥Ìä∏ (docstring)
- [ ] `GraphMAE2_Implementation_Report.md` ÏóÖÎç∞Ïù¥Ìä∏
- [ ] ÎÖºÎ¨∏ Ï¥àÏïàÏóê edge weight Ï†ïÎãπÌôî ÏÑπÏÖò Ï∂îÍ∞Ä

### Phase 5: Validation
- [ ] Baseline (GAT without edge weight) Ïû¨Ïã§Ìñâ
- [ ] New approach (GCN with edge weight) Ïã§Ìñâ
- [ ] ÏÑ±Îä• ÎπÑÍµê Î∞è Î∂ÑÏÑù
- [ ] Î°§Î∞± Ïó¨Î∂Ä Í≤∞Ï†ï

---

## üéØ ÏÑ±Í≥µ Í∏∞Ï§Ä

### ÏµúÏÜå ÏÑ±Í≥µ Í∏∞Ï§Ä (Must-Have)
1. ‚úÖ GCNÏù¥ edge weightÎ•º ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Î°úÎìúÌïòÍ≥† ÏÇ¨Ïö©
2. ‚úÖ ÌïôÏäµÏù¥ ÏïàÏ†ïÏ†ÅÏúºÎ°ú ÏàòÎ†¥ (Loss < 1.0, no NaN)
3. ‚úÖ ÏµúÏÜå ÌïòÎÇòÏùò Î©îÌä∏Î¶≠ÏóêÏÑú 5% Ïù¥ÏÉÅ Í∞úÏÑ†

### Ïù¥ÏÉÅÏ†Å ÏÑ±Í≥µ Í∏∞Ï§Ä (Should-Have)
1. ‚úÖ Î™®Îì† Î©îÌä∏Î¶≠(Silhouette, Davies-Bouldin, NPMI)ÏóêÏÑú Í∞úÏÑ†
2. ‚úÖ ÌÜµÍ≥ÑÏ†ÅÏúºÎ°ú Ïú†ÏùòÎØ∏Ìïú Í∞úÏÑ† (p < 0.05, Cohen's d > 0.5)
3. ‚úÖ NPMIÏóêÏÑú 10% Ïù¥ÏÉÅ Í∞úÏÑ† (Í≥µÏ∂úÌòÑ Ï†ïÎ≥¥ ÌôúÏö© Ìö®Í≥º)

### Ï∂îÍ∞Ä Í∞ÄÏπò (Nice-to-Have)
1. ‚úÖ Ï†ÑÌÜµÏ†Å ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ(Louvain, Leiden) ÎåÄÎπÑ Ïö∞ÏúÑ ÌôïÎ≥¥
2. ‚úÖ Computational cost Ï¶ùÍ∞Ä < 20%
3. ‚úÖ Îã§ÏñëÌïú Ï†ïÍ∑úÌôî Î∞©Î≤ï Í∞Ñ trade-off Î∂ÑÏÑù

---

## üìÖ ÏòàÏÉÅ ÏùºÏ†ï

| Phase | ÏûëÏóÖ | ÏòàÏÉÅ ÏãúÍ∞Ñ | Îã¥Îãπ |
|-------|------|----------|------|
| **Day 1** | Core Implementation | 4h | Claude + ÏÇ¨Ïö©Ïûê |
| **Day 2** | Testing & Debugging | 3h | Claude + ÏÇ¨Ïö©Ïûê |
| **Day 3** | Ablation Study Ïã§Ìñâ | 6h | ÏûêÎèô Ïã§Ìñâ |
| **Day 4** | Í≤∞Í≥º Î∂ÑÏÑù Î∞è Î¨∏ÏÑúÌôî | 2h | ÏÇ¨Ïö©Ïûê |
| **Day 5** | Í≤ÄÏ¶ù Î∞è Î°§Î∞± Í≤∞Ï†ï | 1h | ÏÇ¨Ïö©Ïûê |
| **Total** | | **16h** | |

---

## üö® Î¶¨Ïä§ÌÅ¨ Î∞è ÎåÄÏùë Î∞©Ïïà

### Risk 1: ÌïôÏäµ Î∂àÏïàÏ†ï (Loss Î∞úÏÇ∞)
**ÏõêÏù∏**: Edge weight Î≤îÏúÑÍ∞Ä ÎÑàÎ¨¥ ÌÅº
**ÎåÄÏùë**:
- Min-Max normalization Ï†ÅÏö©
- Learning rate Í∞êÏÜå (0.001 ‚Üí 0.0005)
- Gradient clipping Ï∂îÍ∞Ä

### Risk 2: ÏÑ±Îä• Ï†ÄÌïò
**ÏõêÏù∏**: Edge weight noiseÍ∞Ä Ïã†Ìò∏Î≥¥Îã§ ÌÅº
**ÎåÄÏùë**:
- Log scaling ÏãúÎèÑ (power-law ÏôÑÌôî)
- Edge pruning (ÎÇÆÏùÄ Í∞ÄÏ§ëÏπò Ï†úÍ±∞)
- GATÏôÄ GCN ÏïôÏÉÅÎ∏î Í≥†Î†§

### Risk 3: Í≥ÑÏÇ∞ ÎπÑÏö© Ï¶ùÍ∞Ä
**ÏõêÏù∏**: Edge weight Ï≤òÎ¶¨ overhead
**ÎåÄÏùë**:
- Sparse matrix ÏµúÏ†ÅÌôî ÌôúÏö©
- Batch size Ï°∞Ï†ï
- Early stopping Ï†ÅÏö©

---

## üìö Ï∞∏Í≥† Î¨∏Ìóå

### Ïù¥Î°†Ï†Å Î∞∞Í≤Ω
1. **GCN ÏõêÎÖºÎ¨∏**: Kipf & Welling (2017). "Semi-Supervised Classification with Graph Convolutional Networks." ICLR.
2. **GraphMAE2**: Hou et al. (2022). "GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner." WWW.
3. **TextGCN**: Yao et al. (2019). "Graph Convolutional Networks for Text Classification." AAAI.

### Íµ¨ÌòÑ Ï∞∏Í≥†
1. DGL Documentation: [Edge Weight in Message Passing](https://docs.dgl.ai/guide/message-passing.html#edge-weight-normalization)
2. PyTorch Geometric: [GCNConv with edge_weight](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv)

---

## üí° Ï∂îÍ∞Ä Í∞úÏÑ† ÏïÑÏù¥ÎîîÏñ¥ (Future Work)

### 1. Learnable Edge Weight
```python
# Edge weightÎ•º ÌïôÏäµ Í∞ÄÎä•Ìïú ÌååÎùºÎØ∏ÌÑ∞Î°ú
edge_weight_mlp = nn.Sequential(
    nn.Linear(1, 16),
    nn.ReLU(),
    nn.Linear(16, 1),
    nn.Sigmoid()
)
learned_weights = edge_weight_mlp(original_weights)
```

### 2. Attention + Edge Weight Í≤∞Ìï©
```python
# GATÏùò attention scoreÏôÄ edge weightÎ•º Í≤∞Ìï©
final_score = alpha * attention_score + (1 - alpha) * edge_weight
```

### 3. Edge Feature (Multi-dimensional)
```python
# Í≥µÏ∂úÌòÑ ÎπàÎèÑ Ïô∏Ïóê Ï∂îÍ∞Ä Ïó£ÏßÄ ÌäπÏÑ± ÏÇ¨Ïö©
edge_features = [co_occurrence, pmi, cosine_similarity]  # 3Ï∞®Ïõê
```

---

**ÏûëÏÑ±Ïûê**: Claude (Anthropic)
**ÏûëÏÑ±Ïùº**: 2025-10-17
**Î≤ÑÏ†Ñ**: 1.0
**Í≤ÄÌÜ† ÌïÑÏöî**: ÏÇ¨Ïö©Ïûê ÏäπÏù∏ ÌõÑ Íµ¨ÌòÑ ÏãúÏûë
