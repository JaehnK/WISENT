"""
GRACE (GRAph-based Clustering with Enhanced embeddings) Pipeline

ì „ì²´ íŒŒì´í”„ë¼ì¸:
1. ë°ì´í„° ì „ì²˜ë¦¬ (DocumentService)
2. ì˜ë¯¸ì—°ê²°ë§ êµ¬ì¶• (GraphService)
3. ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© (Word2Vec + BERT)
4. GraphMAE ìê¸°ì§€ë„í•™ìŠµ
5. í´ëŸ¬ìŠ¤í„°ë§ (K-means, DBSCAN ë“±)
6. í‰ê°€ ë° ê²°ê³¼ ë¶„ì„
"""

import os
import sys
import pandas as pd
import torch
import numpy as np
from typing import List, Dict, Optional, Tuple, Any
from pathlib import Path
import json
from datetime import datetime

# ì„œë¹„ìŠ¤ import
from ..Document.DocumentService import DocumentService
from ..Graph.GraphService import GraphService
from ..Graph.NodeFeatureHandler import NodeFeatureHandler
from ..GraphMAE import GraphMAEService, GraphMAEConfig
from entities import Word, WordGraph, NodeFeatureType

from .GRACEConfig import GRACEConfig
from .ClusteringService import ClusteringService
from .MetricsService import MetricsService


class GRACEPipeline:
    """GRACE íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""

    def __init__(self, config: GRACEConfig):
        """
        Args:
            config: GRACE íŒŒì´í”„ë¼ì¸ ì„¤ì •
        """
        self.config = config

        # ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
        self.doc_service: Optional[DocumentService] = None
        self.graph_service: Optional[GraphService] = None
        self.node_feature_handler: Optional[NodeFeatureHandler] = None
        self.clustering_service = ClusteringService(random_state=42)
        self.metrics_service = MetricsService()

        # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
        self.documents: Optional[List[str]] = None
        self.word_graph: Optional[WordGraph] = None
        self.node_features: Optional[torch.Tensor] = None
        self.graphmae_embeddings: Optional[torch.Tensor] = None
        self.cluster_labels: Optional[np.ndarray] = None
        self.cluster_results: Optional[Dict[str, Any]] = None

        # ë©”íƒ€ë°ì´í„°
        self.pipeline_start_time: Optional[datetime] = None
        self.pipeline_end_time: Optional[datetime] = None

    # ============================================================
    # ë©”ì¸ ì‹¤í–‰ ë©”ì„œë“œ
    # ============================================================

    def run(self) -> Dict[str, Any]:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Returns:
            ìµœì¢… ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        self.pipeline_start_time = datetime.now()
        self._log("ğŸš€ GRACE íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        self._log("=" * 80)

        try:
            # 1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
            self._log("\n[1/6] ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬")
            self.load_and_preprocess()

            # 2. ì˜ë¯¸ì—°ê²°ë§ êµ¬ì¶•
            self._log("\n[2/6] ì˜ë¯¸ì—°ê²°ë§ êµ¬ì¶•")
            self.build_semantic_network()

            # 3. ë©€í‹°ëª¨ë‹¬ ë…¸ë“œ íŠ¹ì„± ê³„ì‚°
            self._log("\n[3/6] ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ê³„ì‚°")
            self.compute_node_features()

            # 4. GraphMAE ìê¸°ì§€ë„í•™ìŠµ
            self._log("\n[4/6] GraphMAE ìê¸°ì§€ë„í•™ìŠµ")
            self.train_graphmae()

            # 5. í´ëŸ¬ìŠ¤í„°ë§
            self._log("\n[5/6] í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰")
            self.perform_clustering()

            # 6. í‰ê°€ ë° ê²°ê³¼ ì €ì¥
            self._log("\n[6/6] í‰ê°€ ë° ê²°ê³¼ ì €ì¥")
            results = self.evaluate_and_save()

            self.pipeline_end_time = datetime.now()
            elapsed = (self.pipeline_end_time - self.pipeline_start_time).total_seconds()

            self._log("\n" + "=" * 80)
            self._log(f"âœ… GRACE íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
            self._log("=" * 80)

            return results

        except Exception as e:
            self._log(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            raise

    # ============================================================
    # 1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
    # ============================================================

    def load_and_preprocess(self) -> None:
        """ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬"""
        # CSV ë¡œë“œ
        self._log(f"ğŸ“ ë°ì´í„°ì…‹ ë¡œë“œ: {self.config.csv_path}")

        if not os.path.exists(self.config.csv_path):
            raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.config.csv_path}")

        df = pd.read_csv(self.config.csv_path)
        self._log(f"  ì „ì²´ ë°ì´í„°: {len(df)} í–‰")

        if self.config.text_column not in df.columns:
            raise ValueError(f"CSVì— '{self.config.text_column}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        # ë¬¸ì„œ ì¶”ì¶œ
        self.documents = df[self.config.text_column].dropna().head(
            self.config.num_documents
        ).tolist()
        self._log(f"  ë¡œë“œëœ ë¬¸ì„œ: {len(self.documents)}ê°œ")

        # DocumentService ì´ˆê¸°í™” ë° ì „ì²˜ë¦¬
        self.doc_service = DocumentService()
        self.doc_service.create_sentence_list(documents=self.documents)
        self._log(f"  ì „ì²˜ë¦¬ ì™„ë£Œ: {self.doc_service.get_sentence_count()}ê°œ ë¬¸ì¥")
        self._log(f"  ì¶”ì¶œëœ ë‹¨ì–´: {len(self.doc_service.words_list)}ê°œ")

    # ============================================================
    # 2. ì˜ë¯¸ì—°ê²°ë§ êµ¬ì¶•
    # ============================================================

    def build_semantic_network(self) -> None:
        """ê³µì¶œí˜„ ê¸°ë°˜ ì˜ë¯¸ì—°ê²°ë§ êµ¬ì¶•"""
        if self.doc_service is None:
            raise RuntimeError("DocumentServiceê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_and_preprocess()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

        # GraphService ì´ˆê¸°í™”
        self.graph_service = GraphService(self.doc_service)

        # ê³µì¶œí˜„ ê·¸ë˜í”„ ìƒì„±
        self.word_graph = self.graph_service.build_complete_graph(
            top_n=self.config.top_n_words,
            exclude_stopwords=self.config.exclude_stopwords,
            max_length=self.config.max_sentence_length
        )

        self._log(f"  ë…¸ë“œ ìˆ˜: {self.word_graph.num_nodes}")
        self._log(f"  ì—£ì§€ ìˆ˜: {self.word_graph.num_edges}")

    # ============================================================
    # 3. ë©€í‹°ëª¨ë‹¬ ë…¸ë“œ íŠ¹ì„± ê³„ì‚°
    # ============================================================

    def compute_node_features(self) -> None:
        """ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ê³„ì‚° (Word2Vec + BERT)"""
        if self.word_graph is None or self.graph_service is None:
            raise RuntimeError("WordGraphê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. build_semantic_network()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

        self.node_feature_handler = NodeFeatureHandler(self.doc_service)

        method_desc = {
            'concat': f"Word2Vec({self.config.w2v_dim}) + BERT({self.config.bert_dim})",
            'w2v': f"Word2Vec({self.config.embed_size})",
            'bert': f"BERT({self.config.embed_size})"
        }
        self._log(f"  ì„ë² ë”© ë°©ë²•: {method_desc[self.config.embedding_method]}")

        self.node_features = self.node_feature_handler.calculate_embeddings(
            self.word_graph.words,
            method=self.config.embedding_method,
            embed_size=self.config.embed_size
        )

        self._log(f"  ë…¸ë“œ íŠ¹ì„± í˜•íƒœ: {self.node_features.shape}")

        # WordGraphì— ì„¤ì •
        self.word_graph.set_node_features_custom(self.node_features, NodeFeatureType.CUSTOM)

    # ============================================================
    # 4. GraphMAE ìê¸°ì§€ë„í•™ìŠµ
    # ============================================================

    def train_graphmae(self) -> None:
        """GraphMAE ì‚¬ì „í•™ìŠµ ë° ì„ë² ë”© ì¶”ì¶œ"""
        if self.word_graph is None or self.graph_service is None:
            raise RuntimeError("WordGraphê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        if self.word_graph.node_features is None:
            raise RuntimeError("ë…¸ë“œ íŠ¹ì„±ì´ ê³„ì‚°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. compute_node_features()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

        embed_size = self.word_graph.node_features.shape[1]

        # GraphMAE ì„¤ì •
        mae_config = GraphMAEConfig.create_default(embed_size)
        mae_config.max_epochs = self.config.graphmae_epochs
        mae_config.learning_rate = self.config.graphmae_lr
        mae_config.weight_decay = self.config.graphmae_weight_decay
        mae_config.mask_rate = self.config.mask_rate
        mae_config.device = self.config.graphmae_device or (
            "cuda" if torch.cuda.is_available() else "cpu"
        )

        self._log(f"  í•™ìŠµ ì„¤ì •: {mae_config.max_epochs} epochs, device: {mae_config.device}")
        if torch.cuda.is_available() and mae_config.device == "cuda":
            self._log(f"  GPU: {torch.cuda.get_device_name(0)}")

        # GraphMAE ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        mae_service = GraphMAEService(self.graph_service, mae_config)

        # DGL ê·¸ë˜í”„ë¡œ ë³€í™˜
        dgl_graph = self.graph_service.wordgraph_to_dgl(
            self.word_graph, self.word_graph.node_features
        )

        # ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
        mae_service.model = mae_service.create_mae_model(embed_size)
        device_obj = torch.device(mae_config.device)
        mae_service.model.to(device_obj)
        dgl_graph = dgl_graph.to(device_obj)

        optimizer = torch.optim.Adam(
            mae_service.model.parameters(),
            lr=mae_config.learning_rate,
            weight_decay=mae_config.weight_decay
        )

        # í•™ìŠµ ë£¨í”„
        mae_service.model.train()
        for epoch in range(mae_config.max_epochs):
            optimizer.zero_grad()
            x = dgl_graph.ndata['feat']
            loss = mae_service.model(dgl_graph, x, epoch=epoch)
            loss.backward()
            optimizer.step()

            if (epoch + 1) % self.config.log_interval == 0:
                self._log(f"  Epoch {epoch + 1}/{mae_config.max_epochs}, Loss: {loss.item():.4f}")

        # ì„ë² ë”© ì¶”ì¶œ
        mae_service.model.eval()
        with torch.no_grad():
            self.graphmae_embeddings = mae_service.model.embed(dgl_graph, dgl_graph.ndata['feat'])

        self.graphmae_embeddings = self.graphmae_embeddings.cpu()
        self._log(f"  GraphMAE ì„ë² ë”© ì¶”ì¶œ ì™„ë£Œ: {self.graphmae_embeddings.shape}")

    # ============================================================
    # 5. í´ëŸ¬ìŠ¤í„°ë§
    # ============================================================

    def perform_clustering(self) -> None:
        """í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰"""
        if self.graphmae_embeddings is None:
            raise RuntimeError("GraphMAE ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤. train_graphmae()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

        if self.config.num_clusters is not None:
            # ì§€ì •ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¡œ ì‹¤í–‰
            self.cluster_labels = self.clustering_service.kmeans_clustering(
                self.graphmae_embeddings,
                n_clusters=self.config.num_clusters,
                n_init=10
            )
            self._log(f"  K-means ì™„ë£Œ: {self.config.num_clusters}ê°œ í´ëŸ¬ìŠ¤í„°")
        else:
            # Elbow Methodë¡œ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ íƒìƒ‰
            self._log(f"  Elbow Methodë¡œ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ íƒìƒ‰ ì¤‘ ({self.config.min_clusters}-{self.config.max_clusters})...")

            self.cluster_labels, best_k, inertias, silhouette_scores = \
                self.clustering_service.auto_clustering_elbow(
                    self.graphmae_embeddings,
                    min_clusters=self.config.min_clusters,
                    max_clusters=self.config.max_clusters,
                    n_init=10
                )

            k_range = range(self.config.min_clusters, self.config.max_clusters + 1)
            self._log(f"  Elbow Point: k={best_k}")
            self._log(f"  ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜: {best_k} (Silhouette: {silhouette_scores[best_k - self.config.min_clusters]:.4f})")

            # Elbow curve ì‹œê°í™” ì €ì¥
            if self.config.save_graph_viz:
                from pathlib import Path
                from datetime import datetime
                output_dir = Path(self.config.output_dir)
                output_dir.mkdir(parents=True, exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                save_path = output_dir / f"elbow_curve_{timestamp}.png"
                self.clustering_service.save_elbow_curve(list(k_range), str(save_path))
                self._log(f"  Elbow curve ì €ì¥: {save_path}")

        # í´ëŸ¬ìŠ¤í„°ë³„ ë‹¨ì–´ ë¶„í¬
        cluster_dist = self.clustering_service.get_cluster_distribution()
        self._log(f"  í´ëŸ¬ìŠ¤í„°ë³„ ë‹¨ì–´ ìˆ˜: {cluster_dist}")

    # ============================================================
    # 6. í‰ê°€ ë° ê²°ê³¼ ì €ì¥
    # ============================================================

    def evaluate_and_save(self) -> Dict[str, Any]:
        """í‰ê°€ ì§€í‘œ ê³„ì‚° ë° ê²°ê³¼ ì €ì¥"""
        if self.cluster_labels is None or self.graphmae_embeddings is None:
            raise RuntimeError("í´ëŸ¬ìŠ¤í„°ë§ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        cluster_dist = self.clustering_service.get_cluster_distribution()
        results = {
            'config': self.config.__dict__,
            'graph_stats': self.word_graph.get_graph_stats(),
            'num_clusters': len(cluster_dist),
            'cluster_distribution': cluster_dist,
            'metrics': {},
            'clusters': self._build_cluster_info()
        }

        # í‰ê°€ ì§€í‘œ ê³„ì‚° (MetricsService ì‚¬ìš©)
        metrics = self.metrics_service.calculate_metrics(
            self.graphmae_embeddings,
            self.cluster_labels,
            self.config.eval_metrics
        )
        results['metrics'] = metrics

        # ë©”íŠ¸ë¦­ ì¶œë ¥
        for metric_name, value in metrics.items():
            self._log(f"  {metric_name}: {value:.4f}")

        # ê²°ê³¼ ì €ì¥
        if self.config.save_results:
            self._save_results(results)

        self.cluster_results = results
        return results

    # ============================================================
    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ
    # ============================================================

    def _build_cluster_info(self) -> Dict[int, List[str]]:
        """í´ëŸ¬ìŠ¤í„°ë³„ ë‹¨ì–´ ì •ë³´ êµ¬ì„±"""
        cluster_info = {}
        for cluster_id in np.unique(self.cluster_labels):
            indices = np.where(self.cluster_labels == cluster_id)[0]
            words = [self.word_graph.words[i].content for i in indices]
            cluster_info[int(cluster_id)] = words
        return cluster_info

    def _save_results(self, results: Dict[str, Any]) -> None:
        """ê²°ê³¼ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥"""
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # JSON ê²°ê³¼ ì €ì¥
        json_path = output_dir / f"grace_results_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        self._log(f"  ê²°ê³¼ ì €ì¥: {json_path}")

        # ì„ë² ë”© ì €ì¥
        if self.config.save_embeddings:
            embed_path = output_dir / f"embeddings_{timestamp}.pt"
            torch.save({
                'graphmae_embeddings': self.graphmae_embeddings,
                'node_features': self.node_features,
                'cluster_labels': self.cluster_labels,
                'word_list': [w.content for w in self.word_graph.words]
            }, embed_path)
            self._log(f"  ì„ë² ë”© ì €ì¥: {embed_path}")

    def _log(self, message: str) -> None:
        """ë¡œê·¸ ì¶œë ¥"""
        if self.config.verbose:
            print(message)

    # ============================================================
    # ì™¸ë¶€ ì ‘ê·¼ ë©”ì„œë“œ
    # ============================================================

    def get_cluster_words(self, cluster_id: int) -> List[str]:
        """íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì˜ ë‹¨ì–´ë“¤ ë°˜í™˜"""
        if self.cluster_labels is None:
            raise RuntimeError("í´ëŸ¬ìŠ¤í„°ë§ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        indices = np.where(self.cluster_labels == cluster_id)[0]
        return [self.word_graph.words[i].content for i in indices]

    def get_word_cluster(self, word: str) -> Optional[int]:
        """íŠ¹ì • ë‹¨ì–´ê°€ ì†í•œ í´ëŸ¬ìŠ¤í„° ID ë°˜í™˜"""
        if self.cluster_labels is None or self.word_graph is None:
            raise RuntimeError("í´ëŸ¬ìŠ¤í„°ë§ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        node_id = self.word_graph.get_node_id_by_word(word)
        if node_id is None:
            return None

        return int(self.cluster_labels[node_id])

    def export_cluster_csv(self, output_path: str) -> None:
        """í´ëŸ¬ìŠ¤í„° ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥"""
        if self.cluster_labels is None or self.word_graph is None:
            raise RuntimeError("í´ëŸ¬ìŠ¤í„°ë§ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        data = []
        for i, word in enumerate(self.word_graph.words):
            data.append({
                'word': word.content,
                'cluster': int(self.cluster_labels[i]),
                'frequency': word.freq,
                'pos': word.dominant_pos
            })

        df = pd.DataFrame(data)
        df.to_csv(output_path, index=False, encoding='utf-8')
        self._log(f"  í´ëŸ¬ìŠ¤í„° CSV ì €ì¥: {output_path}")
