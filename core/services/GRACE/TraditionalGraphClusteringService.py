"""
ì „í†µì  ê·¸ë˜í”„ í´ëŸ¬ìŠ¤í„°ë§ ì„œë¹„ìŠ¤

ê·¸ë˜í”„ êµ¬ì¡° ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜:
- Louvain: Modularity ìµœì í™”
- Leiden: Louvain ê°œì„ íŒ (ë” ì •í™•í•œ ì»¤ë®¤ë‹ˆí‹° íƒì§€)
- Girvan-Newman: Edge betweenness ê¸°ë°˜

GRACEì™€ ë™ì¼í•œ WordGraphë¥¼ ê³µìœ í•˜ì—¬ ê³µì •í•œ ë¹„êµë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.
"""

import numpy as np
import networkx as nx
from typing import Optional, Dict, List, Tuple, Any
from pathlib import Path
import warnings

from entities import WordGraph


class TraditionalGraphClusteringService:
    """ì „í†µì  ê·¸ë˜í”„ í´ëŸ¬ìŠ¤í„°ë§ ì„œë¹„ìŠ¤
    
    GRACEì™€ ë™ì¼í•œ WordGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ì „í†µì ì¸ ê·¸ë˜í”„ ì»¤ë®¤ë‹ˆí‹° íƒì§€ ì•Œê³ ë¦¬ì¦˜ ìˆ˜í–‰.
    GraphMAE ì—†ì´ ê·¸ë˜í”„ êµ¬ì¡°ë§Œìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬ GRACEì˜ ì°¨ë³„ì ì„ ì…ì¦.
    """
    
    def __init__(self, random_state: int = 42):
        """
        Args:
            random_state: ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ (Louvain, Leidenì— ì‚¬ìš©)
        """
        self.random_state = random_state
        self.cluster_labels: Optional[np.ndarray] = None
        self.nx_graph: Optional[nx.Graph] = None
        self.clustering_method: Optional[str] = None
        self.num_clusters: Optional[int] = None
        
        # ì•Œê³ ë¦¬ì¦˜ë³„ ì¶”ê°€ ì •ë³´
        self.modularity: Optional[float] = None
        self.communities: Optional[List[set]] = None
        
    # ============================================================
    # Louvain í´ëŸ¬ìŠ¤í„°ë§
    # ============================================================
    
    def louvain_clustering(
        self, 
        word_graph: WordGraph,
        resolution: float = 1.0
    ) -> Tuple[np.ndarray, Dict[str, Any]]:
        """
        Louvain ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì»¤ë®¤ë‹ˆí‹° íƒì§€
        
        Louvainì€ Modularityë¥¼ ìµœì í™”í•˜ëŠ” íƒìš•ì  ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.
        - ì¥ì : ë¹ ë¥¸ ì†ë„, ëŒ€ê·œëª¨ ê·¸ë˜í”„ì— ì í•©
        - ë‹¨ì : ë¡œì»¬ ì˜µí‹°ë©ˆì— ë¹ ì§ˆ ìˆ˜ ìˆìŒ
        
        Args:
            word_graph: GRACEì™€ ë™ì¼í•œ WordGraph ê°ì²´
            resolution: Modularity í•´ìƒë„ (ë†’ì„ìˆ˜ë¡ ì‘ì€ ì»¤ë®¤ë‹ˆí‹° ìƒì„±)
            
        Returns:
            (cluster_labels, metrics_dict)
            - cluster_labels: [num_nodes] ë…¸ë“œë³„ í´ëŸ¬ìŠ¤í„° ID
            - metrics_dict: í‰ê°€ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
        """
        try:
            import community.community_louvain as community_louvain
        except ImportError:
            raise ImportError(
                "python-louvain package required. Install: pip install python-louvain"
            )
        
        # WordGraphë¥¼ NetworkXë¡œ ë³€í™˜ (ê°€ì¤‘ì¹˜ í¬í•¨)
        self.nx_graph = word_graph.export_to_networkx(include_weights=True)
        
        # Louvain ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
        partition = community_louvain.best_partition(
            self.nx_graph,
            resolution=resolution,
            random_state=self.random_state
        )
        
        # ë…¸ë“œ ID ìˆœì„œëŒ€ë¡œ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ë°°ì—´ ìƒì„±
        self.cluster_labels = np.array([partition[i] for i in range(word_graph.num_nodes)])
        
        # ì»¤ë®¤ë‹ˆí‹° ì •ë³´ ì €ì¥
        self.communities = self._labels_to_communities(self.cluster_labels)
        self.num_clusters = len(self.communities)
        
        # Modularity ê³„ì‚°
        self.modularity = community_louvain.modularity(partition, self.nx_graph)
        
        self.clustering_method = "louvain"
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        metrics = {
            'method': 'louvain',
            'num_clusters': self.num_clusters,
            'modularity': self.modularity,
            'resolution': resolution,
            'num_nodes': word_graph.num_nodes,
            'num_edges': word_graph.num_edges
        }
        
        print(f"âœ… Louvain í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {self.num_clusters}ê°œ ì»¤ë®¤ë‹ˆí‹° (Modularity: {self.modularity:.4f})")
        
        return self.cluster_labels, metrics
    
    # ============================================================
    # Leiden í´ëŸ¬ìŠ¤í„°ë§
    # ============================================================
    
    def leiden_clustering(
        self,
        word_graph: WordGraph,
        resolution: float = 1.0,
        n_iterations: int = -1
    ) -> Tuple[np.ndarray, Dict[str, Any]]:
        """
        Leiden ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì»¤ë®¤ë‹ˆí‹° íƒì§€
        
        Leidenì€ Louvainì˜ ê°œì„  ë²„ì „ìœ¼ë¡œ, ë” ì •í™•í•œ ì»¤ë®¤ë‹ˆí‹°ë¥¼ íƒì§€í•©ë‹ˆë‹¤.
        - ì¥ì : Louvainë³´ë‹¤ í’ˆì§ˆ ë†’ì€ ì»¤ë®¤ë‹ˆí‹°, ì—°ê²° ë³´ì¥
        - ë‹¨ì : ì•½ê°„ ëŠë¦¼
        
        Args:
            word_graph: GRACEì™€ ë™ì¼í•œ WordGraph ê°ì²´
            resolution: Modularity í•´ìƒë„
            n_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (-1ì€ ìˆ˜ë ´ê¹Œì§€)
            
        Returns:
            (cluster_labels, metrics_dict)
        """
        try:
            import leidenalg
            import igraph as ig
        except ImportError:
            raise ImportError(
                "leidenalg and igraph required. Install:\n"
                "  pip install leidenalg python-igraph"
            )
        
        # WordGraphë¥¼ NetworkXë¡œ ë³€í™˜
        self.nx_graph = word_graph.export_to_networkx(include_weights=True)
        
        # NetworkXë¥¼ igraphë¡œ ë³€í™˜ (Leidenì€ igraph ì‚¬ìš©)
        g_igraph = self._networkx_to_igraph(self.nx_graph)
        
        # Leiden ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
        partition = leidenalg.find_partition(
            g_igraph,
            leidenalg.ModularityVertexPartition,
            n_iterations=n_iterations,
            seed=self.random_state,
            weights='weight' if 'weight' in g_igraph.es.attributes() else None
        )
        
        # í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ì¶”ì¶œ
        self.cluster_labels = np.array(partition.membership)
        
        # ì»¤ë®¤ë‹ˆí‹° ì •ë³´
        self.communities = self._labels_to_communities(self.cluster_labels)
        self.num_clusters = len(self.communities)
        
        # Modularity ê³„ì‚°
        self.modularity = partition.modularity
        
        self.clustering_method = "leiden"
        
        metrics = {
            'method': 'leiden',
            'num_clusters': self.num_clusters,
            'modularity': self.modularity,
            'resolution': resolution,
            'num_nodes': word_graph.num_nodes,
            'num_edges': word_graph.num_edges,
            'quality': partition.quality()
        }
        
        print(f"âœ… Leiden í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {self.num_clusters}ê°œ ì»¤ë®¤ë‹ˆí‹° (Modularity: {self.modularity:.4f})")
        
        return self.cluster_labels, metrics
    
    # ============================================================
    # Girvan-Newman í´ëŸ¬ìŠ¤í„°ë§
    # ============================================================
    
    def girvan_newman_clustering(
        self,
        word_graph: WordGraph,
        num_clusters: int = None,
        verbose: bool = True
    ) -> Tuple[np.ndarray, Dict[str, Any]]:
        """
        Girvan-Newman ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì»¤ë®¤ë‹ˆí‹° íƒì§€ (ìµœì í™” ë²„ì „)
        
        Edge betweennessë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°˜ë³µì ìœ¼ë¡œ ì—£ì§€ë¥¼ ì œê±°í•˜ë©° ì»¤ë®¤ë‹ˆí‹°ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
        - ì¥ì : ê³„ì¸µì  êµ¬ì¡° íŒŒì•… ê°€ëŠ¥
        - ë‹¨ì : ëŠë¦° ì†ë„ (O(mÂ²n) ë˜ëŠ” O(nÂ³)), ëŒ€ê·œëª¨ ê·¸ë˜í”„ì— ë¶€ì í•©
        
        ìµœì í™” ì „ëµ:
        1. num_clusters ì§€ì •: ì¡°ê¸° ì¢…ë£Œë¡œ ì†ë„ í–¥ìƒ (ê¶Œì¥!)
        2. ê·¸ë˜í”„ í¬ê¸° ì œí•œ: â‰¤150 ë…¸ë“œ ê¶Œì¥
        3. ì§„í–‰ìƒí™© í‘œì‹œ: verbose=True
        
        ë³‘ë ¬ ì²˜ë¦¬ì˜ í•œê³„:
        - Edge betweenness ê³„ì‚°ì€ ë³‘ë ¬í™” ê°€ëŠ¥í•˜ì§€ë§Œ NetworkXëŠ” ì´ë¯¸ Cë¡œ ìµœì í™”ë¨
        - ì•Œê³ ë¦¬ì¦˜ ìì²´ê°€ ìˆœì°¨ì ì´ì–´ì„œ ë‹¨ê³„ ê°„ ë³‘ë ¬í™” ë¶ˆê°€ëŠ¥
        
        Args:
            word_graph: GRACEì™€ ë™ì¼í•œ WordGraph ê°ì²´
            num_clusters: ì›í•˜ëŠ” í´ëŸ¬ìŠ¤í„° ìˆ˜ (Noneì´ë©´ modularity ìµœëŒ€í™”, ëŠë¦¼!)
            verbose: ì§„í–‰ìƒí™© ì¶œë ¥ ì—¬ë¶€
            
        Returns:
            (cluster_labels, metrics_dict)
        """
        from networkx.algorithms.community import girvan_newman
        import time
        
        start_time = time.time()
        
        # WordGraphë¥¼ NetworkXë¡œ ë³€í™˜
        self.nx_graph = word_graph.export_to_networkx(include_weights=True)
        
        if verbose:
            print(f"   Girvan-Newman ì‹œì‘: {word_graph.num_nodes} ë…¸ë“œ, {word_graph.num_edges} ì—£ì§€")
            if num_clusters:
                print(f"   ëª©í‘œ í´ëŸ¬ìŠ¤í„°: {num_clusters} (ì¡°ê¸° ì¢…ë£Œ ëª¨ë“œ)")
            else:
                print(f"   âš ï¸  ìë™ íƒìƒ‰ ëª¨ë“œ (ëŠë¦¼) - num_clusters ì§€ì • ê¶Œì¥")
        
        # Girvan-Newman ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰ (ì œë„ˆë ˆì´í„° ë°˜í™˜)
        communities_generator = girvan_newman(self.nx_graph)
        
        # num_clustersê°€ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ modularityê°€ ìµœëŒ€ì¸ ë¶„í•  ì°¾ê¸°
        if num_clusters is None:
            best_partition = None
            best_modularity = -1
            
            # ìµœëŒ€ 20ê°œ ë¶„í• ê¹Œì§€ë§Œ ì‹œë„ (ê³„ì‚° ë¹„ìš© ê³ ë ¤)
            max_iterations = min(20, word_graph.num_nodes - 1)
            
            if verbose:
                print(f"   ìë™ íƒìƒ‰: ìµœëŒ€ {max_iterations}ë²ˆ ë°˜ë³µ")
            
            for i, partition in enumerate(communities_generator):
                if i >= max_iterations:
                    break
                
                iteration_start = time.time()
                    
                # Modularity ê³„ì‚°
                mod = nx.algorithms.community.modularity(self.nx_graph, partition)
                
                if verbose and i % 5 == 0:
                    elapsed = time.time() - start_time
                    print(f"   ë°˜ë³µ {i+1}/{max_iterations}: {len(partition)} í´ëŸ¬ìŠ¤í„°, "
                          f"modularity={mod:.4f}, ì†Œìš”ì‹œê°„={elapsed:.1f}ì´ˆ")
                
                if mod > best_modularity:
                    best_modularity = mod
                    best_partition = partition
            
            self.communities = [set(c) for c in best_partition]
            self.modularity = best_modularity
        else:
            # ì§€ì •ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜ê¹Œì§€ ë¶„í•  (ë¹ ë¦„!)
            partition = None
            if verbose:
                print(f"   ì¡°ê¸° ì¢…ë£Œ ëª¨ë“œ: {num_clusters} í´ëŸ¬ìŠ¤í„°ê¹Œì§€ ë¶„í• ")
            
            for i in range(num_clusters - 1):
                iteration_start = time.time()
                partition = next(communities_generator)
                
                if verbose and (i % 2 == 0 or i == num_clusters - 2):
                    elapsed = time.time() - start_time
                    print(f"   ë°˜ë³µ {i+1}/{num_clusters-1}: {len(partition)} í´ëŸ¬ìŠ¤í„°, "
                          f"ì†Œìš”ì‹œê°„={elapsed:.1f}ì´ˆ")
            
            self.communities = [set(c) for c in partition]
            self.modularity = nx.algorithms.community.modularity(self.nx_graph, partition)
        
        # ì»¤ë®¤ë‹ˆí‹°ë¥¼ ë¼ë²¨ ë°°ì—´ë¡œ ë³€í™˜
        self.cluster_labels = np.zeros(word_graph.num_nodes, dtype=int)
        for cluster_id, community in enumerate(self.communities):
            for node_id in community:
                self.cluster_labels[node_id] = cluster_id
        
        self.num_clusters = len(self.communities)
        self.clustering_method = "girvan_newman"
        
        total_time = time.time() - start_time
        
        metrics = {
            'method': 'girvan_newman',
            'num_clusters': self.num_clusters,
            'modularity': self.modularity,
            'num_nodes': word_graph.num_nodes,
            'num_edges': word_graph.num_edges,
            'computation_time': total_time
        }
        
        if verbose:
            print(f"âœ… Girvan-Newman í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {self.num_clusters}ê°œ ì»¤ë®¤ë‹ˆí‹°")
            print(f"   Modularity: {self.modularity:.4f}, ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ")
        else:
            print(f"âœ… Girvan-Newman í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {self.num_clusters}ê°œ ì»¤ë®¤ë‹ˆí‹° (Modularity: {self.modularity:.4f})")
        
        return self.cluster_labels, metrics
    
    # ============================================================
    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ
    # ============================================================
    
    def get_cluster_distribution(self) -> Dict[int, int]:
        """
        í´ëŸ¬ìŠ¤í„°ë³„ ë…¸ë“œ ìˆ˜ ë°˜í™˜
        
        Returns:
            {cluster_id: count} ë”•ì…”ë„ˆë¦¬
        """
        if self.cluster_labels is None:
            raise RuntimeError("í´ëŸ¬ìŠ¤í„°ë§ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        unique, counts = np.unique(self.cluster_labels, return_counts=True)
        return {int(k): int(v) for k, v in zip(unique, counts)}
    
    def get_cluster_words(
        self,
        word_graph: WordGraph,
        cluster_id: int,
        top_n: int = 10
    ) -> List[Tuple[str, int]]:
        """
        íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì˜ ìƒìœ„ ë‹¨ì–´ë“¤ ë°˜í™˜ (ë¹ˆë„ ìˆœ)
        
        Args:
            word_graph: WordGraph ê°ì²´
            cluster_id: í´ëŸ¬ìŠ¤í„° ID
            top_n: ë°˜í™˜í•  ë‹¨ì–´ ìˆ˜
            
        Returns:
            [(word, frequency), ...] ë¦¬ìŠ¤íŠ¸
        """
        if self.cluster_labels is None:
            raise RuntimeError("í´ëŸ¬ìŠ¤í„°ë§ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ë…¸ë“œë“¤ ì°¾ê¸°
        cluster_mask = self.cluster_labels == cluster_id
        cluster_node_ids = np.where(cluster_mask)[0]
        
        # ë‹¨ì–´ì™€ ë¹ˆë„ ìˆ˜ì§‘
        word_freq_pairs = []
        for node_id in cluster_node_ids:
            word = word_graph.get_word_by_node_id(int(node_id))
            word_freq_pairs.append((word.content, word.freq))
        
        # ë¹ˆë„ìˆœ ì •ë ¬
        word_freq_pairs.sort(key=lambda x: x[1], reverse=True)
        
        return word_freq_pairs[:top_n]
    
    def get_all_cluster_words(
        self,
        word_graph: WordGraph,
        top_n: int = 10
    ) -> Dict[int, List[Tuple[str, int]]]:
        """
        ëª¨ë“  í´ëŸ¬ìŠ¤í„°ì˜ ìƒìœ„ ë‹¨ì–´ë“¤ ë°˜í™˜
        
        Args:
            word_graph: WordGraph ê°ì²´
            top_n: í´ëŸ¬ìŠ¤í„°ë‹¹ ë°˜í™˜í•  ë‹¨ì–´ ìˆ˜
            
        Returns:
            {cluster_id: [(word, frequency), ...]} ë”•ì…”ë„ˆë¦¬
        """
        if self.cluster_labels is None:
            raise RuntimeError("í´ëŸ¬ìŠ¤í„°ë§ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        result = {}
        for cluster_id in range(self.num_clusters):
            result[cluster_id] = self.get_cluster_words(word_graph, cluster_id, top_n)
        
        return result
    
    def compute_graph_statistics(self) -> Dict[str, Any]:
        """
        ê·¸ë˜í”„ í†µê³„ ì •ë³´ ê³„ì‚°
        
        Returns:
            í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        if self.nx_graph is None:
            raise RuntimeError("ê·¸ë˜í”„ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        stats = {
            'num_nodes': self.nx_graph.number_of_nodes(),
            'num_edges': self.nx_graph.number_of_edges(),
            'density': nx.density(self.nx_graph),
            'average_degree': sum(dict(self.nx_graph.degree()).values()) / self.nx_graph.number_of_nodes(),
        }
        
        # ì—°ê²° ì„±ë¶„ ë¶„ì„
        if nx.is_connected(self.nx_graph):
            stats['is_connected'] = True
            stats['diameter'] = nx.diameter(self.nx_graph)
            stats['average_shortest_path_length'] = nx.average_shortest_path_length(self.nx_graph)
        else:
            stats['is_connected'] = False
            stats['num_connected_components'] = nx.number_connected_components(self.nx_graph)
            # ê°€ì¥ í° ì—°ê²° ì„±ë¶„ì˜ í†µê³„
            largest_cc = max(nx.connected_components(self.nx_graph), key=len)
            largest_subgraph = self.nx_graph.subgraph(largest_cc)
            stats['largest_component_size'] = len(largest_cc)
            stats['largest_component_diameter'] = nx.diameter(largest_subgraph)
        
        # í´ëŸ¬ìŠ¤í„°ë§ ê³„ìˆ˜
        stats['average_clustering_coefficient'] = nx.average_clustering(self.nx_graph)
        
        return stats
    
    # ============================================================
    # Private Helper Methods
    # ============================================================
    
    def _labels_to_communities(self, labels: np.ndarray) -> List[set]:
        """
        ë¼ë²¨ ë°°ì—´ì„ ì»¤ë®¤ë‹ˆí‹° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        
        Args:
            labels: [num_nodes] í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ë°°ì—´
            
        Returns:
            [set(node_ids), ...] ì»¤ë®¤ë‹ˆí‹° ë¦¬ìŠ¤íŠ¸
        """
        unique_labels = np.unique(labels)
        communities = []
        
        for label in unique_labels:
            community = set(np.where(labels == label)[0].tolist())
            communities.append(community)
        
        return communities
    
    def _networkx_to_igraph(self, nx_graph: nx.Graph) -> 'ig.Graph':
        """
        NetworkX ê·¸ë˜í”„ë¥¼ igraphë¡œ ë³€í™˜
        
        Args:
            nx_graph: NetworkX ê·¸ë˜í”„
            
        Returns:
            igraph ê·¸ë˜í”„
        """
        import igraph as ig
        
        # ì—£ì§€ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        edges = list(nx_graph.edges())
        
        # ê°€ì¤‘ì¹˜ ì¶”ì¶œ (ìˆìœ¼ë©´)
        weights = None
        if nx_graph.edges():
            first_edge = list(nx_graph.edges(data=True))[0]
            if 'weight' in first_edge[2]:
                weights = [nx_graph[u][v].get('weight', 1.0) for u, v in edges]
        
        # igraph ìƒì„±
        g = ig.Graph(n=nx_graph.number_of_nodes(), edges=edges, directed=False)
        
        if weights:
            g.es['weight'] = weights
        
        # ë…¸ë“œ ì†ì„± ë³µì‚¬
        for attr in nx_graph.nodes[0].keys() if nx_graph.nodes() else []:
            g.vs[attr] = [nx_graph.nodes[i].get(attr) for i in range(nx_graph.number_of_nodes())]
        
        return g
    
    def save_clustering_results(
        self,
        word_graph: WordGraph,
        output_path: str,
        include_words: bool = True
    ) -> None:
        """
        í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            word_graph: WordGraph ê°ì²´
            output_path: ì €ì¥ ê²½ë¡œ
            include_words: ê° í´ëŸ¬ìŠ¤í„°ì˜ ìƒìœ„ ë‹¨ì–´ í¬í•¨ ì—¬ë¶€
        """
        if self.cluster_labels is None:
            raise RuntimeError("í´ëŸ¬ìŠ¤í„°ë§ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        import json
        from datetime import datetime
        
        results = {
            'method': self.clustering_method,
            'num_clusters': self.num_clusters,
            'modularity': self.modularity,
            'cluster_distribution': self.get_cluster_distribution(),
            'timestamp': datetime.now().isoformat(),
            'graph_stats': {
                'num_nodes': word_graph.num_nodes,
                'num_edges': word_graph.num_edges
            }
        }
        
        if include_words:
            results['cluster_words'] = {
                str(k): [(w, f) for w, f in v]
                for k, v in self.get_all_cluster_words(word_graph, top_n=20).items()
            }
        
        # JSONìœ¼ë¡œ ì €ì¥
        output_dir = Path(output_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥: {output_path}")
