"""
Ablation Study Service

GRACE ì»´í¬ë„ŒíŠ¸ë³„ ê¸°ì—¬ë„ ë¶„ì„
- GraphMAE ìœ /ë¬´
- Multimodal embedding (Word2Vec + BERT) vs ë‹¨ì¼ ì„ë² ë”©
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ì˜í–¥ (mask rate, embedding dimension, epochs)
"""

import numpy as np
import torch
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path
from datetime import datetime
import json

from ..GRACE.GRACEConfig import GRACEConfig
from ..GRACE.GRACEPipeline import GRACEPipeline
from ..GRACE.ClusteringService import ClusteringService
from ..Metric import MetricsService
from ..Document.DocumentService import DocumentService
from ..Graph.GraphService import GraphService
from ..Graph.NodeFeatureHandler import NodeFeatureHandler
from entities import WordGraph, NodeFeatureType


class AblationService:
    """
    Ablation Studyë¥¼ ìœ„í•œ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤

    GRACEì˜ ê° ì»´í¬ë„ŒíŠ¸ê°€ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„:
    1. Embedding Method Ablation: Word2Vec only, BERT only, Multimodal
    2. GraphMAE Ablation: GraphMAE ìœ /ë¬´
    3. Hyperparameter Ablation: mask_rate, embed_size, epochs
    """

    def __init__(self, base_config: GRACEConfig, random_state: int = 42):
        """
        Args:
            base_config: ê¸°ë³¸ GRACE ì„¤ì • (baseline)
            random_state: ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ
        """
        self.base_config = base_config
        self.random_state = random_state

        # ê³µìœ  ë°ì´í„° (ëª¨ë“  ablation ì‹¤í—˜ì—ì„œ ë™ì¼í•˜ê²Œ ì‚¬ìš©)
        self.doc_service: Optional[DocumentService] = None
        self.word_graph: Optional[WordGraph] = None

        # ê²°ê³¼ ì €ì¥
        self.ablation_results: Dict[str, Any] = {}

    # ============================================================
    # 1. ê³µí†µ ì „ì²˜ë¦¬ (í•œ ë²ˆë§Œ ì‹¤í–‰)
    # ============================================================

    def prepare_shared_data(self) -> None:
        """
        ëª¨ë“  ablation ì‹¤í—˜ì—ì„œ ê³µìœ í•  ë°ì´í„° ì¤€ë¹„
        - Documents ë¡œë“œ ë° ì „ì²˜ë¦¬
        - Word Graph êµ¬ì¶•
        """
        print("ğŸ“ ê³µìœ  ë°ì´í„° ì¤€ë¹„ ì¤‘...")

        # DocumentService ì´ˆê¸°í™”
        pipeline = GRACEPipeline(self.base_config)
        pipeline.load_and_preprocess()
        pipeline.build_semantic_network()

        self.doc_service = pipeline.doc_service
        self.word_graph = pipeline.word_graph

        print(f"  âœ… ë¬¸ì„œ: {pipeline.doc_service.get_sentence_count()}ê°œ")
        print(f"  âœ… ê·¸ë˜í”„: {self.word_graph.num_nodes} nodes, {self.word_graph.num_edges} edges")

    # ============================================================
    # 2. Embedding Method Ablation
    # ============================================================

    def run_embedding_ablation(self) -> Dict[str, Dict[str, float]]:
        """
        ì„ë² ë”© ë°©ë²•ë³„ ì„±ëŠ¥ ë¹„êµ

        ì‹¤í—˜:
        - Word2Vec only
        - BERT only
        - Multimodal (Word2Vec + BERT)

        Returns:
            {embedding_method: {metric: value}}
        """
        if self.doc_service is None or self.word_graph is None:
            raise RuntimeError("prepare_shared_data()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

        print("\n" + "=" * 80)
        print("ğŸ”¬ Ablation Study 1: Embedding Method")
        print("=" * 80)

        results = {}
        embedding_methods = [
            ('w2v', "Word2Vec only"),
            ('bert', "BERT only"),
            ('concat', "Multimodal (Word2Vec + BERT)")
        ]

        for method, desc in embedding_methods:
            print(f"\n[{desc}]")
            config = self._create_config_variant(embedding_method=method)
            metrics = self._run_experiment(config, use_graphmae=True)
            results[method] = metrics
            self._print_metrics(metrics)

        self.ablation_results['embedding_method'] = results
        return results

    # ============================================================
    # 3. GraphMAE Ablation
    # ============================================================

    def run_graphmae_ablation(self) -> Dict[str, Dict[str, float]]:
        """
        GraphMAE ìœ /ë¬´ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ

        ì‹¤í—˜:
        - Without GraphMAE (ë©€í‹°ëª¨ë‹¬ ì„ë² ë”©ë§Œ ì‚¬ìš©)
        - With GraphMAE (GRACE ì „ì²´ íŒŒì´í”„ë¼ì¸)

        Returns:
            {graphmae_status: {metric: value}}
        """
        if self.doc_service is None or self.word_graph is None:
            raise RuntimeError("prepare_shared_data()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

        print("\n" + "=" * 80)
        print("ğŸ”¬ Ablation Study 2: GraphMAE Effect")
        print("=" * 80)

        results = {}

        # Without GraphMAE
        print(f"\n[Without GraphMAE]")
        config = self._create_config_variant()
        metrics_without = self._run_experiment(config, use_graphmae=False)
        results['without_graphmae'] = metrics_without
        self._print_metrics(metrics_without)

        # With GraphMAE
        print(f"\n[With GraphMAE]")
        metrics_with = self._run_experiment(config, use_graphmae=True)
        results['with_graphmae'] = metrics_with
        self._print_metrics(metrics_with)

        # ê°œì„ ìœ¨ ê³„ì‚°
        improvement = self._calculate_improvement(metrics_without, metrics_with)
        print(f"\nğŸ“ˆ Improvement:")
        for metric, pct in improvement.items():
            print(f"  {metric}: {pct:+.2f}%")

        results['improvement'] = improvement
        self.ablation_results['graphmae'] = results
        return results

    # ============================================================
    # 4. Hyperparameter Ablation
    # ============================================================

    def run_mask_rate_ablation(
        self,
        mask_rates: Optional[List[float]] = None,
        num_runs: int = 1
    ) -> Dict[float, Dict[str, float]]:
        """
        GraphMAE mask rateì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”

        Args:
            mask_rates: í…ŒìŠ¤íŠ¸í•  mask rate ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: [0.3, 0.5, 0.75, 0.9])
            num_runs: ê° mask rateë‹¹ ë°˜ë³µ ì‹¤í–‰ íšŸìˆ˜ (ê¸°ë³¸: 1)

        Returns:
            {mask_rate: {metric: value}} ë˜ëŠ”
            {mask_rate: {metric_mean: value, metric_std: value}} (num_runs > 1)
        """
        if self.doc_service is None or self.word_graph is None:
            raise RuntimeError("prepare_shared_data()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

        if mask_rates is None:
            mask_rates = [0.3, 0.5, 0.75, 0.9]

        print("\n" + "=" * 80)
        print("ğŸ”¬ Ablation Study 3: Mask Rate")
        if num_runs > 1:
            print(f"   (ê° mask rateë‹¹ {num_runs}íšŒ ë°˜ë³µ ì‹¤í–‰)")
        print("=" * 80)

        results = {}

        for mask_rate in mask_rates:
            print(f"\n[Mask Rate = {mask_rate}]")
            config = self._create_config_variant(mask_rate=mask_rate)

            if num_runs == 1:
                # ë‹¨ì¼ ì‹¤í–‰
                metrics = self._run_experiment(config, use_graphmae=True)
                results[mask_rate] = metrics
                self._print_metrics(metrics)
            else:
                # ë‹¤ì¤‘ ì‹¤í–‰
                all_runs = []
                for run_idx in range(num_runs):
                    print(f"  Run {run_idx + 1}/{num_runs}...", end=" ")

                    # ë§¤ ì‹¤í–‰ë§ˆë‹¤ ë‹¤ë¥¸ ì‹œë“œ ì‚¬ìš© (ì¬í˜„ì„± ìœ ì§€í•˜ë©´ì„œ ë³€ë™ì„± ì¸¡ì •)
                    config_variant = self._create_config_variant(mask_rate=mask_rate)
                    metrics = self._run_experiment(config_variant, use_graphmae=True)
                    all_runs.append(metrics)
                    print(f"Silhouette={metrics['silhouette']:.4f}")

                # í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°
                aggregated = self._aggregate_multiple_runs(all_runs)
                results[mask_rate] = aggregated

                print(f"\n  ğŸ“Š ì§‘ê³„ ê²°ê³¼:")
                self._print_aggregated_metrics(aggregated)

        self.ablation_results['mask_rate'] = results
        return results

    def run_embed_size_ablation(
        self,
        embed_sizes: Optional[List[int]] = None
    ) -> Dict[int, Dict[str, float]]:
        """
        ì„ë² ë”© ì°¨ì›ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”

        Args:
            embed_sizes: í…ŒìŠ¤íŠ¸í•  ì„ë² ë”© ì°¨ì› ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: [32, 64, 128, 256])

        Returns:
            {embed_size: {metric: value}}
        """
        if self.doc_service is None or self.word_graph is None:
            raise RuntimeError("prepare_shared_data()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

        if embed_sizes is None:
            embed_sizes = [32, 64, 128, 256]

        print("\n" + "=" * 80)
        print("ğŸ”¬ Ablation Study 4: Embedding Dimension")
        print("=" * 80)

        results = {}

        for embed_size in embed_sizes:
            print(f"\n[Embed Size = {embed_size}]")
            config = self._create_config_variant(
                embed_size=embed_size,
                w2v_dim=embed_size // 2,
                bert_dim=embed_size // 2
            )
            metrics = self._run_experiment(config, use_graphmae=True)
            results[embed_size] = metrics
            self._print_metrics(metrics)

        self.ablation_results['embed_size'] = results
        return results

    def run_epochs_ablation(
        self,
        epochs_list: Optional[List[int]] = None
    ) -> Dict[int, Dict[str, float]]:
        """
        GraphMAE í•™ìŠµ epochì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”

        Args:
            epochs_list: í…ŒìŠ¤íŠ¸í•  epoch ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: [50, 100, 200, 300])

        Returns:
            {epochs: {metric: value}}
        """
        if self.doc_service is None or self.word_graph is None:
            raise RuntimeError("prepare_shared_data()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

        if epochs_list is None:
            epochs_list = [50, 100, 200, 300]

        print("\n" + "=" * 80)
        print("ğŸ”¬ Ablation Study 5: GraphMAE Epochs")
        print("=" * 80)

        results = {}

        for epochs in epochs_list:
            print(f"\n[Epochs = {epochs}]")
            config = self._create_config_variant(graphmae_epochs=epochs)
            metrics = self._run_experiment(config, use_graphmae=True)
            results[epochs] = metrics
            self._print_metrics(metrics)

        self.ablation_results['epochs'] = results
        return results

    # ============================================================
    # 5. ì „ì²´ Ablation Study ì‹¤í–‰
    # ============================================================

    def run_full_ablation_study(
        self,
        include_embedding: bool = True,
        include_graphmae: bool = True,
        include_mask_rate: bool = True,
        include_embed_size: bool = False,
        include_epochs: bool = False
    ) -> Dict[str, Any]:
        """
        ì „ì²´ Ablation Study ì‹¤í–‰

        Args:
            include_embedding: ì„ë² ë”© ë°©ë²• ablation ì‹¤í–‰ ì—¬ë¶€
            include_graphmae: GraphMAE ablation ì‹¤í–‰ ì—¬ë¶€
            include_mask_rate: Mask rate ablation ì‹¤í–‰ ì—¬ë¶€
            include_embed_size: ì„ë² ë”© ì°¨ì› ablation ì‹¤í–‰ ì—¬ë¶€
            include_epochs: Epochs ablation ì‹¤í–‰ ì—¬ë¶€

        Returns:
            ì „ì²´ ablation ê²°ê³¼
        """
        print("\n" + "=" * 80)
        print("ğŸš€ GRACE Ablation Study ì‹œì‘")
        print("=" * 80)

        start_time = datetime.now()

        # ê³µìœ  ë°ì´í„° ì¤€ë¹„
        self.prepare_shared_data()

        # ê° ablation study ì‹¤í–‰
        if include_embedding:
            self.run_embedding_ablation()

        if include_graphmae:
            self.run_graphmae_ablation()

        if include_mask_rate:
            self.run_mask_rate_ablation()

        if include_embed_size:
            self.run_embed_size_ablation()

        if include_epochs:
            self.run_epochs_ablation()

        end_time = datetime.now()
        elapsed = (end_time - start_time).total_seconds()

        print("\n" + "=" * 80)
        print(f"âœ… Ablation Study ì™„ë£Œ (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
        print("=" * 80)

        return self.ablation_results

    # ============================================================
    # 6. ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”
    # ============================================================

    def save_results(self, output_dir: str = "./ablation_output") -> None:
        """
        Ablation ê²°ê³¼ ì €ì¥

        Args:
            output_dir: ì €ì¥ ë””ë ‰í† ë¦¬
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # JSON ì €ì¥
        json_path = output_path / f"ablation_results_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            # numpy/torch íƒ€ì…ì„ Python ë„¤ì´í‹°ë¸Œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
            serializable_results = self._make_serializable(self.ablation_results)
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {json_path}")

    # ============================================================
    # í—¬í¼ ë©”ì„œë“œ
    # ============================================================

    def _create_config_variant(self, **kwargs) -> GRACEConfig:
        """
        base_configë¥¼ ë³µì‚¬í•˜ê³  íŠ¹ì • íŒŒë¼ë¯¸í„°ë§Œ ë³€ê²½í•œ ì„¤ì • ìƒì„±

        Args:
            **kwargs: ë³€ê²½í•  íŒŒë¼ë¯¸í„°

        Returns:
            ë³€í˜•ëœ GRACEConfig
        """
        import copy
        config = copy.deepcopy(self.base_config)

        for key, value in kwargs.items():
            if hasattr(config, key):
                setattr(config, key, value)

        # ê²°ê³¼ ì €ì¥ ë¹„í™œì„±í™” (ablation ê²°ê³¼ë§Œ ì €ì¥)
        config.save_results = False
        config.save_embeddings = False
        config.save_graph_viz = False
        config.verbose = False

        return config

    def _run_experiment(
        self,
        config: GRACEConfig,
        use_graphmae: bool = True
    ) -> Dict[str, float]:
        """
        ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ ë° ë©”íŠ¸ë¦­ ë°˜í™˜

        Args:
            config: ì‹¤í—˜ ì„¤ì •
            use_graphmae: GraphMAE ì‚¬ìš© ì—¬ë¶€

        Returns:
            {metric_name: value}
        """
        # NodeFeatureHandlerë¡œ ì„ë² ë”© ê³„ì‚°
        node_feature_handler = NodeFeatureHandler(self.doc_service)
        node_features = node_feature_handler.calculate_embeddings(
            self.word_graph.words,
            method=config.embedding_method,
            embed_size=config.embed_size
        )

        # GraphMAE ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
        if use_graphmae:
            # GraphMAEë¡œ ì„ë² ë”© í•™ìŠµ
            from ..GraphMAE import GraphMAEService, GraphMAEConfig

            embed_size = node_features.shape[1]
            mae_config = GraphMAEConfig.create_default(embed_size)
            mae_config.max_epochs = config.graphmae_epochs
            mae_config.learning_rate = config.graphmae_lr
            mae_config.mask_rate = config.mask_rate
            mae_config.device = config.graphmae_device or (
                "cuda" if torch.cuda.is_available() else "cpu"
            )

            graph_service = GraphService(self.doc_service)
            mae_service = GraphMAEService(graph_service, mae_config)

            # WordGraphì— ë…¸ë“œ íŠ¹ì„± ì„¤ì •
            self.word_graph.set_node_features_custom(node_features, NodeFeatureType.CUSTOM)

            # DGL ê·¸ë˜í”„ ë³€í™˜
            dgl_graph = graph_service.wordgraph_to_dgl(self.word_graph, node_features)

            # ëª¨ë¸ í•™ìŠµ
            mae_service.model = mae_service.create_mae_model(embed_size)
            device = torch.device(mae_config.device)
            mae_service.model.to(device)
            dgl_graph = dgl_graph.to(device)

            optimizer = torch.optim.Adam(
                mae_service.model.parameters(),
                lr=mae_config.learning_rate,
                weight_decay=mae_config.weight_decay
            )

            mae_service.model.train()
            for epoch in range(mae_config.max_epochs):
                optimizer.zero_grad()
                x = dgl_graph.ndata['feat']
                loss = mae_service.model(dgl_graph, x, epoch=epoch)
                loss.backward()
                optimizer.step()

            # ì„ë² ë”© ì¶”ì¶œ
            mae_service.model.eval()
            with torch.no_grad():
                embeddings = mae_service.model.embed(dgl_graph, dgl_graph.ndata['feat'])
            embeddings = embeddings.cpu()
        else:
            # GraphMAE ì—†ì´ ë©€í‹°ëª¨ë‹¬ ì„ë² ë”©ë§Œ ì‚¬ìš©
            embeddings = node_features

        # í´ëŸ¬ìŠ¤í„°ë§
        clustering_service = ClusteringService(random_state=self.random_state)

        if config.num_clusters is not None:
            labels = clustering_service.kmeans_clustering(embeddings, n_clusters=config.num_clusters)
        else:
            labels, best_k, _, _ = clustering_service.auto_clustering_elbow(
                embeddings,
                min_clusters=config.min_clusters,
                max_clusters=config.max_clusters
            )

        # í‰ê°€ ì§€í‘œ ê³„ì‚°
        metrics_service = MetricsService()
        metrics = metrics_service.calculate_metrics(
            embeddings,
            labels,
            ['silhouette', 'davies_bouldin', 'calinski_harabasz']
        )

        return metrics

    def _calculate_improvement(
        self,
        baseline: Dict[str, float],
        improved: Dict[str, float]
    ) -> Dict[str, float]:
        """
        ë‘ ê²°ê³¼ ê°„ ê°œì„ ìœ¨ ê³„ì‚°

        Args:
            baseline: ê¸°ì¤€ ê²°ê³¼
            improved: ê°œì„ ëœ ê²°ê³¼

        Returns:
            {metric: improvement_percentage}
        """
        improvement = {}

        for metric in baseline.keys():
            if metric not in improved:
                continue

            base_val = baseline[metric]
            improved_val = improved[metric]

            # davies_bouldinì€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ ë¶€í˜¸ ë°˜ì „
            if metric == 'davies_bouldin':
                pct = ((base_val - improved_val) / base_val) * 100
            else:
                pct = ((improved_val - base_val) / base_val) * 100

            improvement[metric] = pct

        return improvement

    def _print_metrics(self, metrics: Dict[str, float]) -> None:
        """ë©”íŠ¸ë¦­ ì¶œë ¥"""
        for metric_name, value in metrics.items():
            print(f"  {metric_name}: {value:.4f}")

    def _aggregate_multiple_runs(self, runs: List[Dict[str, float]]) -> Dict[str, float]:
        """
        ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•œ ê²°ê³¼ë¥¼ ì§‘ê³„ (í‰ê·  Â± í‘œì¤€í¸ì°¨)

        Args:
            runs: [{metric: value}, {metric: value}, ...]

        Returns:
            {metric_mean: value, metric_std: value, ...}
        """
        if not runs:
            return {}

        # ëª¨ë“  ë©”íŠ¸ë¦­ ì´ë¦„ ì¶”ì¶œ
        metric_names = list(runs[0].keys())
        aggregated = {}

        for metric in metric_names:
            values = [run[metric] for run in runs]
            aggregated[f"{metric}_mean"] = float(np.mean(values))
            aggregated[f"{metric}_std"] = float(np.std(values))
            aggregated[f"{metric}_min"] = float(np.min(values))
            aggregated[f"{metric}_max"] = float(np.max(values))

        # ì‹¤í–‰ íšŸìˆ˜ ì¶”ê°€
        aggregated['num_runs'] = len(runs)

        return aggregated

    def _print_aggregated_metrics(self, aggregated: Dict[str, float]) -> None:
        """ì§‘ê³„ëœ ë©”íŠ¸ë¦­ ì¶œë ¥ (í‰ê·  Â± í‘œì¤€í¸ì°¨)"""
        metric_names = set()
        for key in aggregated.keys():
            if key.endswith('_mean'):
                metric_names.add(key.replace('_mean', ''))

        for metric in sorted(metric_names):
            mean = aggregated.get(f"{metric}_mean", 0)
            std = aggregated.get(f"{metric}_std", 0)
            min_val = aggregated.get(f"{metric}_min", 0)
            max_val = aggregated.get(f"{metric}_max", 0)

            print(f"     {metric:20s}: {mean:.4f} Â± {std:.4f}  (range: [{min_val:.4f}, {max_val:.4f}])")

    def _make_serializable(self, obj: Any) -> Any:
        """
        JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ numpy/torch íƒ€ì…ì„ Python ë„¤ì´í‹°ë¸Œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜

        Args:
            obj: ë³€í™˜í•  ê°ì²´

        Returns:
            ì§ë ¬í™” ê°€ëŠ¥í•œ ê°ì²´
        """
        if isinstance(obj, dict):
            return {str(k): self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, torch.Tensor):
            return obj.cpu().numpy().tolist()
        else:
            return obj
