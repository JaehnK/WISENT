import sys
from typing import List, Optional, Dict
import threading

import spacy

from entities import *

from .TextPreprocssingService import TextPreprocessingService

import sys
from typing import List, Optional, Dict
import threading

class SentenceProcessingService:
    """ë¬¸ì¥ ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self, documents: Documents, preprocessing_service: TextPreprocessingService):
        self._documents = documents
        self._preprocessing = preprocessing_service
    
    def create_sentence_list_parallel(self, batch_size: int = 100, n_process: int = None) -> None:
        """ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ ìƒì„±"""
        rawdata = self._documents.rawdata
        if not rawdata:
            self._documents.sentence_list = []
            return
        
        print(f"Creating {len(rawdata)} sentences with spaCy...", file=sys.stderr)
        
        try:
            # ìœ íš¨í•œ ë¬¸ì„œë“¤ë§Œ í•„í„°ë§
            valid_docs = [(i, doc) for i, doc in enumerate(rawdata) if doc.strip()]
            if not valid_docs:
                self._documents.sentence_list = []
                return
            
            # spaCy ë°°ì¹˜ ì²˜ë¦¬
            texts = [doc for _, doc in valid_docs]
            processed_docs = self._preprocessing.process_text_batch(texts, batch_size, n_process)
            
            # Sentence ê°ì²´ ìƒì„±
            sentences = [None] * len(rawdata)  # ì›ë³¸ ì¸ë±ìŠ¤ ìœ ì§€
            
            for (original_idx, original_text), spacy_doc in zip(valid_docs, processed_docs):
                try:
                    sentence = Sentence(docs_ref=self._documents)
                    sentence.set_from_spacy_doc(spacy_doc, original_text)
                    sentences[original_idx] = sentence
                    
                    if original_idx % 100 == 0 and original_idx > 0:
                        print(f"Processed {original_idx}/{len(rawdata)} documents", file=sys.stderr)
                        
                except Exception as e:
                    print(f'Document {original_idx} processing failed: {e}', file=sys.stderr)
                    sentence = self._create_empty_sentence(original_text)
                    sentences[original_idx] = sentence
            
            # Noneì¸ í•­ëª©ë“¤ ì²˜ë¦¬ (ë¹ˆ ë¬¸ì„œë“¤)
            for i, sentence in enumerate(sentences):
                if sentence is None:
                    sentences[i] = self._create_empty_sentence(rawdata[i])
            
            self._documents.sentence_list = sentences
            
        except Exception as e:
            print(f"Parallel processing failed, falling back to sequential: {e}", file=sys.stderr)
            self._create_sentence_list_sequential()
        
        # ëª¨ë“  ë¬¸ì¥ ì²˜ë¦¬ ì™„ë£Œ í›„ words_list ì—…ë°ì´íŠ¸ (ì¤‘ìš”!)
        print(f"ğŸ” Updating words_list after sentence processing...", file=sys.stderr)
        try:
            all_words = self._documents.word_trie.get_all_words()
            print(f"ğŸ” Found {len(all_words) if all_words else 0} unique words in WordTrie", file=sys.stderr)
        except Exception as e:
            print(f"âŒ Error updating words_list: {e}", file=sys.stderr)
    
    def create_sentence_list_sequential(self) -> None:
        """ìˆœì°¨ ì²˜ë¦¬ë¡œ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ ìƒì„±"""
        self._create_sentence_list_sequential()
    
    def _create_sentence_list_sequential(self) -> None:
        """ìˆœì°¨ ì²˜ë¦¬ êµ¬í˜„"""
        rawdata = self._documents.rawdata
        if not rawdata:
            self._documents.sentence_list = []
            return
        
        print(f"Creating {len(rawdata)} sentences sequentially...", file=sys.stderr)
        
        sentences = []
        for i, doc_text in enumerate(rawdata):
            if i % 100 == 0 and i > 0:
                print(f"Processed {i}/{len(rawdata)} documents", file=sys.stderr)
            
            try:
                # ìƒˆë¡œìš´ ë°©ì‹: ìƒì„±ìì—ì„œ raw í…ìŠ¤íŠ¸ ì„¤ì •
                sentence = Sentence(raw=doc_text, docs_ref=self._documents)
                
                # ê¸°ì¡´ í˜¸í™˜ì„±: raw property setterê°€ ìë™ ì²˜ë¦¬ ìˆ˜í–‰
                if not sentence.is_processed:
                    sentence.raw = doc_text  # property setter í˜¸ì¶œ
                
                sentences.append(sentence)
            except Exception as e:
                print(f'Document {i} processing failed: {e}', file=sys.stderr)
                sentences.append(self._create_empty_sentence(doc_text))
        
        self._documents.sentence_list = sentences
    
    def _create_empty_sentence(self, text: str) -> Sentence:
        """ì‹¤íŒ¨í•œ ê²½ìš° ë¹ˆ Sentence ê°ì²´ ìƒì„±"""
        sentence = Sentence(raw=text, docs_ref=self._documents)
        sentence.lemmatised = []
        sentence.word_objects = []
        sentence.word_indices = []
        sentence.is_processed = True  # ì²˜ë¦¬ ì™„ë£Œë¡œ ë§ˆí¬ (ë¹ˆ ê²°ê³¼ë¼ë„)
        return sentence
