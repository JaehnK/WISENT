"""
ì§ì ‘ GraphMAE í…ŒìŠ¤íŠ¸ - DGL í˜¸í™˜ì„± ë¬¸ì œ ìš°íšŒ
- PyTorch Geometric ê¸°ë°˜ìœ¼ë¡œ GraphMAE í…ŒìŠ¤íŠ¸
- kaggle_RC_2019-05.csv ë°ì´í„° ì‚¬ìš©
- 200ê°œ ë‹¨ì–´, concat íŠ¹ì„± (64ì°¨ì›)
"""

import os
import sys
import pandas as pd
import torch
import numpy as np
from typing import List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# í•„ìš”í•œ ëª¨ë“ˆë“¤ import
from services.Document.DocumentService import DocumentService
from services.Graph.GraphService import GraphService
from services.Graph.NodeFeatureHandler import NodeFeatureHandler
from entities import Word, WordGraph, NodeFeatureType


def create_torch_geometric_data(word_graph: WordGraph):
    """WordGraphë¥¼ PyTorch Geometric Dataë¡œ ë³€í™˜ (ì´ë¯¸ PyG í˜•íƒœì´ì§€ë§Œ í˜¸í™˜ì„±ì„ ìœ„í•´)"""
    # WordGraphê°€ ì´ë¯¸ PyTorch Geometric í˜•íƒœë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì§ì ‘ ë³€í™˜
    return word_graph.to_pytorch_geometric()


def simple_graph_autoencoder_test(data):
    """ê°„ë‹¨í•œ ê·¸ë˜í”„ ì˜¤í† ì¸ì½”ë” í…ŒìŠ¤íŠ¸ (GraphMAE êµ¬ì¡° ëª¨ë°©)"""
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch_geometric.nn import GCNConv

    class SimpleGraphAutoEncoder(nn.Module):
        def __init__(self, input_dim, hidden_dim, output_dim):
            super().__init__()
            # ì¸ì½”ë”
            self.encoder1 = GCNConv(input_dim, hidden_dim)
            self.encoder2 = GCNConv(hidden_dim, output_dim)

            # ë””ì½”ë”
            self.decoder1 = GCNConv(output_dim, hidden_dim)
            self.decoder2 = GCNConv(hidden_dim, input_dim)

            self.dropout = nn.Dropout(0.2)

        def encode(self, x, edge_index):
            h = F.relu(self.encoder1(x, edge_index))
            h = self.dropout(h)
            z = self.encoder2(h, edge_index)
            return z

        def decode(self, z, edge_index):
            h = F.relu(self.decoder1(z, edge_index))
            h = self.dropout(h)
            x_recon = self.decoder2(h, edge_index)
            return x_recon

        def forward(self, x, edge_index):
            z = self.encode(x, edge_index)
            x_recon = self.decode(z, edge_index)
            return x_recon, z

    # ëª¨ë¸ ìƒì„±
    input_dim = data.x.shape[1]
    hidden_dim = 32
    output_dim = 64

    model = SimpleGraphAutoEncoder(input_dim, hidden_dim, output_dim)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    print(f"ëª¨ë¸ êµ¬ì¡°:")
    print(f"  - ì…ë ¥ ì°¨ì›: {input_dim}")
    print(f"  - íˆë“  ì°¨ì›: {hidden_dim}")
    print(f"  - ì¶œë ¥ ì°¨ì›: {output_dim}")
    print(f"  - ë…¸ë“œ ìˆ˜: {data.x.shape[0]}")
    print(f"  - ì—£ì§€ ìˆ˜: {data.edge_index.shape[1]}")

    # í›ˆë ¨
    model.train()
    losses = []

    for epoch in range(10):
        optimizer.zero_grad()
        x_recon, z = model(data.x, data.edge_index)

        # ì¬êµ¬ì„± ì†ì‹¤
        loss = F.mse_loss(x_recon, data.x)
        loss.backward()
        optimizer.step()

        losses.append(loss.item())
        if epoch % 2 == 0:
            print(f"  Epoch {epoch:2d}: Loss = {loss.item():.4f}")

    # ì„ë² ë”© ì¶”ì¶œ
    model.eval()
    with torch.no_grad():
        _, embeddings = model(data.x, data.edge_index)

    return embeddings, losses


def test_graphmae_pipeline():
    """GraphMAE íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸ‰ GraphMAE íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘!")
    print("=" * 60)

    try:
        # 1. ë°ì´í„° ë¡œë“œ
        csv_path = "/home/jaehun/lab/SENTIMENT/kaggle_RC_2019-05.csv"
        df = pd.read_csv(csv_path)
        documents = df['body'].dropna().head(500).tolist()
        print(f"âœ“ ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")

        # 2. ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        doc_service = DocumentService()
        doc_service.create_sentence_list(documents=documents)
        print(f"âœ“ ë¬¸ì¥ ì²˜ë¦¬ ì™„ë£Œ - {doc_service.get_sentence_count()}ê°œ ë¬¸ì¥")

        graph_service = GraphService(doc_service)
        print("âœ“ GraphService ì´ˆê¸°í™” ì™„ë£Œ")

        # 3. ê·¸ë˜í”„ ìƒì„±
        print(f"\nğŸ“Š ê³µì¶œí˜„ ê·¸ë˜í”„ ìƒì„± ì¤‘ (ìƒìœ„ 200ê°œ ë‹¨ì–´)...")
        word_graph = graph_service.build_complete_graph(
            top_n=200,
            exclude_stopwords=True,
            max_length=-1
        )

        print(f"âœ“ ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ")
        print(f"  - ë…¸ë“œ ìˆ˜: {word_graph.num_nodes}")
        print(f"  - ì—£ì§€ ìˆ˜: {word_graph.num_edges}")

        # 4. ë…¸ë“œ íŠ¹ì„± ì„¤ì •
        print(f"\nğŸ¯ ë…¸ë“œ íŠ¹ì„± ì„¤ì • ì¤‘...")
        node_handler = NodeFeatureHandler(graph_service.doc_service)
        concat_features = node_handler.calculate_embeddings(
            word_graph.words,
            method='concat',
            embed_size=64
        )

        word_graph.set_node_features_custom(concat_features, NodeFeatureType.CUSTOM)
        print(f"âœ“ ë…¸ë“œ íŠ¹ì„± ì„¤ì • ì™„ë£Œ: {word_graph.node_features.shape}")

        # 5. PyTorch Geometric ë³€í™˜
        print(f"\nğŸ”„ PyTorch Geometric ë³€í™˜ ì¤‘...")
        pyg_data = create_torch_geometric_data(word_graph)
        print(f"âœ“ PyG ë°ì´í„° ìƒì„± ì™„ë£Œ")
        print(f"  - ë…¸ë“œ íŠ¹ì„±: {pyg_data.x.shape}")
        print(f"  - ì—£ì§€ ì¸ë±ìŠ¤: {pyg_data.edge_index.shape}")

        # 6. ê·¸ë˜í”„ ì˜¤í† ì¸ì½”ë” í…ŒìŠ¤íŠ¸
        print(f"\nğŸ¤– ê·¸ë˜í”„ ì˜¤í† ì¸ì½”ë” í…ŒìŠ¤íŠ¸ ì¤‘...")
        embeddings, losses = simple_graph_autoencoder_test(pyg_data)

        print(f"âœ“ ì˜¤í† ì¸ì½”ë” í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print(f"  - ì¶œë ¥ ì„ë² ë”©: {embeddings.shape}")
        print(f"  - ìµœì¢… ì†ì‹¤: {losses[-1]:.4f}")
        print(f"  - ì†ì‹¤ ê°ì†Œ: {losses[0]:.4f} -> {losses[-1]:.4f}")

        # 7. ì„ë² ë”© í†µê³„
        print(f"\nğŸ“Š ì„ë² ë”© í†µê³„:")
        print(f"  - í‰ê· : {embeddings.mean().item():.4f}")
        print(f"  - í‘œì¤€í¸ì°¨: {embeddings.std().item():.4f}")
        print(f"  - ìµœì†Ÿê°’: {embeddings.min().item():.4f}")
        print(f"  - ìµœëŒ“ê°’: {embeddings.max().item():.4f}")

        # 8. ìƒìœ„ ë‹¨ì–´ ì¶œë ¥
        print(f"\nğŸ“‹ ìƒìœ„ 10ê°œ ë‹¨ì–´:")
        for i, word in enumerate(word_graph.words[:10]):
            emb_norm = torch.norm(embeddings[i]).item()
            print(f"  {i+1:2d}. {word.content:<15} (ë¹ˆë„: {word.freq:4d}, ì„ë² ë”© ë†ˆ: {emb_norm:.3f})")

        print(f"\nğŸŠ GraphMAE íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("=" * 60)
        print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")

        return embeddings

    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    embeddings = test_graphmae_pipeline()
    if embeddings is not None:
        print(f"\nğŸ¯ ìµœì¢… ê²°ê³¼: {embeddings.shape} ì„ë² ë”© ìƒì„± ì„±ê³µ!")