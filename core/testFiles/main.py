import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'main-service'))

import time
from typing import List, Optional

import pandas as pd
import numpy as np

# ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ import
from services import *
from entities import *

class TextProcessingBenchmark:
    """í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ê·¸ë˜í”„ ìƒì„± ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.doc_service: Optional[DocumentService] = None
        self.graph_service: Optional[GraphService] = None
        self.word_graph: Optional[WordGraph] = None
        
        # ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼
        self.timings = {}
        self.stats = {}
    
    def load_data(self, csv_path: str, text_column: str = 'body', limit: int = 5000) -> List[str]:
        """CSV ë°ì´í„° ë¡œë“œ"""
        print(f"ğŸ“„ Loading data from {csv_path} (limit: {limit})")
        
        start_time = time.time()
        df = pd.read_csv(csv_path)
        text_data = df[text_column][:limit].tolist()
        del df  # ë©”ëª¨ë¦¬ ì •ë¦¬
        
        load_time = time.time() - start_time
        self.timings['data_loading'] = load_time
        
        print(f"âœ… Loaded {len(text_data)} texts in {load_time:.3f} seconds")
        return text_data
    
    def preprocess_documents(self, texts: List[str], 
                           model_name: str = 'en_core_web_sm',
                           use_parallel: bool = True,
                           batch_size: int = 100,
                           n_process: int = None) -> DocumentService:
        """ë¬¸ì„œ ì „ì²˜ë¦¬ ì‹¤í–‰"""
        print(f"ğŸ”„ Starting document preprocessing...")
        print(f"   - Model: {model_name}")
        print(f"   - Parallel: {use_parallel}")
        print(f"   - Batch size: {batch_size}")
        
        start_time = time.time()
        
        # DocumentService ìƒì„±
        self.doc_service = DocumentService(model_name=model_name)
        
        # í…ìŠ¤íŠ¸ ì„¤ì • ë° ì²˜ë¦¬
        if use_parallel:
            # rawdata ì„¤ì •ì€ ìë™ìœ¼ë¡œ create_sentence_list() í˜¸ì¶œ
            # í•˜ì§€ë§Œ ë³‘ë ¬ì²˜ë¦¬ë¥¼ ì›í•˜ë©´ ìˆ˜ë™ìœ¼ë¡œ í˜¸ì¶œ
            self.doc_service._documents.rawdata = texts  # ìë™ ì²˜ë¦¬ ìš°íšŒ
            self.doc_service.create_sentence_list_parallel(batch_size=batch_size, n_process=n_process)
        else:
            self.doc_service.rawdata = texts  # ìë™ìœ¼ë¡œ ìˆœì°¨ ì²˜ë¦¬
        
        preprocess_time = time.time() - start_time
        self.timings['preprocessing'] = preprocess_time
        
        # === ë””ë²„ê¹…: ì¤‘ê°„ ê²°ê³¼ í™•ì¸ ===
        print(f"\nğŸ” DEBUGGING PREPROCESSING RESULTS:")
        print(f"   ğŸ“Š Document count: {len(self.doc_service.rawdata) if self.doc_service.rawdata else 0}")
        print(f"   ğŸ“Š Sentence count: {len(self.doc_service.sentence_list) if self.doc_service.sentence_list else 0}")
        
        # ì²« ë²ˆì§¸ ë¬¸ì¥ í™•ì¸
        if self.doc_service.sentence_list and len(self.doc_service.sentence_list) > 0:
            first_sentence = self.doc_service.sentence_list[0]
            print(f"   ğŸ“Š First sentence processed: {first_sentence.is_processed}")
            print(f"   ğŸ“Š First sentence words: {len(first_sentence.lemmatised)}")
            print(f"   ğŸ“Š First sentence preview: '{first_sentence.get_text_preview(50)}'")
            if first_sentence.lemmatised:
                print(f"   ğŸ“Š First few lemmas: {first_sentence.lemmatised[:5]}")
            if first_sentence.processing_errors:
                print(f"   âš ï¸  First sentence errors: {first_sentence.processing_errors}")
        
        # ë‹¨ì–´ í†µê³„ í™•ì¸
        words_list = self.doc_service.words_list
        print(f"   ğŸ“Š Total unique words: {len(words_list) if words_list else 0}")
        
        if words_list and len(words_list) > 0:
            print(f"   ğŸ“Š First few words: {[w.content for w in words_list[:5]]}")
            top_words = self.doc_service.get_top_words(top_n=10)
            print(f"   ğŸ“Š Top 10 words: {[(w.content, w.freq) for w in top_words]}")
        else:
            print(f"   âŒ No words found in words_list!")
            
            # WordTrie ì§ì ‘ í™•ì¸
            word_trie = self.doc_service._documents.word_trie
            print(f"   ğŸ“Š WordTrie word_count: {word_trie.word_count}")
            
            all_words = word_trie.get_all_words()
            print(f"   ğŸ“Š WordTrie all_words: {len(all_words) if all_words else 0}")
            
            if all_words:
                print(f"   ğŸ“Š WordTrie first words: {[w.content for w in all_words[:5]]}")
        
        # í†µê³„ ìˆ˜ì§‘
        self.stats['document_stats'] = self.doc_service.get_stats()
        
        print(f"âœ… Document preprocessing completed in {preprocess_time:.3f} seconds")
        print(f"   ğŸ“Š {self.doc_service}")
        
        return self.doc_service
    
    def create_graph(self, top_n: int = 500, 
                    exclude_stopwords: bool = True,
                    use_word_graph: bool = True) -> GraphService:
        """ê·¸ë˜í”„ ìƒì„± ì‹¤í–‰"""
        if self.doc_service is None:
            raise ValueError("Documents not preprocessed. Call preprocess_documents() first.")
        
        print(f"ğŸ“Š Starting graph creation...")
        print(f"   - Top words: {top_n}")
        print(f"   - Exclude stopwords: {exclude_stopwords}")
        print(f"   - Use WordGraph: {use_word_graph}")
        
        start_time = time.time()
        
        # GraphService ìƒì„±
        self.graph_service = GraphService(self.doc_service)
        
        if use_word_graph:
            # ìƒˆë¡œìš´ ë°©ì‹: WordGraph ê°ì²´ ìƒì„±
            self.word_graph = self.graph_service.build_complete_graph(
                top_n=top_n, 
                exclude_stopwords=exclude_stopwords
            )
        else:
            # ê¸°ì¡´ ë°©ì‹ (í˜¸í™˜ì„±)
            self.graph_service.create_graph(top_n=top_n, exclude_stopwords=exclude_stopwords)
            self.graph_service.create_co_occurrence_edges()
        
        graph_time = time.time() - start_time
        self.timings['graph_creation'] = graph_time
        
        # ê·¸ë˜í”„ í†µê³„ ìˆ˜ì§‘
        if use_word_graph and self.word_graph:
            self.stats['graph_stats'] = self.word_graph.get_graph_stats()
        else:
            self.stats['graph_stats'] = self.graph_service.get_graph_statistics()
        
        print(f"âœ… Graph creation completed in {graph_time:.3f} seconds")
        print(f"   ğŸ“Š {self.graph_service}")
        
        return self.graph_service
    
    def visualize_graph(self, top_k: int = 50, 
                       save_path: str = './graph_visualization.png',
                       min_weight: float = None,
                       show_labels: bool = True,
                       figsize: tuple = (12, 8)) -> None:
        """ê·¸ë˜í”„ ì‹œê°í™”"""
        if self.graph_service is None:
            raise ValueError("Graph not created. Call create_graph() first.")
        
        print(f"ğŸ¨ Creating graph visualization...")
        print(f"   - Top nodes: {top_k}")
        print(f"   - Save path: {save_path}")
        
        start_time = time.time()
        
        if self.word_graph:
            # ìƒˆë¡œìš´ ë°©ì‹: WordGraph ì‹œê°í™”
            self.graph_service.visualize_word_graph(
                self.word_graph,
                top_k=top_k,
                save_path=save_path,
                min_weight=min_weight,
                show_labels=show_labels,
                figsize=figsize
            )
        else:
            # ê¸°ì¡´ ë°©ì‹ (í˜¸í™˜ì„±)
            self.graph_service.visualize(
                top_k=top_k,
                save_path=save_path,
                min_weight=min_weight,
                show_labels=show_labels,
                figsize=figsize
            )
        
        viz_time = time.time() - start_time
        self.timings['visualization'] = viz_time
        
        print(f"âœ… Visualization completed in {viz_time:.3f} seconds")
    
    def print_performance_report(self) -> None:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š PERFORMANCE REPORT")
        print("="*60)
        
        # ì‹œê°„ ì¸¡ì • ê²°ê³¼
        print("â±ï¸  TIMING RESULTS:")
        total_time = 0
        for stage, duration in self.timings.items():
            print(f"   ğŸ• {stage.replace('_', ' ').title()}: {duration:.3f} sec")
            total_time += duration
        print(f"   ğŸ• Total Processing Time: {total_time:.3f} sec")
        
        # ë¬¸ì„œ í†µê³„
        if 'document_stats' in self.stats:
            print(f"\nğŸ“„ DOCUMENT STATISTICS:")
            doc_stats = self.stats['document_stats']
            print(f"   ğŸ“Š Documents: {doc_stats.get('document_count', 0)}")
            print(f"   ğŸ“Š Sentences: {doc_stats.get('sentence_count', 0)}")
            print(f"   ğŸ“Š Unique Words: {doc_stats.get('unique_word_count', 0)}")
            print(f"   ğŸ“Š Content Words: {doc_stats.get('content_words', 0)}")
        
        # ê·¸ë˜í”„ í†µê³„
        if 'graph_stats' in self.stats:
            print(f"\nğŸ•¸ï¸  GRAPH STATISTICS:")
            graph_stats = self.stats['graph_stats']
            print(f"   ğŸ“Š Nodes: {graph_stats.get('num_nodes', 0)}")
            print(f"   ğŸ“Š Edges: {graph_stats.get('num_edges', 0)}")
            print(f"   ğŸ“Š Density: {graph_stats.get('density', 0):.4f}")
            print(f"   ğŸ“Š Avg Degree: {graph_stats.get('avg_degree', 0):.2f}")
        
        print("="*60 + "\n")
    
    def run_complete_benchmark(self, csv_path: str, 
                                limit: int = 10000,
                                top_n: int = 500,
                                top_k_viz: int = 50,
                                save_path: str = './graph_output.png') -> None:
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        try:
            # 1. ë°ì´í„° ë¡œë“œ
            texts = self.load_data(csv_path, limit=limit)
            
            # 2. ë¬¸ì„œ ì „ì²˜ë¦¬
            self.preprocess_documents(texts, use_parallel=True)
            
            # 3. ê·¸ë˜í”„ ìƒì„±
            self.create_graph(top_n=top_n, use_word_graph=True)
            
            # # 4. ê·¸ë˜í”„ ì‹œê°í™”
            # self.visualize_graph(top_k=top_k_viz, save_path=save_path)
            
            # # 5. ì„±ëŠ¥ ë¦¬í¬íŠ¸
            # self.print_performance_report()
            
            try:
                # Word2Vec ì„œë¹„ìŠ¤ ìƒì„±
                print("Creating Word2Vec service...")
                word2vec_service = Word2VecService.create_default(self.doc_service)
                
                # ì–´íœ˜ ì •ë³´ ì¶œë ¥
                vocab_info = word2vec_service.get_vocabulary_info()
                print(f"Vocabulary size: {vocab_info['vocab_size']}")
                print(f"Total sentences: {vocab_info['total_sentences']}")
                print(f"Total tokens: {vocab_info['total_tokens']}")
                print(f"Most frequent words: {[word for word, _ in vocab_info['most_frequent_words'][:10]]}")
                
                # ëª¨ë¸ í›ˆë ¨
                print("\nStarting Word2Vec training...")
                word2vec_service.train("word2vec_embeddings.txt")
                
                # 7. ë‹¨ì–´ ë²¡í„° í…ŒìŠ¤íŠ¸
                print("\n" + "-"*40)
                print("Word Vector Tests")
                print("-"*40)
                
                # ë¹ˆë„ ë†’ì€ ë‹¨ì–´ë“¤ë¡œ í…ŒìŠ¤íŠ¸
                test_words = [word for word, _ in vocab_info['most_frequent_words'][:5]]
                
                for word in test_words:
                    vector = word2vec_service.get_word_vector(word)
                    if vector is not None:
                        print(f"'{word}' vector shape: {vector.shape}, norm: {np.linalg.norm(vector):.4f}")
                        print(f"  First 5 dimensions: {vector[:5]}")
                
                # 8. ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸
                print("\n" + "-"*40)
                print("Word Similarity Tests")
                print("-"*40)
                
                # ìƒìœ„ ë¹ˆë„ ë‹¨ì–´ë“¤ì˜ ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°
                for word in test_words[:3]:
                    similar_words = word2vec_service.find_similar_words(word, top_k=5)
                    if similar_words:
                        print(f"\nWords similar to '{word}':")
                        for similar_word, similarity in similar_words:
                            print(f"  {similar_word}: {similarity:.4f}")
                
                # 9. ë‹¨ì–´ ìœ ì¶” í…ŒìŠ¤íŠ¸ (ê°€ëŠ¥í•œ ê²½ìš°)
                print("\n" + "-"*40)
                print("Word Analogy Tests")
                print("-"*40)
                
                # ì–´íœ˜ì—ì„œ ì ì ˆí•œ ë‹¨ì–´ ìŒì„ ì°¾ì•„ì„œ í…ŒìŠ¤íŠ¸
                vocab_words = list(word2vec_service.data_loader.word2id.keys())
                
                # ì¼ë°˜ì ì¸ ìœ ì¶” í…ŒìŠ¤íŠ¸ ë‹¨ì–´ë“¤ (ìˆëŠ” ê²½ìš°ì—ë§Œ)
                analogy_tests = [
                    ("man", "woman", "king"),  # man:woman = king:?
                    ("good", "better", "bad"), # good:better = bad:?
                    ("big", "bigger", "small") # big:bigger = small:?
                ]
                
                for word_a, word_b, word_c in analogy_tests:
                    if all(word in vocab_words for word in [word_a, word_b, word_c]):
                        result = word2vec_service.word_analogy(word_a, word_b, word_c, top_k=3)
                        if result:
                            print(f"\n{word_a} is to {word_b} as {word_c} is to:")
                            for word, score in result:
                                print(f"  {word}: {score:.4f}")
                    else:
                        missing = [w for w in [word_a, word_b, word_c] if w not in vocab_words]
                        print(f"Skipping analogy test - missing words: {missing}")
                
                # 10. ë²¡í„° ì—°ì‚° í…ŒìŠ¤íŠ¸
                print("\n" + "-"*40)
                print("Vector Operations Tests")
                print("-"*40)
                
                if len(test_words) >= 3:
                    word1, word2, word3 = test_words[:3]
                    
                    vec1 = word2vec_service.get_word_vector(word1)
                    vec2 = word2vec_service.get_word_vector(word2)
                    vec3 = word2vec_service.get_word_vector(word3)
                    
                    if all(v is not None for v in [vec1, vec2, vec3]):
                        # ë²¡í„° í•©
                        vec_sum = vec1 + vec2
                        print(f"Vector addition: {word1} + {word2}")
                        print(f"  Result norm: {np.linalg.norm(vec_sum):.4f}")
                        
                        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                        def cosine_similarity(v1, v2):
                            return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
                        
                        sim_12 = cosine_similarity(vec1, vec2)
                        sim_13 = cosine_similarity(vec1, vec3)
                        sim_23 = cosine_similarity(vec2, vec3)
                        
                        print(f"\nCosine similarities:")
                        print(f"  {word1} <-> {word2}: {sim_12:.4f}")
                        print(f"  {word1} <-> {word3}: {sim_13:.4f}")
                        print(f"  {word2} <-> {word3}: {sim_23:.4f}")
                
                # 11. ëª¨ë¸ ì €ì¥
                print("\n" + "-"*40)
                print("Model Saving")
                print("-"*40)
                
                model_path = "word2vec_model.pth"
                word2vec_service.save_model(model_path)
                print(f"Model saved to {model_path}")
                
                # 12. ì „ì²´ í‰ê°€
                print("\n" + "-"*40)
                print("Overall Evaluation")
                print("-"*40)
                
                word2vec_service.evaluate_similarity(test_words[:5], top_k=3)
                
                # 13. ì„œë¹„ìŠ¤ ì •ë³´ ìš”ì•½
                print("\n" + "-"*40)
                print("Word2Vec Service Summary")
                print("-"*40)
                print(word2vec_service)
            
            except Exception as e:
                print(f"Word2Vec training/testing failed: {e}")
                import traceback
                traceback.print_exc()            
            
            
        except Exception as e:
            print(f"âŒ Error during benchmark: {e}")
            raise


def main():
    """ë©”ì¸ í•¨ìˆ˜ - ë¦¬íŒ©í† ë§ëœ ë²„ì „"""
    print("Starting Text Processing & Graph Creation Benchmark")
    print("Using New Architecture: DocumentService + GraphService + WordGraph")
    print("-" * 60)
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = TextProcessingBenchmark()
    
    try:
        benchmark.run_complete_benchmark(
            csv_path="../kaggle_RC_2019-05.csv",
            limit=10000,
            top_n=500,
            top_k_viz=50,
            save_path='./graphOutput.png'
        )
        
        print("ğŸ‰ Benchmark completed successfully!")
        
    except FileNotFoundError:
        print("âŒ CSV file not found. Please check the path.")
    except Exception as e:
        print(f"âŒ Benchmark failed: {e}")


def comparison_test():
    """ê¸°ì¡´ ë°©ì‹ vs ìƒˆë¡œìš´ ë°©ì‹ ë¹„êµ í…ŒìŠ¤íŠ¸"""
    print("ğŸ”„ Running Comparison Test: Old vs New Architecture")
    print("-" * 60)
    
    # ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    try:
        df = pd.read_csv("../kaggle_RC_2019-05.csv")
        texts = df['body'][:1000].tolist()  # 1000ê°œë§Œ í…ŒìŠ¤íŠ¸
        del df
        
        # ìƒˆë¡œìš´ ë°©ì‹ í…ŒìŠ¤íŠ¸
        print("ğŸ†• Testing New Architecture...")
        new_benchmark = TextProcessingBenchmark()
        
        start_time = time.time()
        new_benchmark.preprocess_documents(texts, use_parallel=True)
        new_benchmark.create_graph(top_n=100, use_word_graph=True)
        new_total_time = time.time() - start_time
        
        print(f"âœ… New Architecture Total Time: {new_total_time:.3f} seconds")
        
        # í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤)
        print("ğŸ”„ Testing Compatibility Mode...")
        compat_benchmark = TextProcessingBenchmark()
        
        start_time = time.time()
        compat_benchmark.preprocess_documents(texts, use_parallel=False)
        compat_benchmark.create_graph(top_n=100, use_word_graph=False)  # ê¸°ì¡´ ë°©ì‹
        compat_total_time = time.time() - start_time
        
        print(f"âœ… Compatibility Mode Total Time: {compat_total_time:.3f} seconds")
        
        # ê²°ê³¼ ë¹„êµ
        print(f"\nğŸ“Š Performance Comparison:")
        print(f"   ğŸ†• New Architecture: {new_total_time:.3f}s")
        print(f"   ğŸ”„ Compatibility Mode: {compat_total_time:.3f}s")
        print(f"   ğŸ“ˆ Speed Difference: {((compat_total_time - new_total_time) / compat_total_time * 100):+.1f}%")
        
    except Exception as e:
        print(f"âŒ Comparison test failed: {e}")


if __name__ == "__main__":
    # ê¸°ë³¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    main()
    
    # ì„ íƒì ìœ¼ë¡œ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    # comparison_test()