import re
import time

from preprocess import *

def comparsion(docs):
    print("\n" + "="*60)
    print("CONTRACTION PROCESSING ANALYSIS")
    print("="*60)
    
    # ì¶•ì•½í˜•ì´ ì–´ë–»ê²Œ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
    print("Raw vs Processed comparison:")
    for i, sentence in enumerate(docs.sentence_list[:5]):
        if sentence and sentence.word_indices is not None:
            print(f"\nSentence {i+1}:")
            print(f"  ğŸ“ Raw: {sentence.raw}")
            
            # ì „ì²˜ë¦¬ ê³¼ì • ë‹¨ê³„ë³„ í™•ì¸
            expanded = sentence._expand_contractions(sentence.raw)
            cleaned_step1 = re.sub(r"'s\b", " s", expanded)
            cleaned_final = re.sub(r'[^\w\s]', '', cleaned_step1)
            
            print(f"  ğŸ”„ Expanded: '{expanded}'")
            print(f"  ğŸ§¹ Cleaned: '{cleaned_final}'")
            print(f"  ğŸ”¤ Lemmatised: {sentence._lemmatised}")
            print(f"  ğŸ“Š Word indices: {sentence.word_indices}")
            
            # ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ë‹¨ì–´ë“¤ ì²´í¬
            problematic = []
            for word in sentence._lemmatised:
                if len(word) <= 1 or word in ['t', 's', 'll', 've', 're', 'm', 'd']:
                    problematic.append(word)
            
            if problematic:
                print(f"  âš ï¸  Potentially problematic tokens: {problematic}")
            else:
                print(f"  âœ… No problematic tokens detected")
        else:
            print(f"\nSentence {i+1}: âŒ Processing failed")
    
def part_of_speech_analysis(docs):
    print("\n" + "="*60)
    print("PART-OF-SPEECH ANALYSIS")
    print("="*60)
    
    # í’ˆì‚¬ë³„ ë‹¨ì–´ ë¶„ë¥˜
    pos_categories = {}
    for word in docs.words_list:
        category = word.get_pos_category()
        if category not in pos_categories:
            pos_categories[category] = []
        pos_categories[category].append(word)
    
    print("Words by POS category:")
    for category in sorted(pos_categories.keys()):
        words_in_category = pos_categories[category]
        print(f"\nğŸ“‚ {category}: {len(words_in_category)} words")
        
        # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•´ì„œ ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
        top_words = sorted(words_in_category, key=lambda w: w.freq, reverse=True)[:5]
        for word in top_words:
            pos_info = f"{word.dominant_pos}"
            if len(word.pos_counts) > 1:
                pos_info += f" (ë‹¤ì¤‘: {word.pos_counts})"
            print(f"    '{word.content}' - freq: {word.freq}, pos: {pos_info}")
        
        if len(words_in_category) > 5:
            print(f"    ... and {len(words_in_category) - 5} more {category.lower()}s")
    
    print(f"\nğŸ” Multi-POS words (ë™ì¼ ë‹¨ì–´ì˜ ë‹¤ë¥¸ í’ˆì‚¬ ìš©ë²•):")
    multi_pos_words = [w for w in docs.words_list if len(w.pos_counts) > 1]
    if multi_pos_words:
        for word in sorted(multi_pos_words, key=lambda w: len(w.pos_counts), reverse=True)[:10]:
            print(f"  '{word.content}': {word.pos_counts} (ì£¼í’ˆì‚¬: {word.dominant_pos})")
    else:
        print("  No words with multiple POS tags found")
    
    print(f"\nğŸ“Š POS Category Distribution:")
    total_words = len(docs.words_list)
    for category in sorted(pos_categories.keys()):
        count = len(pos_categories[category])
        percentage = (count / total_words) * 100
        print(f"  {category}: {count} words ({percentage:.1f}%)")
    
def detailed_word_analysis(docs):
    print("\n" + "="*60)
    print("DETAILED WORD ANALYSIS")
    print("="*60)
    
    print("Sample detailed word information:")
    sample_words = sorted(docs.words_list, key=lambda w: w.freq, reverse=True)[:30]
    for word in sample_words:
        print(f"\nğŸ”¤ Word: '{word.content}'")
        print(f"   ğŸ“Š Frequency: {word.freq}")
        print(f"   ğŸ”¢ Index: {word.idx}")
        print(f"   ğŸ“ Category: {word.get_pos_category()}")
        print(f"   ğŸ·ï¸  Dominant POS: {word.dominant_pos}")
        print(f"   ğŸ·ï¸  StopWord: {word.is_stopword}")
        print(f"   ğŸ“‹ All POS tags: {word.pos_counts}")
        
        # í¸ì˜ ë©”ì„œë“œ í…ŒìŠ¤íŠ¸
        pos_checks = []
        if word.is_noun(): pos_checks.append("NOUN")
        if word.is_verb(): pos_checks.append("VERB") 
        if word.is_adjective(): pos_checks.append("ADJECTIVE")
        if word.is_pronoun(): pos_checks.append("PRONOUN")
        if word.is_adverb(): pos_checks.append("ADVERB")
        
        if pos_checks:
            print(f"   âœ… Type checks: {', '.join(pos_checks)}")
        else:
            print(f"   â„¹ï¸  Type: OTHER ({word.get_pos_category()})")

def contraction_test(docs):
    print("\n" + "="*60)
    print("CONTRACTION DETECTION")
    print("="*60)
    
    # ì›ë³¸ ë¬¸ì¥ì—ì„œ ì¶•ì•½í˜• íŒ¨í„´ ì°¾ê¸°
    contraction_patterns = [
        "don't", "can't", "won't", "it's", "we're", "wasn't", 
        "I'd", "shouldn't", "I'll", "aren't", "you're", "he'll", "I've"
    ]
    
    found_contractions = []
    for sentence in docs.sentence_list:
        if sentence and sentence.raw:
            for pattern in contraction_patterns:
                if pattern.lower() in sentence.raw.lower():
                    found_contractions.append((pattern, sentence.raw))
    
    print(f"Found contractions in original text:")
    for contraction, sentence in found_contractions[:5]:  # ì²˜ìŒ 5ê°œë§Œ
        print(f"  '{contraction}' in: {sentence}")
    
    # ì „ì²´ í† í° ìˆ˜ vs ì˜ˆìƒ í† í° ìˆ˜ ë¹„êµ
    total_original_tokens = sum(len(doc.split()) for doc in test_docs)
    total_processed_tokens = sum(len(s.word_indices) if s.word_indices else 0 for s in docs.sentence_list)
    
    print(f"\nğŸ“Š Token count comparison:")
    print(f"  Original word count (space-split): {total_original_tokens}")
    print(f"  Processed token count: {total_processed_tokens}")
    print(f"  Difference: {total_processed_tokens - total_original_tokens}")
    
    if total_processed_tokens > total_original_tokens:
        print("  â„¹ï¸  More tokens after processing (contractions likely split)")
    elif total_processed_tokens < total_original_tokens:
        print("  âš ï¸  Fewer tokens after processing (some tokens lost)")
    else:
        print("  âœ… Same token count")
        
    suspicious_words = [w for w in docs.words_list if w.content in ['t', 's', 'll', 've', 're', 'm', 'd', 'nt']]
    if suspicious_words:
        print("ğŸš¨ Issues detected with contraction handling:")
        print("   - Contractions are being split into fragments")
        print("   - Consider using a contraction expansion library")
        print("   - Or modify preprocessing to handle apostrophes better")
    else:
        print("âœ… No obvious contraction handling issues detected")

if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ
    docs = Docs()
    
    # ì¶•ì•½í˜•ê³¼ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ê°€ í¬í•¨ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_docs = [
        "I don't think he'll come today.",
        "She can't find her keys anywhere.",
        "They won't be here until tomorrow.",
        "It's not what I've been looking for.",
        "We're going to John's house tonight.",
        "That wasn't the answer I'd expected.",
        "You shouldn't have done that.",
        "I'll be there in five minutes.",
        "The dog's tail is wagging happily.",
        "These aren't the droids you're looking for."
    ]
    
    print("ğŸ§ª TESTING CONTRACTIONS AND APOSTROPHES")
    print("="*60)
    start = time.time()
    docs.rawdata = test_docs
    end = time.time()
    print(f"ğŸ• {end - start:.5f} sec") 
    
    print("\n" + "=" * 60)
    print("DOCUMENT PROCESSING SUMMARY")
    print("="*60)
    print(docs)
    
    # comparsion(docs)
    # part_of_speech_analysis(docs)
    detailed_word_analysis(docs)
    contraction_test(docs)