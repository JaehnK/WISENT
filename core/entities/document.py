from dataclasses import dataclass
from typing import List, Optional, Dict

from .word import Word
from .sentence import Sentence  
from .trie import WordTrie

@dataclass
class Document:
    """ë‹¨ìˆœí•œ ê°œë³„ ë¬¸ì„œ ë°ì´í„°"""
    content: str
    doc_id: Optional[str] = None
    
    def __post_init__(self):
        if self.doc_id is None:
            self.doc_id = f"doc_{id(self)}"
    
    @property
    def word_count(self) -> int:
        return len(self.content.split()) if self.content else 0
    
    def is_valid(self) -> bool:
        return bool(self.content and self.content.strip())


class Documents:
    """ì „ì²´ ë¬¸ì„œ ì»¬ë ‰ì…˜ì˜ ë°ì´í„° ë ˆì´ì–´"""
    
    def __init__(self):
        # í•µì‹¬ ë°ì´í„° 3ê°œ
        self._rawdata: Optional[List[str]] = None
        self._sentence_list: Optional[List[Sentence]] = None
        self._word_trie = WordTrie()
        
        # ë¶€ê°€ ì •ë³´
        self._document_objects: Optional[List[Document]] = None
    
    @property
    def rawdata(self) -> Optional[List[str]]:
        return self._rawdata
    
    @rawdata.setter 
    def rawdata(self, docs: List[str]):
        """ì›ë³¸ ë°ì´í„° ì„¤ì • ë° ê¸°ë³¸ ê²€ì¦"""
        if len(docs) < 10:
            raise ValueError("Number of documents is less than 10. Not sufficient for clustering.")
        
        self._rawdata = docs
        
        # Document ê°ì²´ë“¤ ìƒì„± (ì„ íƒì )
        self._document_objects = [
            Document(content=content, doc_id=f"doc_{i}") 
            for i, content in enumerate(docs)
        ]
        
        # ê¸°ì¡´ ì²˜ë¦¬ ê²°ê³¼ ì´ˆê¸°í™”
        self._sentence_list = None
        self._word_trie = WordTrie()
    
    @property
    def sentence_list(self) -> Optional[List[Sentence]]:
        return self._sentence_list
    
    @sentence_list.setter
    def sentence_list(self, sentences: List[Sentence]):
        self._sentence_list = sentences
    
    @property
    def word_trie(self) -> WordTrie:
        return self._word_trie
    
    @property
    def document_objects(self) -> Optional[List[Document]]:
        return self._document_objects
    
    @property
    def words_list(self) -> Optional[List[Word]]:
        """WordTrieì—ì„œ ëª¨ë“  ë‹¨ì–´ ì¶”ì¶œ"""
        if self._word_trie is None:
            return None
        
        try:
            all_words = self._word_trie.get_all_words()
            print(f"ğŸ” Documents.words_list: WordTrie returned {len(all_words) if all_words else 0} words")
            return all_words
        except Exception as e:
            print(f"âŒ Error getting words from WordTrie: {e}")
            return None
    
    def get_document_count(self) -> int:
        """ë¬¸ì„œ ê°œìˆ˜"""
        return len(self._rawdata) if self._rawdata else 0
    
    def get_sentence_count(self) -> int:
        """ë¬¸ì¥ ê°œìˆ˜"""
        return len(self._sentence_list) if self._sentence_list else 0
    
    def get_word_count(self) -> int:
        """ê³ ìœ  ë‹¨ì–´ ê°œìˆ˜"""
        return self._word_trie.word_count if self._word_trie else 0
    
    def get_document(self, index: int) -> str:
        """ì¸ë±ìŠ¤ë¡œ ì›ë³¸ ë¬¸ì„œ ì¡°íšŒ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        if not self._rawdata or index >= len(self._rawdata):
            raise IndexError("Document index out of range")
        return self._rawdata[index]
    
    def get_valid_documents(self) -> List[str]:
        """ìœ íš¨í•œ ë¬¸ì„œë“¤ë§Œ ë°˜í™˜"""
        if not self._rawdata:
            return []
        return [doc for doc in self._rawdata if doc and doc.strip()]
    
    def add_word(self, word_content: str, pos_tag: str = None) -> Word:
        """ë‹¨ì–´ë¥¼ íŠ¸ë¦¬ì— ì¶”ê°€í•˜ê³  Word ê°ì²´ ë°˜í™˜"""
        try:
            word_obj = self._word_trie.insert_or_get_word(word_content, pos_tag)
            return word_obj
        except Exception as e:
            print(f"âŒ Error in Documents.add_word: {e}")
            raise
    
    def get_co_occurrence_edges(self, word_to_node: Dict[str, int]):
        """ê³µì¶œí˜„ ì—£ì§€ ìƒì„± (ê¸°ì¡´ ë©”ì„œë“œ ìœ ì§€)"""
        from .co_occurence import build_cooccurrence_edges
        return build_cooccurrence_edges(word_to_node, self._sentence_list)
    
    def clear(self):
        """ëª¨ë“  ë°ì´í„° ì´ˆê¸°í™”"""
        self._rawdata = None
        self._sentence_list = None
        self._word_trie = WordTrie()
        self._document_objects = None
    
    def get_stats(self) -> Dict[str, int]:
        """ì „ì²´ í†µê³„"""
        return {
            'document_count': self.get_document_count(),
            'sentence_count': self.get_sentence_count(), 
            'unique_word_count': self.get_word_count(),
            'total_words': sum(doc.word_count for doc in (self._document_objects or [])),
            'valid_documents': len(self.get_valid_documents())
        }
    
    def __str__(self):
        """ê¸°ì¡´ Docs.__str__ í˜¸í™˜"""
        return (f"Documents: {self.get_document_count()} docs, "
                f"{self.get_sentence_count()} sentences, "
                f"{self.get_word_count()} unique words")
    
    def __getitem__(self, idx):
        """ê¸°ì¡´ Docs.__getitem__ í˜¸í™˜"""
        return self.get_document(idx)
    
    def __len__(self):
        return self.get_document_count()